{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What is Cluster.dev?","text":"<p>Cluster.dev is an open-source tool designed to manage cloud native infrastructures with simple declarative manifests - stack templates. It allows you to describe an entire infrastructure and deploy it with a single tool.</p> <p>Stack templates can be based on Terraform modules, Kubernetes manifests, Shell scripts, Helm charts and Argo CD/Flux applications, OPA policies, etc. Cluster.dev brings those components together so that you can deploy, test and distribute a whole set of components with pinned versions.</p>"},{"location":"#when-do-i-need-clusterdev","title":"When do I need Cluster.dev?","text":"<ol> <li>If you have a common infrastructure pattern that contains multiple components stuck together. This could be a bunch of TF-modules, or a set of K8s add-ons where you need to re-use this pattern inside your projects.</li> <li>If you develop an infrastructure platform that you ship to other teams and they need to launch new infrastructures from your template.</li> <li>If you build a complex infrastructure that contains different technologies and you need to perform integration testing to confirm the components' interoperability. Once done, you can then promote the changes to next environments.</li> <li>If you are a software vendor and need to deliver infrastructure deployment along with your software.</li> </ol>"},{"location":"#base-concept-diagrams","title":"Base concept diagrams","text":"<p>Stack templates are composed of units - Lego-like building blocks responsible for passing variables to a particular technology.</p> <p></p> <p>Templates define infrastructure patterns or even the whole platform.</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Common variables, secrets, and templating for different technologies.</li> <li>Same GitOps Development experience for Terraform, Shell, Kubernetes.</li> <li>Can be used with any cloud, on-premises or hybrid scenarios.</li> <li>Encourage teams to follow technology best practices.</li> </ul>"},{"location":"DevOpsDays21/","title":"DevOps Days 2021","text":"<p>Hi Guys, I'm Vova from SHALB!</p> <p>In SHALB we build and support a hundreds of infrastructures so we have some outcome and experience that we'd like to share.</p>"},{"location":"DevOpsDays21/#problems-of-the-modern-cloud-native-infrastructures","title":"Problems of the modern Cloud Native infrastructures","text":""},{"location":"DevOpsDays21/#multiple-technologies-needs-to-be-coupled","title":"Multiple technologies needs to be coupled","text":"<p>Infrastructure code for complete infra contains a different technologies: Terraform, Helm, Docker, Bash, Ansible, Cloud-Init, CI/CD-scripts, SQL's, GitOps applications, Secrets, etc..</p> <p>With a bunch of specific DSL'es: yaml, hcl, go-template, json(net).</p> <p>And each with the specific code styles: declarative, imperative, interrogative. With the different diff'ing: two or three way merges. And even using different patching across one tool, like: patchesStrategicMerge, patchesJson6902 in kustomize.</p> <p>So you need to compile all that stuff together to be able spawn a whole infra with one shot. And you need one-shot to be clear that it is fully automated and can be GitOps-ed :)!</p>"},{"location":"DevOpsDays21/#even-super-powerful-tool-has-own-limits","title":"Even super-powerful tool has own limits","text":"<p>So thats why:</p> <ul> <li>Terragrunt, Terraspace and Atlantis exist for Terraform.</li> <li>Helmfile, Helm Operator exist form Helm.</li> <li>and Helm exist for K8s yaml :).</li> </ul>"},{"location":"DevOpsDays21/#its-hard-to-deal-with-variables-and-secrets","title":"Its hard to deal with variables and secrets","text":"<ol> <li> <p>Should be passed between different technologies in sometimes unpredictable sequences. In example you need to set the IAM role arn created by Terraform to Cert-Manager controller deployed with Helm values.</p> </li> <li> <p>Variables should be passed across different infrastructures, even located on different clouds. Imagine you need to obtain DNS Zone from CloudFlare, then set 'NS' records in AWS Route53, and then grant an External-DNS controller which is deployed in    on-prem K8s provisioned with Rancher to change this zone in AWS...</p> </li> <li> <p>Secrets that needs to be secured and shared across different team members and teams.  Team members sometime leave, or accounts could be compromised and you need completely revoke access from them across a set of infras with one shot.</p> </li> <li> <p>Variables should be decoupled from infrastructure pattern itself and needs a wise sane defaults.    If you hardcode variables - its hard to reuse such code.</p> </li> </ol>"},{"location":"DevOpsDays21/#development-and-testing","title":"Development and Testing","text":"<p>You'd like to maximize reusage of the existing infrastructure patterns:</p> <pre><code>- Terraform modules\n- Helm Charts\n- K8s Operators\n- Dockerfile's\n</code></pre> <p>Pin versions for all you have in your infra, in example: Pin the aws cli and terraform binary version along with Helm, Prometheus operator version and your private kustomize application.</p>"},{"location":"DevOpsDays21/#available-solutions","title":"Available solutions","text":"<p>So to couple their infrastructure with some 'glue' most of engineers have a several ways:</p> <ul> <li>CI/CD sequential appying, ex Jenkins/Gitlab job that deploys infra components one by one.</li> <li>Own bash scripts and Makefiles, that pulls code from different repos and applies using hardcoded sequence.</li> <li>Some of them struggle to write everything with one tech: ex Pulumi(but you need to know how to code in JS, GO, .NET), or Terraform (and you fail) :)</li> <li>Some of them rely on existing API (Kuberenetes) architecture like a Crossplane.</li> </ul>"},{"location":"DevOpsDays21/#we-create-own-tool-clusterdev-or-cdev","title":"We create own tool - cluster.dev or 'cdev'","text":"<p>It's Capabilities:</p> <ul> <li>Re-using all existing Terraform private and public modules and Helm Charts.</li> <li>Templating anything with Go-template functions, even Terraform modules in Helm-style templates.</li> <li>Applying parallel changes in multiple infrastructures concurrently.</li> <li>Using the same global variables and secrets across different infrastructures, clouds and technologies.</li> <li>Create and manage secrets with Sops or cloud secret storages.</li> <li>Generate a ready to use Terraform code.</li> </ul>"},{"location":"DevOpsDays21/#short-demo","title":"Short Demo","text":""},{"location":"ROADMAP/","title":"Project Roadmap","text":""},{"location":"ROADMAP/#v01x-basic-scenario","title":"v.0.1.x - Basic Scenario","text":"<ul> <li> Create a state storage (AWS S3+Dynamo) for infrastructure resources</li> <li> Deploy a Kubernetes(Minikube) in AWS using default VPC</li> <li> Provision Kubernetes with addons: Ingress-Nginx, Load Balancer, Cert-Manager, ExtDNS, ArgoCD</li> <li> Deploy a sample \"WordPress\" application to Kubernetes cluster using ArgoCD</li> <li> Delivered as GitHub Actions and Docker Image</li> </ul>"},{"location":"ROADMAP/#v02x-bash-based-poc","title":"v0.2.x - Bash-based PoC","text":"<ul> <li> Deliver with cluster creation a default DNS sub-zone:   <code>*.username-clustername.cluster.dev</code></li> <li> Create a cluster.dev backend to register newly created clusters</li> <li> Support for GitLab CI Pipelines</li> <li> ArgoCD sample applications (raw manifests, local helm chart, public helm chart)</li> <li> Support for DigitalOcean Kubernetes cluster 59</li> <li> DigitalOcean Domains sub-zones 65</li> <li> AWS EKS provisioning. Spot and Mixed ASG support.</li> <li> Support for Operator Lifecycle Manager</li> </ul>"},{"location":"ROADMAP/#v03x-go-based-beta","title":"v0.3.x - Go-based Beta","text":"<ul> <li> Go-based reconciler</li> <li> External secrets management with Sops and godaddy/kubernetes-external-secrets</li> <li> Team and user management with Keycloak</li> <li> Apps deployment: Kubernetes Dashboard, Grafana and Kibana.</li> <li> OIDC access to kubeconfig with Keycloak and jetstack/kube-oidc-proxy/ 53</li> <li> SSO access to ArgoCD and base applications: Kubernetes Dashboard, Grafana, Kibana</li> <li> OIDC integration with GitHub, GitLab, Google Auth, Okta</li> </ul>"},{"location":"ROADMAP/#v04x","title":"v0.4.x","text":"<ul> <li> CLI Installer 54</li> <li> Add GitHub runner and test GitHub Action Continuous Integration workflow</li> <li> Argo Workflows for DAG and CI tasks inside Kubernetes cluster</li> <li> Google Cloud Platform Kubernetes (GKE) support</li> <li> Custom Terraform modules and reconcilation</li> <li> Kind provisioner</li> </ul>"},{"location":"ROADMAP/#v05x","title":"v0.5.x","text":"<ul> <li> kops provisioner support</li> <li> k3s provisioner</li> <li> Cost $$$ estimation during installation</li> <li> Web user interface design</li> </ul>"},{"location":"ROADMAP/#v06x","title":"v0.6.x","text":"<ul> <li> Rancher RKE provisioner support</li> <li> Multi-cluster support for user management and SSO</li> <li> Multi-cluster support for ArgoCD</li> <li> Crossplane integration</li> </ul>"},{"location":"cdev-vs-helmfile/","title":"Cluster.dev vs. Helmfile: Managing Kubernetes Helm Charts","text":"<p>Kubernetes, with its dynamic and versatile nature, requires efficient tools to manage its deployments. Two tools that have gained significant attention for this purpose are Cluster.dev and Helmfile. Both are designed to manage Kubernetes Helm charts but with varying focuses and features. This article offers a comparative analysis of Cluster.dev and Helmfile, spotlighting their respective strengths.</p>"},{"location":"cdev-vs-helmfile/#1-introduction","title":"1. Introduction","text":"<p>Cluster.dev:</p> <ul> <li>A versatile tool designed for managing cloud-native infrastructures with declarative manifests, known as stack templates.</li> <li>Integrates with various technologies, including Terraform modules, Kubernetes manifests, Helm charts, and more.</li> <li>Promotes a unified approach to deploying, testing, and distributing infrastructure components.</li> </ul> <p>Helmfile:</p> <ul> <li>A declarative specification for deploying and synchronizing Helm charts.</li> <li>Provides automation and workflow tooling around the Helm tool, making it easier to deploy and manage Helm charts across several clusters or environments.</li> </ul>"},{"location":"cdev-vs-helmfile/#2-core-features-abilities","title":"2. Core Features &amp; Abilities","text":""},{"location":"cdev-vs-helmfile/#declarative-manifests","title":"Declarative Manifests","text":"<ul> <li> <p>Cluster.dev: Uses stack templates, allowing integration with various technologies. This versatility makes Cluster.dev suitable for describing and deploying an entire infrastructure.</p> </li> <li> <p>Helmfile: Uses a specific declarative structure for Helm charts. Helmfile's <code>helmfile.yaml</code> describes the desired state of Helm releases, promoting consistent deployments across environments.</p> </li> </ul>"},{"location":"cdev-vs-helmfile/#integration-and-flexibility","title":"Integration and Flexibility","text":"<ul> <li> <p>Cluster.dev: Supports a wide array of technologies beyond Helm, such as Terraform and Kubernetes manifests. This broad scope makes it suitable for diverse cloud-native projects.</p> </li> <li> <p>Helmfile: Exclusively focuses on Helm, providing tailored utilities, commands, and functions that enhance the Helm experience.</p> </li> </ul>"},{"location":"cdev-vs-helmfile/#configuration-management","title":"Configuration Management","text":"<ul> <li> <p>Cluster.dev: Uses stack templates to handle configurations, integrating them with the respective technology modules. For Helm, it provides a dedicated \"helm\" unit type.</p> </li> <li> <p>Helmfile: Employs <code>helmfile.yaml</code>, where users can specify Helm chart details, dependencies, repositories, and values. Helmfile also supports templating and layering of values, providing powerful configuration management.</p> </li> </ul>"},{"location":"cdev-vs-helmfile/#workflow-and-automation","title":"Workflow and Automation","text":"<ul> <li> <p>Cluster.dev: Offers a GitOps Development experience across different technologies, ensuring consistent deployment practices.</p> </li> <li> <p>Helmfile: Provides a suite of commands (<code>apply</code>, <code>sync</code>, <code>diff</code>, etc.) tailored for Helm workflows, making it easy to manage Helm releases in an automated manner.</p> </li> </ul>"},{"location":"cdev-vs-helmfile/#values-and-templating","title":"Values and Templating","text":"<ul> <li> <p>Cluster.dev: Supports values templating for Helm units, and offers functions like <code>remoteState</code> and <code>insertYAML</code> for dynamic inputs.</p> </li> <li> <p>Helmfile: Robustly supports values templating, with features like environment-specific value files and Go templating. It allows for dynamic generation of values based on the environment or external commands.</p> </li> </ul>"},{"location":"cdev-vs-helmfile/#3-ideal-use-cases","title":"3. Ideal Use Cases","text":"<p>Cluster.dev:</p> <ul> <li>Large-scale cloud-native projects integrating various technologies.</li> <li>Unified deployment and management of multi-technology stacks.</li> <li>Organizations aiming for a consistent GitOps approach across their stack.</li> </ul> <p>Helmfile:</p> <ul> <li>Projects heavily reliant on Helm for deployments.</li> <li>Organizations needing advanced configuration management for Helm charts.</li> <li>Scenarios requiring repetitive and consistent Helm chart deployments across various clusters or environments.</li> </ul>"},{"location":"cdev-vs-helmfile/#4-conclusion","title":"4. Conclusion","text":"<p>Cluster.dev and Helmfile, while both capable of managing Helm charts, cater to different spectrums of the Kubernetes deployment landscape. Cluster.dev aims for a holistic approach to cloud-native infrastructure management, integrating various technologies. Helmfile, on the other hand, delves deep into Helm's ecosystem, offering advanced tooling for Helm chart management.</p> <p>Your choice between the two should depend on the specifics of your infrastructure needs, the technologies you're predominantly using, and your desired management granularity.</p> <p>Note: Always consider evaluating the tools in your specific context, and it may even be beneficial to use them in tandem if they fit the project's requirements.</p>"},{"location":"cdev-vs-pulumi/","title":"Cluster.dev vs. Pulumi and Crossplane","text":"<p>Pulumi and Crossplane are modern alternatives to Terraform. </p> <p>These are great tools and we admire alternative views on infrastructure management. </p> <p>What makes Cluster.dev different is its purpose and limitations.  Tools like Pulumi, Crossplane, and Terraform are aimed to manage clouds - creating new instances or clusters, cloud resources like databases, and others.  Cluster.dev however is designed to manage the whole infrastructure, including those tools as units. That means you can run Terraform, then run Pulumi, or Bash, or Ansible with variables received from Terraform, and then run Crossplane or something else. Cluster.dev is created to connect and manage all infrastructure tools. </p> <p>With infrastructure tools, users are often restricted with one-tool usage that has a specific language or DSL, whereas Cluster.dev allows to have a limitless number of units and workflow combinations between tools. </p> <p>For now, Cluster.dev has major support for Terraform only, mainly because we want to provide the best experience for the majority of users. Moreover, Terraform is a de-facto industry standard and already has many modules created by the community.  To read more on the subject please refer to the Cluster.dev vs. Terraform section.</p> <p>If you or your company would like to use Pulumi or Crossplane with Cluster.dev, please feel free to contact us. </p>"},{"location":"cdev-vs-terraform/","title":"Cluster.dev vs. Terraform","text":"<p>Terraform is a great and popular tool for creating infrastructures. Despite the fact that it was founded more than five years ago, Terraform supports many providers and resources, which is impressive. </p> <p>Cluster.dev loves Terraform (and even supports export to the plain Terraform code). Still, Terraform lacks a robust relation system, fast plans, automatic reconciliation, and configuration templates. </p> <p>Cluster.dev, on the other hand, is a management software that uses Terraform alongside other infrastructure tools as building blocks. </p> <p>As a higher abstraction, Cluster.dev fixes all listed problems: builds a single source of truth, and combines and orchestrates different infrastructure tools under the same roof.  </p> <p>Let's dig further into the problems that Cluster.dev solves.</p>"},{"location":"cdev-vs-terraform/#internal-relation","title":"Internal relation","text":"<p>As Terraform has pretty complex rendering logic, it affects the relations between its pieces. For example, you cannot define a provider for, let say, k8s or Helm, in the same codebase that creates a k8s cluster. This forces users to resort to internal hacks or having to employ a custom wrapper to have two different deploys \u2014 that is a problem we solved via Cluster.dev. </p> <p>Another problem with internal relations concerns huge execution plans that Terraform creates for massive projects. Users who tried to avoid this issue by using small Terraform repos, faced the challenges of weak \"remote state\" relations and limited possibilities of reconciliation: it was not possible to trigger a related module, if the output of the module it relied upon had been changed.</p> <p>Cluster.dev however allows you to trigger only the necessary parts, as it is a GitOps-first tool. </p>"},{"location":"cdev-vs-terraform/#templating","title":"Templating","text":"<p>The second limitation of Terraform is templating: Terraform doesn\u2019t support templating of tf files that it uses. This forces users to perform hacks that further tangle their Terraform files.  As for the Cluster.dev templating, it allows to include, let\u2019s say, Jenkins Terraform module with custom inputs for dev environment and not to include it for staging and production \u2014 all in the same codebase. </p>"},{"location":"cdev-vs-terraform/#third-party","title":"Third Party","text":"<p>Terraform allows for executing Bash or Ansible. However, it doesn't contain many instruments to control where and how these external tools will be run. </p> <p>Cluster.dev as a cloud native manager on the other hand provides all tools with the same level of support and integration.</p>"},{"location":"cdev-vs-terragrunt/","title":"Cluster.dev vs. Terragrunt: A Comparative Analysis","text":"<p>Both Cluster.dev and Terragrunt have been increasingly popular tools within the DevOps community, particularly among those working with Terraform. However, each tool brings its unique offerings to the table. This article dives deep into a comparison of these tools to provide a clear understanding of their capabilities and respective strengths.</p>"},{"location":"cdev-vs-terragrunt/#1-introduction","title":"1. Introduction","text":"<p>Cluster.dev</p> <ul> <li>A comprehensive tool designed for managing cloud-native infrastructures using declarative manifests called stack templates.</li> <li>Integrates with various components such as Terraform modules, Kubernetes manifests, Shell scripts, Helm charts, Argo CD/Flux applications, and OPA policies.</li> <li>Provides a unified approach to deploy, test, and distribute components.</li> </ul> <p>Terragrunt</p> <ul> <li>An extension for Terraform designed to provide additional utilities to manage Terraform modules.</li> <li>Helps in keeping Terraform configurations DRY (Don\u2019t Repeat Yourself), ensuring modularity and reuse across multiple environments.</li> <li>Offers a layered approach to configuration, simplifying the management of Terraform deployments.</li> </ul>"},{"location":"cdev-vs-terragrunt/#2-core-features-abilities","title":"2. Core Features &amp; Abilities","text":""},{"location":"cdev-vs-terragrunt/#configuration-management","title":"Configuration Management","text":"<ul> <li>Cluster.dev: Uses stack templates for configuration. Templates can integrate with various technologies like Terraform, Kubernetes, and Helm. A single template can describe and deploy an entire infrastructure.</li> <li>Terragrunt: Primarily deals with Terraform configurations. Enables reuse and modularity of configurations by linking to Terraform modules and managing inputs/outputs between them.</li> </ul>"},{"location":"cdev-vs-terragrunt/#flexibility-integration","title":"Flexibility &amp; Integration","text":"<ul> <li>Cluster.dev: Highly flexible, supporting a multitude of components from Terraform modules to Kubernetes manifests. Its design promotes integrating diverse cloud-native technologies.</li> <li>Terragrunt: Primarily focuses on Terraform. While it offers great utility functions for Terraform, its integration capabilities are confined to Terraform's ecosystem.</li> </ul>"},{"location":"cdev-vs-terragrunt/#workflow-management","title":"Workflow Management","text":"<ul> <li>Cluster.dev: Aims for a consistent GitOps Development experience across multiple technologies.</li> <li>Terragrunt: Facilitates workflows within Terraform, such as ensuring consistent remote state management and modular Terraform deployments.</li> </ul>"},{"location":"cdev-vs-terragrunt/#versioning-source-management","title":"Versioning &amp; Source Management","text":"<ul> <li>Cluster.dev: Allows pinning versions for components and supports specifying module versions directly within the stack templates.</li> <li>Terragrunt: Uses a version reference for Terraform modules, making it easier to manage and switch between different versions of modules.</li> </ul>"},{"location":"cdev-vs-terragrunt/#special-features","title":"Special Features","text":"<ul> <li>Cluster.dev: Provides templating for different technologies, can be used in any cloud or on-premises scenarios, and promotes technology best practices.</li> <li>Terragrunt: Provides utilities like automatic retries, locking, and helper scripts for advanced scenarios in Terraform.</li> </ul>"},{"location":"cdev-vs-terragrunt/#3-when-to-use-which","title":"3. When to Use Which?","text":"<p>Cluster.dev is ideal for:</p> <ul> <li>Managing infrastructures that integrate multiple cloud-native technologies.</li> <li>Projects that need unified deployment, testing, and distribution.</li> <li>Environments that require a consistent GitOps development experience across technologies.</li> </ul> <p>Terragrunt shines when:</p> <ul> <li>You're working exclusively or primarily with Terraform.</li> <li>Needing to maintain configurations DRY and modular across multiple environments.</li> <li>Complex Terraform projects that require additional utilities like locking, retries, and advanced configuration management.</li> </ul>"},{"location":"cdev-vs-terragrunt/#4-conclusion","title":"4. Conclusion","text":"<p>While both Cluster.dev and Terragrunt cater to infrastructure as code and Terraform enthusiasts, their ideal use cases differ. Cluster.dev provides a more holistic approach to cloud-native infrastructure management, incorporating a range of technologies. In contrast, Terragrunt focuses on enhancing the Terraform experience.</p> <p>Your choice between Cluster.dev and Terragrunt should be influenced by your specific project requirements, the technologies you're using, and the level of integration you desire.</p> <p>Remember, the choice of tool often depends on the specifics of the project, organizational practices, and individual preferences. Always evaluate tools in the context of your needs.</p>"},{"location":"cli-commands/","title":"CLI Commands","text":""},{"location":"cli-commands/#general","title":"General","text":"<ul> <li> <p><code>apply</code>       Deploy or update an infrastructure according to project configuration.</p> </li> <li> <p><code>build</code>       Build cache dirs for all units in the current project.</p> </li> <li> <p><code>cdev</code>        Refer to Cluster.dev docs for details. </p> </li> <li> <p><code>destroy</code>     Destroy an infrastructure deployed by the current project.</p> </li> <li> <p><code>help</code>        Get help about any command.</p> </li> <li> <p><code>output</code>      Display project outputs.</p> </li> <li> <p><code>plan</code>        Show changes that will be applied in the current project.</p> </li> <li> <p><code>validate</code>    Validate the configuration files in a directory, referring only to the configuration and not accessing any remote state buckets.</p> <p>Validate runs checks that verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. It is thus primarily useful for general verification of reusable stack templates. </p> </li> </ul>"},{"location":"cli-commands/#project","title":"Project","text":"<ul> <li> <p><code>project</code>           Manage projects.</p> </li> <li> <p><code>project info</code>      Show detailed information about the current project, such as the number of units and their types, the number of stacks, etc.</p> </li> <li> <p><code>project create</code>    Generate a new project from generator-template in the current directory. The directory should not contain <code>yaml</code> or <code>yml</code> files.</p> </li> </ul>"},{"location":"cli-commands/#secret","title":"Secret","text":"<ul> <li> <p><code>secret</code>           Manage secrets.</p> </li> <li> <p><code>secret ls</code>        List secrets in the current project.</p> </li> <li> <p><code>secret edit [secret_name]</code>     Create a new secret or edit the existing one.</p> </li> <li> <p><code>secret create</code>    Generate a new secret in the current directory. The directory must contain the project.</p> </li> </ul>"},{"location":"cli-commands/#state","title":"State","text":"<ul> <li> <p><code>state</code>            State operations. </p> </li> <li> <p><code>state unlock</code>     Unlock state forcibly.</p> </li> <li> <p><code>state pull</code>       Download the remote state.</p> </li> <li> <p><code>state update</code>     Update the state of the current project to version %v. Make sure that the state of the project is consistent (run <code>cdev apply</code> with the old version before updating).</p> </li> </ul>"},{"location":"cli-options/","title":"CLI Options","text":""},{"location":"cli-options/#global-flags","title":"Global flags","text":"<ul> <li> <p><code>--cache</code>             Use previously cached build directory.</p> </li> <li> <p><code>-l, --log-level string</code>   Set the logging level ('debug'|'info'|'warn'|'error'|'fatal') (default \"info\").</p> </li> <li> <p><code>--parallelism int</code>    Max parallel threads for module applying (default - <code>3</code>).</p> </li> </ul>"},{"location":"cli-options/#apply-flags","title":"Apply flags","text":"<ul> <li> <p><code>--force</code>              Skip interactive approval.</p> </li> <li> <p><code>-h</code>, <code>--help</code>         Help for apply.</p> </li> <li> <p><code>--ignore-state</code>       Apply even if the state has not changed.</p> </li> </ul>"},{"location":"cli-options/#create-flags","title":"Create flags","text":"<ul> <li> <p><code>-h</code>, <code>--help</code>        Help for create.</p> </li> <li> <p><code>--interactive</code>       Use interactive mode for project generation.</p> </li> <li> <p><code>--list-templates</code>    Show all available templates for project generator.</p> </li> </ul>"},{"location":"cli-options/#destroy-flags","title":"Destroy flags","text":"<ul> <li> <p><code>--force</code>              Skip interactive approval.</p> </li> <li> <p><code>-h</code>, <code>--help</code>         Help for destroy.</p> </li> <li> <p><code>--ignore-state</code>       Destroy current configuration of units employed in a project, and ignore the state. </p> </li> </ul>"},{"location":"cluster-state/","title":"Cluster State","text":"<p>Cluster.dev state is a dataset representing the current infrastructure state. It maps real-world resources to your configuration, tracks changes, and stores dependencies between units.</p> <p>Cluster.dev works with both cdev and Terraform states. The cdev state is an abstraction over the Terraform state, streamlining state validation for efficiency. Refer to the official documentation for more details on Terraform state.</p> <p>Cdev and Terraform states can be stored locally or remotely, determined by the backend configuration. By default, Cluster.dev uses a local backend to store the cluster state unless a remote storage location is specified in the <code>project.yaml</code>:</p> <pre><code>  name: dev\n  kind: Project\n  backend: aws-backend\n  variables:\n    organization: cluster.dev\n    region: eu-central-1\n    state_bucket_name: test-tmpl-dev\n</code></pre> <p>State is generated during the unit application stage. When changes are made to a project, Cluster.dev constructs state based on the current project, considering the modifications. It then compares the current and desired configurations, highlighting the differences \u2013 the units to be modified, applied, or destroyed. Running <code>cdev apply</code> deploys the changes and updates the state.</p> <p>Units that fail with an error during <code>cdev apply</code> or <code>cdev destroy</code>, or those partially applied due to an aborted process, are marked as <code>tainted</code> in the state.</p> <p>While deleting the cdev state is discouraged, it is not critical, unlike Terraform state. Cluster.dev units, being Terraform-based, maintain their own states. In the event of deletion, the state will be redeployed with the next <code>cdev apply</code>.\"</p> <p>Use dedicated commands to interact with the cdev state. Manual editing of the state file is highly discouraged.</p>"},{"location":"env-variables/","title":"Environment Variables","text":"<ul> <li><code>CDEV_TF_BINARY</code>      Indicates which Terraform binary to use. Recommended usage: for debug during template development.</li> </ul>"},{"location":"examples-aws-eks/","title":"AWS-EKS","text":"<p>Cluster.dev uses stack templates to generate users' projects in a desired cloud. AWS-EKS is a stack template that creates and provisions Kubernetes clusters in AWS cloud by means of Amazon Elastic Kubernetes Service (EKS).</p> <p>On this page you will find guidance on how to create an EKS cluster on AWS using one of the Cluster.dev prepared samples \u2013 the AWS-EKS stack template. Running the example code will have the following resources created:</p> <ul> <li> <p>EKS cluster with addons:</p> <ul> <li> <p>cert-manager</p> </li> <li> <p>ingress-nginx</p> </li> <li> <p>external-dns</p> </li> <li> <p>argocd</p> </li> </ul> </li> <li> <p>AWS IAM roles for EKS IRSA cert-manager and external-dns</p> </li> <li> <p>(optional, if you use cluster.dev domain) Route53 zone .cluster.dev <li> <p>(optional, if vpc_id is not set) VPC for EKS cluster</p> </li>"},{"location":"examples-aws-eks/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Terraform version 1.4+</p> </li> <li> <p>AWS account</p> </li> <li> <p>AWS CLI installed</p> </li> <li> <p>kubectl installed</p> </li> <li> <p>Cluster.dev client installed</p> </li> </ol>"},{"location":"examples-aws-eks/#authentication","title":"Authentication","text":"<p>Cluster.dev requires cloud credentials to manage and provision resources. You can configure access to AWS in two ways:</p> <p>Info</p> <p>Please note that you have to use IAM user with granted administrative permissions.</p> <ul> <li> <p>Environment variables: provide your credentials via the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>, the environment variables that represent your AWS Access Key and AWS Secret Key. You can also use the <code>AWS_DEFAULT_REGION</code> or <code>AWS_REGION</code> environment variable to set a region, if needed. Example usage:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"MYACCESSKEY\"\nexport AWS_SECRET_ACCESS_KEY=\"MYSECRETKEY\"\nexport AWS_DEFAULT_REGION=\"eu-central-1\"\n</code></pre> </li> <li> <p>Shared Credentials File (recommended): set up an AWS configuration file to specify your credentials.</p> <p>Credentials file <code>~/.aws/credentials</code> example:</p> <pre><code>[cluster-dev]\naws_access_key_id = MYACCESSKEY\naws_secret_access_key = MYSECRETKEY\n</code></pre> <p>Config: <code>~/.aws/config</code> example:</p> <pre><code>[profile cluster-dev]\nregion = eu-central-1\n</code></pre> <p>Then export <code>AWS_PROFILE</code> environment variable.</p> <pre><code>export AWS_PROFILE=cluster-dev\n</code></pre> </li> </ul>"},{"location":"examples-aws-eks/#install-aws-client","title":"Install AWS client","text":"<p>If you don't have the AWS CLI installed, refer to AWS CLI official installation guide, or use commands from the example:</p> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\naws s3 ls\n</code></pre>"},{"location":"examples-aws-eks/#create-s3-bucket","title":"Create S3 bucket","text":"<p>Cluster.dev uses S3 bucket for storing states. Create the bucket with the command:</p> <pre><code>aws s3 mb s3://cdev-states\n</code></pre>"},{"location":"examples-aws-eks/#dns-zone","title":"DNS Zone","text":"<p>In the AWS-EKS stack template example, you need to define a Route 53 hosted zone. Options:</p> <ol> <li> <p>You already have a Route 53 hosted zone.</p> </li> <li> <p>Create a new hosted zone using a Route 53 documentation example.</p> </li> <li> <p>Use \"cluster.dev\" domain for zone delegation.</p> </li> </ol>"},{"location":"examples-aws-eks/#create-project","title":"Create project","text":"<ol> <li> <p>Configure access to AWS and export required variables.</p> </li> <li> <p>Create locally a project directory, cd into it and execute the command:</p> <p><pre><code>  cdev project create https://github.com/shalb/cdev-aws-eks\n</code></pre> This will create a new, empty project.</p> </li> <li> <p>Edit variables in the example's files, if necessary:</p> <ul> <li> <p><code>project.yaml</code> - main project config. Sets common global variables for the current project such as organization, region, state bucket name etc. See project configuration docs.</p> </li> <li> <p><code>backend.yaml</code> - configures backend for Cluster.dev states (including Terraform states). Uses variables from <code>project.yaml</code>. See backend docs.</p> </li> <li> <p><code>stack.yaml</code> - describes stack configuration. See stack docs.</p> </li> </ul> </li> <li> <p>Run <code>cdev plan</code> to build the project. In the output you will see an infrastructure that is going to be created after running <code>cdev apply</code>.</p> <p>Note</p> <p>Prior to running <code>cdev apply</code> make sure to look through the <code>stack.yaml</code> file and replace the commented fields with real values. If you would like to use existing VPC and subnets, uncomment preset options and set the correct VPC ID and subnets' IDs. If you leave them as is, Cluster.dev will have VPC and subnets created for you.</p> </li> <li> <p>Run <code>cdev apply</code></p> <p>Tip</p> <p>We highly recommend running <code>cdev apply</code> in a debug mode so that you can see the Cluster.dev logging in the output: <code>cdev apply -l debug</code></p> </li> <li> <p>After <code>cdev apply</code> is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and that the stack template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the <code>stack.yaml</code>.</p> </li> <li> <p>Displayed in the output will also be a command on how to get kubeconfig and connect to your Kubernetes cluster.</p> </li> <li> <p>Destroy the cluster and all created resources with the command <code>cdev destroy</code></p> </li> </ol>"},{"location":"examples-aws-k3s-prometheus/","title":"AWS-K3s Prometheus","text":"<p>The code, the text, and the screencast prepared by Oleksii Kurinnyi, a monitoring engineer at SHALB. </p>"},{"location":"examples-aws-k3s-prometheus/#goal","title":"Goal","text":"<p>In this section we will use and modify the basic AWS-K3s Cluster.dev template to deploy the Prometheus monitoring stack to a cluster. As a result, we will have a K3s cluster on AWS with a set of required controllers (Ingress, cert-manager, Argo CD) and installed kube-prometheus stack. The code samples are available in the GitHub repository.</p>"},{"location":"examples-aws-k3s-prometheus/#requirements","title":"Requirements","text":""},{"location":"examples-aws-k3s-prometheus/#os","title":"OS","text":"<p>We should have some client host with Ubuntu 20.04 to use this manual without any customization. </p>"},{"location":"examples-aws-k3s-prometheus/#docker","title":"Docker","text":"<p>We should install Docker to the client host.</p>"},{"location":"examples-aws-k3s-prometheus/#aws-account","title":"AWS account","text":"<ul> <li> <p>Log in to an existing AWS account or register a new one. </p> </li> <li> <p>Select an AWS region in order to deploy the cluster in that region. </p> </li> <li> <p>Add a programmatic access key for a new or existing user. Note that it should be an IAM user with granted administrative permissions. </p> </li> <li> <p>Open <code>bash</code> terminal on the client host. </p> </li> <li> <p>Get an example environment file <code>env</code> to set our AWS credentials:</p> <pre><code>    curl https://raw.githubusercontent.com/shalb/monitoring-examples/main/cdev/monitoring-cluster-blog/env &gt; env\n</code></pre> </li> <li> <p>Add the programmatic access key to the environment file <code>env</code>:</p> <pre><code>    editor env\n</code></pre> </li> </ul>"},{"location":"examples-aws-k3s-prometheus/#create-and-deploy-the-project","title":"Create and deploy the project","text":""},{"location":"examples-aws-k3s-prometheus/#get-example-code","title":"Get example code","text":"<pre><code>mkdir -p cdev &amp;&amp; mv env cdev/ &amp;&amp; cd cdev &amp;&amp; chmod 777 ./\nalias cdev='docker run -it -v $(pwd):/workspace/cluster-dev --env-file=env clusterdev/cluster.dev:v0.6.3'\ncdev project create https://github.com/shalb/cdev-aws-k3s?ref=v0.3.0\ncurl https://raw.githubusercontent.com/shalb/monitoring-examples/main/cdev/monitoring-cluster-blog/stack.yaml &gt; stack.yaml\ncurl https://raw.githubusercontent.com/shalb/monitoring-examples/main/cdev/monitoring-cluster-blog/project.yaml &gt; project.yaml\ncurl https://raw.githubusercontent.com/shalb/monitoring-examples/main/cdev/monitoring-cluster-blog/monitoring.yaml &gt; monitoring.yaml\n</code></pre>"},{"location":"examples-aws-k3s-prometheus/#create-s3-bucket-to-store-the-project-state","title":"Create S3 bucket to store the project state","text":"<p>Go to AWS S3 and create a new bucket. Replace the value of <code>state_bucket_name</code> key in config file <code>project.yaml</code> by the name of the created bucket: </p> <pre><code>editor project.yaml\n</code></pre>"},{"location":"examples-aws-k3s-prometheus/#customize-project-settings","title":"Customize project settings","text":"<p>We will set all the settings needed for our project in the <code>project.yaml</code> config file. All the variables that have <code># example</code> comment in the end of a line should be customized.</p>"},{"location":"examples-aws-k3s-prometheus/#select-aws-region","title":"Select AWS region","text":"<p>The value of the <code>region</code> key in the config file <code>project.yaml</code> should be replaced by our region.</p>"},{"location":"examples-aws-k3s-prometheus/#set-unique-cluster-name","title":"Set unique cluster name","text":"<p>By default we will use the <code>cluster.dev</code> domain as a root domain for cluster ingresses. We should also replace the value of the <code>cluster_name</code> key by a unique string in config file <code>project.yaml</code>, because the default ingress will use it in the resulting DNS name.</p> <p>This command will help us generate a random name and check whether it is in use:</p> <pre><code>CLUSTER_NAME=$(echo \"$(tr -dc a-z0-9 &lt;/dev/urandom | head -c 5)\") \ndig argocd.${CLUSTER_NAME}.cluster.dev | grep -q \"^${CLUSTER_NAME}\" || echo \"OK to use cluster_name: ${CLUSTER_NAME}\"\n</code></pre> <p>If the cluster name is available we should see the message <code>OK to use cluster_name: ...</code></p>"},{"location":"examples-aws-k3s-prometheus/#set-ssh-keys","title":"Set SSH keys","text":"<p>We should have access to cluster nodes via SSH. To add the existing SSH key we should replace the value of <code>public_key</code> key in config file <code>project.yaml</code>. If we have no SSH key, then we should create it.</p>"},{"location":"examples-aws-k3s-prometheus/#set-argo-cd-password","title":"Set Argo CD password","text":"<p>In our project we shall use Argo CD to deploy our applications to the cluster. To secure Argo CD we should replace the value of <code>argocd_server_admin_password</code> key by a unique password in config file <code>project.yaml</code>. The default value is a bcrypted password string.</p> <p>To encrypt our custom password we can use an online tool or encrypt the password by command:</p> <pre><code>alias cdev_bash='docker run -it -v $(pwd):/workspace/cluster-dev --env-file=env --network=host --entrypoint=\"\" clusterdev/cluster.dev:v0.6.3 bash'\ncdev_bash\npassword=$(tr -dc a-zA-Z0-9,._! &lt;/dev/urandom | head -c 20)\napt install -y apache2-utils &amp;&amp; htpasswd -bnBC 10 \"\" ${password} | tr -d ':\\n' ; echo ''\necho \"Password: $password\"\nexit\n</code></pre>"},{"location":"examples-aws-k3s-prometheus/#set-grafana-password","title":"Set Grafana password","text":"<p>Now we are going to add a custom password for Grafana. To secure Grafana we should replace the value of the <code>grafana_password</code> key by a unique password in config file <code>project.yaml</code>. This command will help us generate a random password:</p> <pre><code>echo \"$(tr -dc a-zA-Z0-9,._! &lt;/dev/urandom | head -c 20)\"\n</code></pre>"},{"location":"examples-aws-k3s-prometheus/#run-bash-in-clusterdev-container","title":"Run Bash in Cluster.dev container","text":"<p>To avoid installation of all needed tools directly to the client host, we will run all commands inside the Cluster.dev container. In order to execute Bash inside the Cluster.dev container and proceed to deploy, run the command:</p> <pre><code>cdev_bash\n</code></pre>"},{"location":"examples-aws-k3s-prometheus/#deploy-the-project","title":"Deploy the project","text":"<p>Now we should deploy our project to AWS via <code>cdev</code> command:</p> <pre><code>cdev apply -l debug | tee apply.log\n</code></pre> <p>Where deployment is successful, we should get further instructions on how to access Kubernetes, and the URLs of Argo CD and Grafana web UIs. Sometimes, because of DNS update delays we need to wait a while to access those web UIs. In this instance we can forward all needed services via <code>kubectl</code> to the client host:</p> <pre><code>kubectl port-forward svc/argocd-server -n argocd 18080:443  &gt; /dev/null 2&gt;&amp;1 &amp;\nkubectl port-forward svc/monitoring-grafana -n monitoring 28080:80  &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>We may test our forwards via <code>curl</code>:</p> <pre><code>curl 127.0.0.1:18080\ncurl 127.0.0.1:28080\n</code></pre> <p>If we see no errors from <code>curl</code>, then the client host should access these endpoints via any browser.</p>"},{"location":"examples-aws-k3s-prometheus/#destroy-the-project","title":"Destroy the project","text":"<p>We can delete our cluster with the command: </p> <pre><code>cdev apply -l debug\ncdev destroy -l debug | tee destroy.log\n</code></pre>"},{"location":"examples-aws-k3s-prometheus/#conclusion","title":"Conclusion","text":"<p>In this section we have learnt how to deploy the Prometheus monitoring stack with the Cluster.dev AWS-K3s template. The resulting stack allows us to monitor workloads in our cluster. We can also reuse the stack as a prepared infrastructure pattern to launch environments for testing monitoring cases, before applying them to production.   </p>"},{"location":"examples-aws-k3s/","title":"AWS-K3s","text":"<p>Cluster.dev uses stack templates to generate users' projects in a desired cloud. AWS-K3s is a stack template that creates and provisions Kubernetes clusters in AWS cloud by means of K3s utility. </p> <p>On this page you will find guidance on how to create a K3s cluster on AWS using one of the Cluster.dev prepared samples \u2013 the AWS-K3s stack template. Running the example code will have the following resources created:</p> <ul> <li> <p>K3s cluster with addons:</p> <ul> <li> <p>cert-manager</p> </li> <li> <p>ingress-nginx</p> </li> <li> <p>external-dns</p> </li> <li> <p>argocd</p> </li> </ul> </li> <li> <p>AWS Key Pair to access the cluster running instances</p> </li> <li> <p>AWS IAM Policy for managing your DNS zone by external-dns</p> </li> <li> <p>(optional, if you use cluster.dev domain) Route53 zone .cluster.dev <li> <p>(optional, if vpc_id is not set) VPC for EKS cluster</p> </li>"},{"location":"examples-aws-k3s/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Terraform version 1.4+</p> </li> <li> <p>AWS account</p> </li> <li> <p>AWS CLI installed</p> </li> <li> <p>kubectl installed</p> </li> <li> <p>Cluster.dev client installed</p> </li> </ol>"},{"location":"examples-aws-k3s/#authentication","title":"Authentication","text":"<p>Cluster.dev requires cloud credentials to manage and provision resources. You can configure access to AWS in two ways:</p> <p>Info</p> <p>Please note that you have to use IAM user with granted administrative permissions.</p> <ul> <li> <p>Environment variables: provide your credentials via the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>, the environment variables that represent your AWS Access Key and AWS Secret Key. You can also use the <code>AWS_DEFAULT_REGION</code> or <code>AWS_REGION</code> environment variable to set a region, if needed. Example usage:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"MYACCESSKEY\"\nexport AWS_SECRET_ACCESS_KEY=\"MYSECRETKEY\"\nexport AWS_DEFAULT_REGION=\"eu-central-1\"\n</code></pre> </li> <li> <p>Shared Credentials File (recommended): set up an AWS configuration file to specify your credentials.</p> <p>Credentials file <code>~/.aws/credentials</code> example:</p> <pre><code>[cluster-dev]\naws_access_key_id = MYACCESSKEY\naws_secret_access_key = MYSECRETKEY\n</code></pre> <p>Config: <code>~/.aws/config</code> example:</p> <pre><code>[profile cluster-dev]\nregion = eu-central-1\n</code></pre> <p>Then export <code>AWS_PROFILE</code> environment variable.</p> <pre><code>export AWS_PROFILE=cluster-dev\n</code></pre> </li> </ul>"},{"location":"examples-aws-k3s/#install-aws-client","title":"Install AWS client","text":"<p>If you don't have the AWS CLI installed, refer to the AWS CLI official installation guide, or use commands from the example:</p> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\naws s3 ls\n</code></pre>"},{"location":"examples-aws-k3s/#create-s3-bucket","title":"Create S3 bucket","text":"<p>Cluster.dev uses S3 bucket for storing states. Create the bucket with the command:</p> <pre><code>aws s3 mb s3://cdev-states\n</code></pre>"},{"location":"examples-aws-k3s/#dns-zone","title":"DNS Zone","text":"<p>In AWS-K3s stack template example, you will need to define a Route 53 hosted zone. Options:</p> <ol> <li> <p>You already have a Route 53 hosted zone.</p> </li> <li> <p>Create a new hosted zone using a Route 53 documentation example.</p> </li> <li> <p>Use \"cluster.dev\" domain for zone delegation.</p> </li> </ol>"},{"location":"examples-aws-k3s/#create-project","title":"Create project","text":"<ol> <li> <p>Configure access to AWS and export required variables.</p> </li> <li> <p>Create locally a project directory, cd into it and execute the command:</p> <p><pre><code>  cdev project create https://github.com/shalb/cdev-aws-k3s\n</code></pre> This will create a new, empty project.</p> </li> <li> <p>Edit variables in the example's files, if necessary:</p> <ul> <li> <p><code>project.yaml</code> - main project config. Sets common global variables for the current project, such as organization, region, state bucket name etc. See project configuration docs.</p> </li> <li> <p><code>backend.yaml</code> - configures backend for Cluster.dev states (including Terraform states). Uses variables from <code>project.yaml</code>. See backend docs.</p> </li> <li> <p><code>stack.yaml</code> - describes stack configuration. See stack docs.</p> </li> </ul> </li> <li> <p>Run <code>cdev plan</code> to build the project. In the output you will see an infrastructure that is going to be created after running <code>cdev apply</code>.</p> <p>Note</p> <p>Prior to running <code>cdev apply</code>, make sure to look through the <code>stack.yaml</code> file and replace the commented fields with real values. If you would like to use existing VPC and subnets, uncomment preset options and set correct VPC ID and subnets' IDs. If you leave them as is, Cluster.dev will have VPC and subnets created for you.</p> </li> <li> <p>Run <code>cdev apply</code></p> <p>Tip</p> <p>We highly recommend running <code>cdev apply</code> in a debug mode so that you can see the Cluster.dev logging in the output: <code>cdev apply -l debug</code></p> </li> <li> <p>After <code>cdev apply</code> is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and that the stack template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the <code>stack.yaml</code>.</p> </li> <li> <p>Displayed in the output will also be a command on how to get kubeconfig and connect to your Kubernetes cluster.</p> </li> <li> <p>Destroy the cluster and all created resources with the command <code>cdev destroy</code></p> </li> </ol>"},{"location":"examples-azure-aks/","title":"Azure-AKS","text":"<p>Cluster.dev uses stack templates to generate users' projects in a desired cloud. Azure-AKS is a stack template that creates and provisions Kubernetes clusters in Azure cloud by means of Azure Kubernetes Service (AKS).</p> <p>On this page you will find guidance on how to start an AKS cluster on Azure using one of the Cluster.dev prepared samples \u2013 the Azure-AKS stack template. Running the example code will have the following resources created:</p> <ul> <li> <p>Azure DNS Zone</p> </li> <li> <p>Azure Virtual Network</p> </li> <li> <p>AKS Kubernetes cluster with addons:</p> <ul> <li> <p>cert-manager</p> </li> <li> <p>ingress-nginx</p> </li> <li> <p>external-secrets (with Azure Key Vault backend)</p> </li> <li> <p>external-dns</p> </li> <li> <p>argocd</p> </li> </ul> </li> </ul>"},{"location":"examples-azure-aks/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Terraform version 1.4+</p> </li> <li> <p>Azure account and a subscription</p> </li> <li> <p>Azure CLI installed and configured with your Azure account</p> </li> <li> <p>kubectl installed</p> </li> <li> <p>Cluster.dev client installed</p> </li> <li> <p>Parent Domain</p> </li> </ol>"},{"location":"examples-azure-aks/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Clone example project:     <pre><code>git clone https://github.com/shalb/cdev-azure-aks.git\ncd examples/\n</code></pre></p> </li> <li> <p>Update <code>project.yaml</code>:     <pre><code>name: demo-project\nkind: Project\nbackend: azure-backend\nvariables:\n  location: eastus\n  domain: azure.cluster.dev\n  resource_group_name: cdevResourceGroup\n  state_storage_account_name: cdevstates\n  state_container_name: tfstate\n</code></pre></p> </li> <li> <p>Create the Azure Storage Account and a container for Terraform backend:     <pre><code>az group create --name cdevResourceGroup --location EastUS\naz storage account create --name cdevstates --resource-group cdevResourceGroup --location EastUS --sku Standard_LRS\naz storage container create --name tfstate --account-name cdevstates\n</code></pre></p> </li> <li> <p>It may be necessary to assign the <code>Storage Blob Data Contributor</code> and <code>Storage Queue Data Contributor</code> roles to your user account for the storage account:     <pre><code>STORAGE_ACCOUNT_ID=$(az storage account show --name cdevstates --query id --output tsv)\nUSER_OBJECT_ID=$(az ad signed-in-user show --query id --output tsv)\naz role assignment create --assignee \"$USER_OBJECT_ID\" --role \"Storage Blob Data Contributor\" --scope \"$STORAGE_ACCOUNT_ID\"\naz role assignment create --assignee \"$USER_OBJECT_ID\" --role \"Storage Queue Data Contributor\" --scope \"$STORAGE_ACCOUNT_ID\"\n</code></pre></p> </li> <li> <p>Edit variables in the example's files, if necessary.</p> </li> <li> <p>Run <code>cdev plan</code></p> </li> <li> <p>Run <code>cdev apply</code></p> </li> <li> <p>Set up DNS delegation for the subdomain by creating NS records for the subdomain in parent domain. Run <code>cdev output</code> <pre><code>domain = demo.azure.cluster.dev.\nname_servers = [\n  \"ns1-36.azure-dns.com.\",\n  \"ns2-36.azure-dns.net.\",\n  \"ns3-36.azure-dns.org.\",\n  \"ns4-36.azure-dns.info.\"\n]\n</code></pre>     Add records from the <code>name_server</code> list.</p> </li> <li> <p>Connect to AKS cluster. Run <code>cdev output</code> <pre><code>kubeconfig_cmd = az aks get-credentials --name &lt;aks-cluster-name&gt; --resource-group &lt;aks-cluster-resource-group&gt; --overwrite-existing\n</code></pre>     Execute the command in <code>kubeconfig_cmd</code></p> </li> <li> <p>Retrieve the ArgoCD admin password:     <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret  -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre></p> </li> </ol>"},{"location":"examples-azure-aks/#destroy-sample-architecture","title":"Destroy sample architecture","text":"<ol> <li> <p>Run <code>cdev destroy</code></p> </li> <li> <p>Remove NS records for subdomain in parent domain.</p> </li> <li> <p>Delete Azure Storage Account and a container for Terraform backend:     <pre><code>az group delete --name cdevResourceGroup\n</code></pre></p> </li> </ol>"},{"location":"examples-develop-stack-template/","title":"Develop Stack Template","text":"<p>Cluster.dev gives you freedom to modify existing templates or create your own. You can add inputs and outputs to already preset units, take the output of one unit and send it as an input to another, or write new units and add them to a template.</p> <p>In our example we shall use the tmpl-development sample to create a K3s cluster on AWS. Then we shall modify the project stack template by adding new parameters to the units.</p>"},{"location":"examples-develop-stack-template/#workflow-steps","title":"Workflow steps","text":"<ol> <li> <p>Create a project following the steps described in Create Own Project section.</p> </li> <li> <p>To start working with the stack template, cd into the template directory and open the <code>template.yaml</code> file: ./template/template.yaml.</p> <p>Our sample stack template contains 3 units. Now, let's elaborate on each of them and see how we can modify it.</p> </li> <li> <p>The <code>create-bucket</code> unit uses a remote Terraform module to create an S3 bucket on AWS:</p> <pre><code>name: create-bucket\ntype: tfmodule\nproviders: *provider_aws\nsource: terraform-aws-modules/s3-bucket/aws\nversion: \"2.9.0\"\ninputs:\n  bucket: {{ .variables.bucket_name }}\n  force_destroy: true\n</code></pre> <p>We can modify the unit by adding more parameters in inputs. For example, let's add some tags using the <code>insertYAML</code> function:</p> <pre><code>name: create-bucket\ntype: tfmodule\nproviders: *provider_aws\nsource: terraform-aws-modules/s3-bucket/aws\nversion: \"2.9.0\"\ninputs:\n  bucket: {{ .variables.bucket_name }}\n  force_destroy: true\n  tags: {{ insertYAML .variables.tags }}\n</code></pre> <p>Now we can see the tags in <code>stack.yaml</code>:</p> <pre><code>name: cdev-tests-local\ntemplate: ./template/\nkind: Stack\nbackend: aws-backend\nvariables:\n  bucket_name: \"tmpl-dev-test\"\n  region: {{ .project.variables.region }}\n  organization: {{ .project.variables.organization }}\n  name: \"Developer\"\n  tags:\n    tag1_name: \"tag 1 value\"\n    tag2_name: \"tag 2 value\"\n</code></pre> <p>To check the configuration, run <code>cdev plan --tf-plan</code> command. In the output you can see that Terraform will create a bucket with the defined tags. Run <code>cdev apply -l debug</code> to have the configuration applied.</p> </li> <li> <p>The <code>create-s3-object</code> unit uses local Terraform module to get the bucket ID and save data inside the bucket. The Terraform module is stored in s3-file directory, <code>main.tf</code> file:</p> <pre><code>name: create-s3-object\ntype: tfmodule\nproviders: *provider_aws\nsource: ./s3-file/\ndepends_on: this.create-bucket\ninputs:\n  bucket_name: {{ remoteState \"this.create-bucket.s3_bucket_id\" }}\n  data: |\n    The data that will be saved in the S3 bucket after being processed by the template engine.\n    Organization: {{ .variables.organization }}\n    Name: {{ .variables.name }}\n</code></pre> <p>The unit sends 2 parameters. The bucket_name is retrieved from the <code>create-bucket</code> unit by means of <code>remoteState</code> function. The data parameter uses templating to retrieve the Organization and Name variables from <code>stack.yaml</code>. </p> <p>Let's add to data input bucket_regional_domain_name variable to obtain the region-specific domain name of the bucket:</p> <pre><code>name: create-s3-object\ntype: tfmodule\nproviders: *provider_aws\nsource: ./s3-file/\ndepends_on: this.create-bucket\ninputs:\n  bucket_name: {{ remoteState \"this.create-bucket.s3_bucket_id\" }}\n  data: |\n    The data that will be saved in the s3 bucket after being processed by the template engine.\n    Organization: {{ .variables.organization }}\n    Name: {{ .variables.name }}\n    Bucket regional domain name: {{ remoteState \"this.create-bucket.s3_bucket_bucket_regional_domain_name\" }}\n</code></pre> <p>Check the configuration by running <code>cdev plan</code> command; apply it with <code>cdev apply -l debug</code>.</p> </li> <li> <p>The <code>print_outputs</code> unit retrieves data from two other units by means of <code>remoteState</code> function: bucket_domain variable from <code>create-bucket</code> unit and s3_file_info from <code>create-s3-object</code> unit:</p> <pre><code>name: print_outputs\ntype: printer\ninputs:\n  bucket_domain: {{ remoteState \"this.create-bucket.s3_bucket_bucket_domain_name\" }}\n  s3_file_info: \"To get file use: aws s3 cp {{ remoteState \"this.create-s3-object.file_s3_url\" }} ./my_file &amp;&amp; cat my_file\"\n</code></pre> </li> <li> <p>Having finished your work, run <code>cdev destroy</code> to eliminate the created resources. </p> </li> </ol>"},{"location":"examples-do-k8s/","title":"DO-K8s","text":"<p>Cluster.dev uses stack templates to generate users' projects in a desired cloud. DO-K8s is a stack template that creates and provisions Kubernetes clusters in the DigitalOcean cloud.</p> <p>On this page you will find guidance on how to create a Kubernetes cluster on DigitalOcean using one of the Cluster.dev prepared samples \u2013 the DO-K8s stack template. Running the example code will have the following resources created:</p> <ul> <li> <p>DO Kubernetes cluster with addons:</p> <ul> <li> <p>cert-manager</p> </li> <li> <p>argocd</p> </li> </ul> </li> <li> <p>(optional, if vpc_id is not set) VPC for Kubernetes cluster</p> </li> </ul>"},{"location":"examples-do-k8s/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Terraform version 1.4+</p> </li> <li> <p>DigitalOcean account</p> </li> <li> <p>doctl installed</p> </li> <li> <p>Cluster.dev client installed</p> </li> </ol>"},{"location":"examples-do-k8s/#authentication","title":"Authentication","text":"<p>Create an access token for a user.</p> <p>Info</p> <p>Make sure to grant the user with administrative permissions.</p> <p>For details on using DO spaces bucket as a backend, see here.</p>"},{"location":"examples-do-k8s/#do-access-configuration","title":"DO access configuration","text":"<ol> <li> <p>Install <code>doctl</code>. For more information, see the official documentation.</p> <pre><code>cd ~\nwget https://github.com/digitalocean/doctl/releases/download/v1.57.0/doctl-1.57.0-linux-amd64.tar.gz\ntar xf ~/doctl-1.57.0-linux-amd64.tar.gz\nsudo mv ~/doctl /usr/local/bin\n</code></pre> </li> <li> <p>Export your DIGITALOCEAN_TOKEN - for details see here.</p> <pre><code>export DIGITALOCEAN_TOKEN=\"MyDIGITALOCEANToken\"\n</code></pre> </li> <li> <p>Export SPACES_ACCESS_KEY_ID and SPACES_SECRET_ACCESS_KEY environment variables - for details see here.</p> <pre><code>export SPACES_ACCESS_KEY_ID=\"dSUGdbJqa6xwJ6Fo8qV2DSksdjh...\"\nexport SPACES_SECRET_ACCESS_KEY=\"TEaKjdj8DSaJl7EnOdsa...\"\n</code></pre> </li> <li> <p>Create a spaces bucket for Terraform states in the chosen region (in the example we used the 'cdev-data' bucket name).</p> </li> <li> <p>Create a domain in DigitalOcean domains service.</p> </li> </ol> <p>Info</p> <p>In the project generated by default we used 'k8s.cluster.dev' zone as an example. Please make sure to change it.</p>"},{"location":"examples-do-k8s/#create-project","title":"Create project","text":"<ol> <li> <p>Configure access to DigitalOcean and export required variables.</p> </li> <li> <p>Create locally a project directory, cd into it and execute the command:</p> <p><pre><code>  cdev project create https://github.com/shalb/cdev-do-k8s\n</code></pre> This will create a new, empty project.</p> </li> <li> <p>Edit variables in the example's files, if necessary:</p> <ul> <li> <p><code>project.yaml</code> - main project config. Sets common global variables for the current project, such as organization, region, state bucket name etc. See project configuration docs.</p> </li> <li> <p><code>backend.yaml</code> - configures backend for Cluster.dev states (including Terraform states). Uses variables from <code>project.yaml</code>. See backend docs.</p> </li> <li> <p><code>stack.yaml</code> - describes stack configuration. See stack docs.</p> </li> </ul> </li> <li> <p>Run <code>cdev plan</code> to build the project. In the output you will see an infrastructure that is going to be created after running <code>cdev apply</code>.</p> <p>Note</p> <p>Prior to running <code>cdev apply</code>, make sure to look through the <code>stack.yaml</code> file and replace the commented fields with real values. In case you would like to use existing VPC and subnets, uncomment preset options and set correct VPC ID and subnets' IDs. If you leave them as is, Cluster.dev will have VPC and subnets created for you.</p> </li> <li> <p>Run <code>cdev apply</code></p> <p>Tip</p> <p>We highly recommend running <code>cdev apply</code> in a debug mode so that you can see the Cluster.dev logging in the output: <code>cdev apply -l debug</code></p> </li> <li> <p>After <code>cdev apply</code> is successfully executed, in the output you will see the ArgoCD URL of your cluster. Sign in to the console to check whether ArgoCD is up and running and that the stack template has been deployed correctly. To sign in, use the \"admin\" login and the bcrypted password that you have generated for the <code>stack.yaml</code>.</p> </li> <li> <p>Displayed in the output will also be a command on how to get kubeconfig and connect to your Kubernetes cluster.</p> </li> <li> <p>Destroy the cluster and all created resources with the command <code>cdev destroy</code></p> </li> </ol>"},{"location":"examples-eks-model/","title":"Kubernetes infrastructure for Hugging Face models and chat on AWS","text":"<p>In this section we will use Cluster.dev to launch an LLM from the Hugging Face Hub with chat on top of Kubernetes, and make it production-ready. </p> <p>While we'll demonstrate the workflow using Amazon AWS cloud and managed EKS, it can be adapted for any other cloud provider and Kubernetes version.</p>"},{"location":"examples-eks-model/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>AWS cloud account credentials.</p> </li> <li> <p>AWS Quota change requested for g5 or other desired types of GPU instances.</p> </li> <li> <p>Cluster.dev and Terraform installed.</p> </li> <li> <p>Selected Hugging Face model with .safetensors weights from Hub. Alternatively, you can upload the model to an S3 bucket; see the example in bootstrap.ipynb.</p> </li> <li> <p>Route53 DNS zone (optional).</p> </li> </ul> <p>Create an S3 Bucket for storing state files:</p> <pre><code>aws s3 mb s3://cdev-states\n</code></pre> <p>Clone the repository with the example:</p> <pre><code>git clone https://github.com/shalb/cdev-examples/\ncd cdev-examples/aws/eks-model/cluster.dev/\n</code></pre>"},{"location":"examples-eks-model/#edit-configuration-files","title":"Edit Configuration files","text":"<p><code>project.yaml</code> - the main project configuration that sets common global variables for the current project, such as organization, region, state bucket name, etc. It can also be used to set global environment variables.</p> <p><code>backend.yaml</code> - configures the backend for Cluster.dev states (including Terraform states) and uses variables from <code>project.yaml</code>.</p> <p><code>stack-eks.yaml</code> - describes AWS infrastructure configuration, including VPC, Domains, and EKS (Kubernetes) settings. Refer to the Stack docs.</p> <p>The main configuration here is about your GPU nodes. Define their capacity_type (ON_DEMAND, SPOT), instance types, and autoscaling (min/max/desired) settings. Also, configure disk size and node labels if needed. The most important settings to configure next are:</p> <pre><code>  cluster_name: k8s-model # change this to your cluster name\n  domain: cluster.dev # if you leave this domain it will be auto-delegated with the zone *.cluster_name.cluster.dev\n  eks_managed_node_groups:\n    gpu-nodes:\n      name: ondemand-gpu-nodes\n      capacity_type: ON_DEMAND\n      block_device_mappings:\n        xvda:\n          device_name: \"/dev/xvda\"\n          ebs:\n            volume_size: 120\n            volume_type: \"gp3\"\n            delete_on_termination: true\n      instance_types:\n        - \"g5.xlarge\"\n      labels:\n        gpu-type: \"a10g\"\n      max_size: 1\n      desired_size: 1\n      min_size: 0\n</code></pre> <p>You can create any additional node groups by adding similar blocks to this YAML. The complete list of available settings can be found in the relative Terraform module.</p> <p><code>stack-model.yaml</code> - describes the HF model Stack. It refers to the model stackTemplate in the <code>model-template</code> folder, and it also installs the required Nvidia drivers.</p> <p>The model stack mostly uses the values from the huggingface-model Helm chart, which we have prepared and continue to develop. The list of all available options for the chart can be checked in the default values.yaml. Here are the main things you need to change:</p> <pre><code>  chart:\n    model:\n      organization: \"HuggingFaceH4\"\n      name: \"zephyr-7b-beta\"\n    init:\n      s3:\n        enabled: false # if set to false the model would be cloned directly from HF git space\n        bucketURL: s3://k8s-model-zephyr/llm/deployment/zephyr-7b-beta  # see ../bootstrap.ipynb on how to upload model\n    huggingface:\n      args:\n        - \"--max-total-tokens\"\n        - \"4048\"\n        #- --quantize\n        #- \"awq\"\n    replicaCount: 1\n    persistence:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: gp2\n      storage: 100Gi\n    resources:\n      requests:\n        cpu: \"2\"\n        memory: \"8Gi\"\n      limits:\n        nvidia.com/gpu: 1\n    chat:\n      enabled: true\n</code></pre>"},{"location":"examples-eks-model/#deploy-stacks","title":"Deploy Stacks","text":"<p>After you finish the configuration, you can deploy everything with just one command:</p> <pre><code>cdev apply\n</code></pre> <p>Under the hood, it would create the following resources:</p> <pre><code>Plan results:\n+----------------------------+\n|      WILL BE DEPLOYED      |\n+----------------------------+\n| cluster.route53            |\n| cluster.vpc                |\n| cluster.eks                |\n| cluster.eks-addons         |\n| cluster.kubeconfig         |\n| cluster.outputs            |\n| model.nvidia-device-plugin |\n| model.model                |\n| model.outputs              |\n+----------------------------+\n</code></pre> <p>The process takes around 30 minutes to complete; check this video to get an idea:</p> <p></p>"},{"location":"examples-eks-model/#working-with-infrastructure","title":"Working with Infrastructure","text":"<p>So, let's imagine some tasks that we can perform on top of the stack.</p>"},{"location":"examples-eks-model/#interacting-with-kubernetes","title":"Interacting with Kubernetes","text":"<p>After the stack is deployed, you will get the <code>kubeconfig</code> file that can be used for authorization to the cluster, checking workloads, logs, etc.:</p> <pre><code># First we need to export KUBECONFIG to use kubectl\nexport KUBECONFIG=`pwd`/kubeconfig\n# Then we can examine workloads deployed in the `default` namespace, since we have defined it in the stack-model.yaml\nkubectl get pod\n# To get logs from model startup, check if the model is loaded without errors\nkubectl logs -f &lt;output model pod name from kubectl get pod&gt;\n# To list services (should be model, chat and mongo if chat is enabled)\nkubectl get svc\n# Then you can port-forward the service to your host\nkubectl port-forward svc/&lt;model-output from above&gt;  8080:8080\n# Now you can chat with your model\ncurl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"Continue funny story: John decide to stick finger into outlet\",\"parameters\":{\"max_new_tokens\":1000}}' \\\n    -H 'Content-Type: application/json'\n</code></pre>"},{"location":"examples-eks-model/#changing-node-size-and-type","title":"Changing node size and type","text":"<p>Let's imagine we have a large model, and we need to serve it with some really large instances. But we'd like to use spot instances which are cheaper. So we need to change the type of the node group:</p> <pre><code>    gpu-nodes:\n      name: spot-gpu-nodes\n      capacity_type: SPOT\n      block_device_mappings:\n        xvda:\n          device_name: \"/dev/xvda\"\n          ebs:\n            volume_size: 120\n            volume_type: \"gp3\"\n            delete_on_termination: true\n      instance_types:\n        - \"g5.12xlarge\"\n      labels:\n        gpu-type: \"a10g\"\n      max_size: 1\n      desired_size: 1\n      min_size: 0\n</code></pre> <p>And then apply the changes by running <code>cdev apply</code>.</p> <p>Please note that spot instances are not always available in the region. If the spot request can not be fulfilled, you can check in your AWS Console under EC2 -&gt; Auto Scaling groups -&gt; eks-spot-gpu-nodes -&gt; Activity. If it fails, try changing to <code>ON_DEMAND</code> or modify instance_types in the manifest and rerun <code>cdev apply</code>.</p>"},{"location":"examples-eks-model/#changing-the-model","title":"Changing the model","text":"<p>In case you need to change the model, simply edit its name and organization. Then apply the changes by running <code>cdev apply</code>:</p> <pre><code>    model:\n      organization: \"WizardLM\"\n      name: \"WizardCoder-15B-V1.0\"\n</code></pre>"},{"location":"examples-eks-model/#enabling-chat-ui","title":"Enabling Chat-UI","text":"<p>To enable Chat-UI, simply set <code>chart.chat.enable:true</code>. You will get a service that can be port-forwarded and used from the browser. If you need to expose the chat to external users, add ingress configuration, as shown in the sample:</p> <pre><code>    chat:\n      enabled: true\n      modelConfig:\n      extraEnvVars:\n        - name: PUBLIC_ORIGIN\n          value: \"http://localhost:8080\"\n      ingress:\n        enabled: true\n        annotations:\n          cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n        hosts:\n          - host: chat.k8s-model.cluster.dev\n            paths:\n              - path: /\n                pathType: Prefix\n        tls:\n          - hosts:\n              - chat.k8s-model.cluster.dev\n            secretName: huggingface-model-chat\n</code></pre> <p>Note that if you are using the <code>cluster.dev</code> domain with your project prefix (please make it unique), the DNS zone will be auto-configured. HTTPS certificates for the domain will also be generated automatically. To check the progress use the command: <code>kubectl describe certificaterequests.cert-manager.io</code></p> <p>If you'd like to expose the API for your model, set the Ingress in the corresponding model section.</p>"},{"location":"examples-eks-model/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"<p>You can find the instructions for setting-up Prometheus and Grafana in bootstrap.ipynb. We are planning to release a new stack template with monitoring enabled through a single option.</p> <p>In this loom video, you can see the configuration for Grafana:</p> <p></p>"},{"location":"examples-eks-model/#questions-help-and-feature-requests","title":"Questions, Help and Feature Requests","text":"<p>Feel free to use GitHub repository Discussions.</p>"},{"location":"examples-external-articles/","title":"External Articles","text":"<ul> <li> <p>Cluster.dev: All-in-One Tool for Cloud Native Infrastructure</p> </li> <li> <p>Mastering Cloud Infrastructure with Cluster.dev</p> </li> <li> <p>Revolutionizing Infrastructure Management with Cluster.dev: A Journey into Effortless Orchestration</p> </li> <li> <p>Discover Cluster.dev: A Simple Solution for Cloud Infrastructure Management</p> </li> <li> <p>Streamlining SonarQube on AWS ECS: Simplified Deployment Using Cluster.dev</p> </li> <li> <p>Deploy Kubernetes in Minutes: Effortless Infrastructure Creation and Application Deployment with Cluster.dev and Helm Charts</p> </li> <li> <p>Cluster.dev: Is it the right Kubernetes Bootstrapping tool for you?</p> </li> <li> <p>Cluster.dev: Expanding the Options for SaaS Deployment</p> </li> </ul>"},{"location":"examples-gcp-gke/","title":"GCP-GKE","text":"<p>Cluster.dev uses stack templates to generate users' projects in a desired cloud. GCP-GKE is a stack template that creates and provisions Kubernetes clusters in GCP cloud by means of Google Kubernetes Engine (GKE).</p> <p>On this page you will find guidance on how to create a GKE cluster on GCP using one of the Cluster.dev prepared samples \u2013 the GCP-GKE stack template. Running the example code will have the following resources created:</p> <ul> <li> <p>VPC</p> </li> <li> <p>GKE Kubernetes cluster with addons:</p> <ul> <li> <p>cert-manager</p> </li> <li> <p>ingress-nginx</p> </li> <li> <p>external-secrets (with GCP Secret Manager backend)</p> </li> <li> <p>external-dns</p> </li> <li> <p>argocd</p> </li> </ul> </li> </ul>"},{"location":"examples-gcp-gke/#prerequisites","title":"Prerequisites","text":"<ol> <li>Terraform version &gt;= 1.4</li> <li>GCP account and project</li> <li>GCloud CLI installed and configured with your GCP account</li> <li>kubectl installed</li> <li>Cluster.dev client installed</li> <li>Parent Domain</li> </ol>"},{"location":"examples-gcp-gke/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Create or select a Google Cloud project: <pre><code>gcloud projects create cdev-demo\ngcloud config set project cdev-demo\n</code></pre></p> </li> <li> <p>Enable billing for your project.</p> </li> <li> <p>Enable the Google Kubernetes Engine API.</p> </li> <li> <p>Enable Secret Manager:    <pre><code>gcloud services enable secretmanager.googleapis.com\n</code></pre></p> </li> </ol>"},{"location":"examples-gcp-gke/#quick-start","title":"Quick Start","text":"<ol> <li>Clone example project:     <pre><code>git clone https://github.com/shalb/cdev-gcp-gke.git\ncd examples/\n</code></pre></li> <li>Update <code>project.yaml</code>:     <pre><code>name: demo-project\nkind: Project\nbackend: gcs-backend\nvariables:\n  organization: my-organization\n  project: cdev-demo\n  region: us-west1\n  state_bucket_name: gke-demo-state\n  state_bucket_prefix: demo\n</code></pre></li> <li>Create GCP bucket for Terraform backend:     <pre><code>gcloud projects create cdev-demo\ngcloud config set project cdev-demo\ngsutil mb gs://gke-demo-state\n</code></pre></li> <li>Edit variables in the example's files, if necessary.</li> <li>Run <code>cdev plan</code></li> <li>Run <code>cdev apply</code></li> <li> <p>Set up DNS delegation for subdomain by creating    NS records for subdomain in parent domain.    Run <code>cdev output</code>:    <pre><code>cdev output\n12:58:52 [INFO] Printer: 'cluster.outputs', Output:\ndomain = demo.gcp.cluster.dev.\nname_server = [\n  \"ns-cloud-d1.googledomains.com.\",\n  \"ns-cloud-d2.googledomains.com.\",\n  \"ns-cloud-d3.googledomains.com.\",\n  \"ns-cloud-d4.googledomains.com.\"\n]\nregion = us-west1\n</code></pre>    Add records from name_server list.</p> </li> <li> <p>Authorize cdev/Terraform to interact with GCP via SDK:     <pre><code>gcloud auth application-default login\n</code></pre></p> </li> <li>Connect to GKE cluster:     <pre><code>gcloud components install gke-gcloud-auth-plugin\ngcloud container clusters get-credentials demo-cluster --zone us-west1-a --project cdev-demo\n</code></pre></li> <li>Retrieve the ArgoCD admin password,    install the ArgoCD CLI:     <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret  -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre></li> </ol>"},{"location":"examples-modify-aws-eks/","title":"Modify AWS-EKS","text":"<p>The code and the text prepared by Orest Kapko, a DevOps engineer at SHALB. </p> <p>In this section we shall customize the basic AWS-EKS Cluster.dev template in order to add some features.</p>"},{"location":"examples-modify-aws-eks/#workflow-steps","title":"Workflow steps","text":"<ol> <li> <p>Go to the GitHub page via the AWS-EKS link and download the stack template.</p> </li> <li> <p>If you are not planning to use some preset addons, edit <code>aws-eks.yaml</code> to exclude them. In our case, it was cert-manager, cert-manager-issuer, ingress-nginx, argocd, and argocd_apps.</p> </li> <li> <p>In order to dynamically retrieve the AWS account ID parameter, we have added a data block to our stack template:</p> <pre><code>  - name: data\n    type: tfmodule\n    providers: *provider_aws\n    depends_on: this.eks\n    source: ./terraform-submodules/data/\n</code></pre> <pre><code>{{ remoteState \"this.data.account_id\" }}\n</code></pre> <p>The block is also used in eks_auth ConfigMap and expands its functionality with groups of users:</p> <pre><code>  apiVersion: v1\n  data:\n    mapAccounts: |\n      []\n    mapRoles: |\n      - \"groups\":\n        - \"system:bootstrappers\"\n        - \"system:nodes\"\n        \"rolearn\": \"{{ remoteState \"this.eks.worker_iam_role_arn\" }}\"\n        \"username\": \"system:node:{{ \"{{EC2PrivateDNSName}}\" }}\"\n    - \"groups\":\n      - \"system:masters\"\n      \"rolearn\": \"arn:aws:iam::{{ remoteState \"this.data.account_id\" }}:role/OrganizationAccountAccessRole\"\n      \"username\": \"general-role\"\n    mapUsers: |\n      - \"groups\":\n        - \"system:masters\"\n        \"userarn\": \"arn:aws:iam::{{ remoteState \"this.data.account_id\" }}:user/jenkins-eks\"\n        \"username\": \"jenkins-eks\"\n  kind: ConfigMap\n  metadata:\n    name: aws-auth\n    namespace: kube-system\n</code></pre> <p>The data block configuration in main.tf: <code>data \"aws_caller_identity\" \"current\" {}</code></p> <p>In output.tf:</p> <p><code>yaml output \"account_id\" {   value = data.aws_caller_identity.current.account_id }</code></p> </li> <li> <p>As it was decided to use Traefik Ingress controller instead of basic Nginx, we spun up two load balancers (first - internet-facing ALB for public ingresses, and second - internal ALB for private ingresses) and security groups necessary for its work, and described them in albs unit. The unit configuration within the template is as follows:</p> <pre><code>{{- if .variables.ingressControllerEnabled }}\n- name: albs\n  type: tfmodule\n  providers: *provider_aws\n  source: ./terraform-submodules/albs/\n  inputs:\n    main_domain: {{ .variables.alb_main_domain }}\n    main_external_domain: {{ .variables.alb_main_external_domain }}\n    main_vpc: {{ .variables.vpc_id }}\n    acm_external_certificate_arn: {{ .variables.alb_acm_external_certificate_arn }}\n    private_subnets: {{ insertYAML .variables.private_subnets }}\n    public_subnets: {{ insertYAML .variables.public_subnets }}\n    environment: {{ .name }}\n{{- end }}\n</code></pre> </li> <li> <p>We have also created a dedicated unit for testing Ingress through Route 53 records:</p> <pre><code>data \"aws_route53_zone\" \"existing\" {\n  name         = var.domain\n  private_zone = var.private_zone\n}\nmodule \"records\" {\n  source  = \"terraform-aws-modules/route53/aws//modules/records\"\n  version = \"~&gt; 2.0\"\n  zone_id      = data.aws_route53_zone.existing.zone_id\n  private_zone = var.private_zone\n  records = [\n    {\n      name    = \"test-ingress-eks\"\n      type    = \"A\"\n      alias   = {\n        name    = var.private_lb_dns_name\n        zone_id = var.private_lb_zone_id\n        evaluate_target_health = false\n      }\n    },\n    {\n      name    = \"test-ingress-2-eks\"\n      type    = \"A\"\n      alias   = {\n        name    = var.private_lb_dns_name\n        zone_id = var.private_lb_zone_id\n        evaluate_target_health = false\n      }\n    }\n  ]\n}\n</code></pre> <p>The unit configuration within the template:</p> <pre><code> {{- if .variables.ingressControllerRoute53Enabled }}\n - name: route53_records\n   type: tfmodule\n   providers: *provider_aws\n   source: ./terraform-submodules/route53_records/\n   inputs:\n     private_zone: {{ .variables.private_zone }}\n     domain: {{ .variables.domain }}\n     private_lb_dns_name: {{ remoteState \"this.albs.eks_ingress_lb_dns_name\" }}\n     public_lb_dns_name: {{ remoteState \"this.albs.eks_public_lb_dns_name\" }}\n     private_lb_zone_id: {{ remoteState \"this.albs.eks_ingress_lb_zone_id\" }}\n{{- end }}\n</code></pre> </li> <li> <p>Also, to map service accounts to AWS IAM roles we have created a separate template for IRSA. Example configuration for a cluster autoscaler: </p> <pre><code>  kind: StackTemplate\n  name: aws-eks\n  units:\n    {{- if .variables.cluster_autoscaler_irsa.enabled }}\n    - name: iam_assumable_role_autoscaling_autoscaler\n      type: tfmodule\n      source: \"terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc\"\n      version: \"~&gt; 3.0\"\n      providers: *provider_aws\n      inputs:\n        role_name: \"eks-autoscaling-autoscaler-{{ .variables.cluster_name }}\"\n        create_role: true\n        role_policy_arns:\n          - {{ remoteState \"this.iam_policy_autoscaling_autoscaler.arn\" }}\n        oidc_fully_qualified_subjects: {{ insertYAML .variables.cluster_autoscaler_irsa.subjects }}\n        provider_url: {{ .variables.provider_url }}\n    - name: iam_policy_autoscaling_autoscaler\n      type: tfmodule\n      source: \"terraform-aws-modules/iam/aws//modules/iam-policy\"\n      version: \"~&gt; 3.0\"\n      providers: *provider_aws\n      inputs:\n        name: AllowAutoScalingAccessforClusterAutoScaler-{{ .variables.cluster_name }}\n        policy: {{ insertYAML .variables.cluster_autoscaler_irsa.policy }}\n    {{- end }}\n</code></pre> </li> </ol> <p>In our example we have modified the prepared AWS-EKS stack template by adding a customized data block and excluding some addons. </p> <p>We have also changed the template's structure by placing the Examples directory into a separate repository, in order to decouple the abstract template from its implementation for concrete setups. This enabled us to use the template via Git and mark the template's version with Git tags. </p>"},{"location":"examples-overview/","title":"Overview","text":"<p>In the Examples section you will find ready-to-use Cluster.dev samples that will help you bootstrap cloud infrastructures. Running the sample code will get you a provisioned Kubernetes cluster with add-ons in the cloud. The available options include:</p> <ul> <li> <p>EKS cluster in AWS </p> </li> <li> <p>K3s cluster in AWS</p> </li> <li> <p>AKS cluster in Azure</p> </li> <li> <p>GKE cluster in GCP</p> </li> <li> <p>Kubernetes cluster in DigitalOcean </p> </li> <li> <p>Dedicated monitoring cluster in AWS </p> </li> <li> <p>Launching a chat-enabled Hugging Face LLM on top of EKS cluster in AWS</p> </li> </ul> <p>You will also find examples on how to customize the existing templates in order to expand their functionality: </p> <ul> <li>Modify AWS-EKS template</li> </ul> <p>Also please check our Medium blog</p> <ul> <li> <p>GitOps for Terraform and Helm with Cluster.dev</p> </li> <li> <p>Building UI for DevOps operations with Cluster.dev and Streamlit</p> </li> </ul>"},{"location":"generators-overview/","title":"Overview","text":"<p>Generators are part of the Cluster.dev functionality. They enable users to create parts of infrastructure just by filling stack variables in script dialogues, with no infrastructure coding required. This simplifies the creation of new stacks for developers who may lack the Ops skills, and could be useful for quick infrastructure deployment from ready parts (units).</p> <p>Generators create project from a preset profile - a set of data predefined as a project, with variables for stack template. Each template may have a profile for generator, which is stored in the .cdev-metadata/generator directory.</p>"},{"location":"generators-overview/#how-it-works","title":"How it works","text":"<p>Generator creates <code>backend.yaml</code>, <code>project.yaml</code>, <code>infra.yaml</code> by populating the files with user-entered values. The asked-for stack variables are listed in config.yaml under options:</p> <pre><code>  options:\n    - name: name\n      description: Project name\n      regex: \"^[a-zA-Z][a-zA-Z_0-9\\\\-]{0,32}$\"\n      default: \"demo-project\"\n    - name: organization\n      description: Organization name\n      regex: \"^[a-zA-Z][a-zA-Z_0-9\\\\-]{0,64}$\"\n      default: \"my-organization\"\n    - name: region\n      description: DigitalOcean region\n      regex: \"^[a-zA-Z][a-zA-Z_0-9\\\\-]{0,32}$\"\n      default: \"ams3\"\n    - name: domain\n      description: DigitalOcean DNS zone domain name\n      regex: \"^[a-zA-Z0-9][a-zA-Z0-9-\\\\.]{1,61}[a-zA-Z0-9]\\\\.[a-zA-Z]{2,}$\"\n      default: \"cluster.dev\"\n    - name: bucket_name\n      description: DigitalOcean spaces bucket name for states\n      regex: \"^[a-zA-Z][a-zA-Z0-9\\\\-]{0,64}$\"\n      default: \"cdev-state\"\n</code></pre> <p>In options you can define default parameters and add other variables to the generator's list. The variables included by default are project name, organization name, region, domain and bucket name.</p> <p>In config.yaml you can also define a help message text.</p>"},{"location":"get-started-cdev-aws/","title":"Getting Started with Cluster.dev on AWS","text":"<p>This guide will walk you through the steps to deploy your first project with Cluster.dev on AWS.</p> <pre><code>                          +-------------------------+\n                          | Project.yaml            |\n                          |  - region               |\n                          +------------+------------+\n                                       |\n                                       |\n                          +------------v------------+\n                          | Stack.yaml              |\n                          |  - bucket_name          |\n                          |  - region               |\n                          |  - content              |\n                          +------------+------------+\n                                       |\n                                       |\n+--------------------------------------v-----------------------------------------+\n| StackTemplate: s3-website                                                      |\n|                                                                                |\n|  +---------------------+     +-------------------------+     +--------------+  |\n|  | bucket              |     | web-page-object         |     | outputs      |  |\n|  | type: tfmodule      |     | type: tfmodule          |     | type: printer|  |\n|  | inputs:             |     | inputs:                 |     | outputs:     |  |\n|  |  bucket_name        |     | bucket (from bucket ID) |     | websiteUrl   |  |\n|  |  region             |     | content                 |     +--------------+  |\n|  |  website settings   |     |                         |             |         |\n|  +---------------------+     +-----------^-------------+             |         |\n|        |                          | bucket ID                        |         |\n|        |                          | via remoteState                  |         |\n+--------|--------------------------|----------------------------------|---------+\n         |                          |                                  |\n         v                          v                                  v\n   AWS S3 Bucket              AWS S3 Object (index.html)       WebsiteUrl Output\n</code></pre>"},{"location":"get-started-cdev-aws/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following are installed and set up:</p> <ul> <li>Terraform: Version 1.4 or above. Install Terraform.</li> </ul> <pre><code>terraform --version\n</code></pre> <ul> <li>AWS CLI:</li> </ul> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\naws --version\n</code></pre> <ul> <li>Cluster.dev client:</li> </ul> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\ncdev --version\n</code></pre>"},{"location":"get-started-cdev-aws/#authentication","title":"Authentication","text":"<p>Choose one of the two methods below:</p> <ol> <li> <p>Shared Credentials File (recommended):</p> <ul> <li> <p>Populate <code>~/.aws/credentials</code>:</p> <pre><code>[cluster-dev]\naws_access_key_id = YOUR_AWS_ACCESS_KEY\naws_secret_access_key = YOUR_AWS_SECRET_KEY\n</code></pre> </li> <li> <p>Configure <code>~/.aws/config</code>:</p> <pre><code>[profile cluster-dev]\nregion = eu-central-1\n</code></pre> </li> <li> <p>Set the AWS profile:</p> <pre><code>export AWS_PROFILE=cluster-dev\n</code></pre> </li> </ul> </li> <li> <p>Environment Variables:</p> </li> </ol> <pre><code>export AWS_ACCESS_KEY_ID=\"YOUR_AWS_ACCESS_KEY\"\nexport AWS_SECRET_ACCESS_KEY=\"YOUR_AWS_SECRET_KEY\"\nexport AWS_DEFAULT_REGION=\"eu-central-1\"\n</code></pre>"},{"location":"get-started-cdev-aws/#creating-an-s3-bucket-for-storing-state","title":"Creating an S3 Bucket for Storing State","text":"<pre><code>aws s3 mb s3://cdev-states\n</code></pre>"},{"location":"get-started-cdev-aws/#setting-up-your-project","title":"Setting Up Your Project","text":""},{"location":"get-started-cdev-aws/#project-configuration-projectyaml","title":"Project Configuration (<code>project.yaml</code>)","text":"<ul> <li>Defines the overarching project settings. All subsequent stack configurations will inherit and can override these settings.</li> <li>It points to aws-backend as the backend, meaning that the Cluster.dev state for resources defined in this project will be stored in the S3 bucket specified in <code>backend.yaml</code>.</li> <li>Project-level variables are defined here and can be referenced in other configurations.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; project.yaml\nname: dev\nkind: Project\nbackend: aws-backend\nvariables:\n  organization: cluster.dev\n  region: eu-central-1\n  state_bucket_name: cdev-states\nEOF\n</code></pre>"},{"location":"get-started-cdev-aws/#backend-configuration-backendyaml","title":"Backend Configuration (<code>backend.yaml</code>)","text":"<p>This specifies where Cluster.dev will store its own state and the Terraform states for any infrastructure it provisions or manages. Given the backend type as S3, it's clear that AWS is the chosen cloud provider.</p> <pre><code>cat &lt;&lt;EOF &gt; backend.yaml\nname: aws-backend\nkind: Backend\nprovider: s3\nspec:\n  bucket: {{ .project.variables.state_bucket_name }}\n  region: {{ .project.variables.region }}\nEOF\n</code></pre>"},{"location":"get-started-cdev-aws/#stack-configuration-stackyaml","title":"Stack Configuration (<code>stack.yaml</code>)","text":"<ul> <li>This represents a distinct set of infrastructure resources to be provisioned.</li> <li>It references a local template (in this case, the previously provided stack template) to know what resources to create.</li> <li>Variables specified in this file will be passed to the Terraform modules called in the template.</li> <li>The content variable here is especially useful; it dynamically populates the content of an S3 bucket object (a webpage in this case) using the local <code>index.html</code> file.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; stack.yaml\nname: s3-website\ntemplate: ./template/\nkind: Stack\nbackend: aws-backend\nvariables:\n  bucket_name: \"tmpl-dev-test\"\n  region: {{ .project.variables.region }}\n  content: |\n    {{- readFile \"./files/index.html\" | nindent 4 }}\nEOF\n</code></pre>"},{"location":"get-started-cdev-aws/#stack-template-templateyaml","title":"Stack Template (<code>template.yaml</code>)","text":"<p>The <code>StackTemplate</code> serves as a pivotal object within Cluster.dev. It lays out the actual infrastructure components you intend to provision using Terraform modules and resources. Essentially, it determines how your cloud resources should be laid out and interconnected.</p> <pre><code>mkdir template\ncat &lt;&lt;EOF &gt; template/template.yaml\n_p: &amp;provider_aws\n- aws:\n    region: {{ .variables.region }}\n\nname: s3-website\nkind: StackTemplate\nunits:\n  -\n    name: bucket\n    type: tfmodule\n    providers: *provider_aws\n    source: terraform-aws-modules/s3-bucket/aws\n    inputs:\n      bucket: {{ .variables.bucket_name }}\n      force_destroy: true\n      acl: \"public-read\"\n      control_object_ownership: true\n      object_ownership: \"BucketOwnerPreferred\"\n      attach_public_policy: true\n      block_public_acls: false\n      block_public_policy: false\n      ignore_public_acls: false\n      restrict_public_buckets: false\n      website:\n        index_document: \"index.html\"\n        error_document: \"error.html\"\n  -\n    name: web-page-object\n    type: tfmodule\n    providers: *provider_aws\n    source: \"terraform-aws-modules/s3-bucket/aws//modules/object\"\n    version: \"3.15.1\"\n    inputs:\n      bucket: {{ remoteState \"this.bucket.s3_bucket_id\" }}\n      key: \"index.html\"\n      acl: \"public-read\"\n      content_type: \"text/html\"\n      content: |\n        {{- .variables.content | nindent 8 }}\n\n  -\n    name: outputs\n    type: printer\n    depends_on: this.web-page-object\n    outputs:\n      websiteUrl: http://{{ .variables.bucket_name }}.s3-website.{{ .variables.region }}.amazonaws.com/\nEOF\n</code></pre> Click to expand explanation of the Stack Template 1. Provider Definition (_p)   This section employs a YAML anchor, pre-setting the cloud provider and region for the resources in the stack. For this example, AWS is the designated provider, and the region is dynamically passed from the variables:  <pre><code>_p: &amp;provider_aws\n- aws:\n    region: {{ .variables.region }}\n</code></pre> 2. Units   The units section is where the real action is. Each unit is a self-contained \"piece\" of infrastructure, typically associated with a particular Terraform module or a direct cloud resource.  Bucket Unit   This unit is utilizing the `terraform-aws-modules/s3-bucket/aws` module to provision an S3 bucket. Inputs for the module, such as the bucket name, are populated using variables passed into the Stack.  <pre><code>name: bucket\ntype: tfmodule\nproviders: *provider_aws\nsource: terraform-aws-modules/s3-bucket/aws\ninputs:\n  bucket: {{ .variables.bucket_name }}\n  ...\n</code></pre> Web-page Object Unit   After the bucket is created, this unit takes on the responsibility of creating a web-page object inside it. This is done using a sub-module from the S3 bucket module specifically designed for object creation. A notable feature is the remoteState function, which dynamically pulls the ID of the S3 bucket created by the previous unit:  <pre><code>name: web-page-object\ntype: tfmodule\nproviders: *provider_aws\nsource: \"terraform-aws-modules/s3-bucket/aws//modules/object\"\ninputs:\n  bucket: {{ remoteState \"this.bucket.s3_bucket_id\" }}\n  ...\n</code></pre> Outputs Unit   Lastly, this unit is designed to provide outputs, allowing users to view certain results of the Stack execution. For this template, it provides the website URL of the hosted S3 website.  <pre><code>name: outputs\ntype: printer\ndepends_on: this.web-page-object\noutputs:\n  websiteUrl: http://{{ .variables.bucket_name }}.s3-website.{{ .variables.region }}.amazonaws.com/\n</code></pre> 3. Variables and Data Flow   The Stack Template is adept at harnessing variables, not just from the Stack (e.g., `stack.yaml`), but also from other resources via the remoteState function. This facilitates a seamless flow of data between resources and units, enabling dynamic infrastructure creation based on real-time cloud resource states and user-defined variables."},{"location":"get-started-cdev-aws/#sample-website-file-filesindexhtml","title":"Sample Website File (<code>files/index.html</code>)","text":"<pre><code>mkdir files\ncat &lt;&lt;EOF &gt; files/index.html\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n    &lt;title&gt;Cdev Demo Website Home Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Welcome to my website&lt;/h1&gt;\n  &lt;p&gt;Now hosted on Amazon S3!&lt;/p&gt;\n  &lt;h2&gt;See you!&lt;/h2&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n</code></pre>"},{"location":"get-started-cdev-aws/#deploying-with-clusterdev","title":"Deploying with Cluster.dev","text":"<ul> <li> <p>Plan the deployment:</p> <pre><code>cdev plan\n</code></pre> </li> <li> <p>Apply the changes:</p> <pre><code>cdev apply\n</code></pre> </li> </ul>"},{"location":"get-started-cdev-aws/#example-screen-cast","title":"Example Screen Cast","text":""},{"location":"get-started-cdev-aws/#clean-up","title":"Clean up","text":"<p>To remove the cluster with created resources run the command:</p> <pre><code>cdev destroy\n</code></pre>"},{"location":"get-started-cdev-aws/#more-examples","title":"More Examples","text":"<p>In the Examples section you will find ready-to-use advanced Cluster.dev samples that will help you bootstrap more complex cloud infrastructures with Helm and Terraform compositions:</p> <ul> <li>EKS cluster in AWS</li> <li>Modify AWS-EKS</li> <li>K3s cluster in AWS</li> <li>AWS-K3s Prometheus</li> </ul>"},{"location":"get-started-cdev-azure/","title":"Getting Started with Cluster.dev on Azure Cloud","text":"<p>This guide will walk you through the steps to deploy your first project with Cluster.dev on Azure Cloud.</p> <pre><code>                          +-------------------------+\n                          | Project.yaml            |\n                          |  - location             |\n                          +------------+------------+\n                                       |\n                                       |\n                          +------------v------------+\n                          | Stack.yaml              |\n                          |  - storage_account_name |\n                          |  - location             |\n                          |  - file_content         |\n                          +------------+------------+\n                                       |\n                                       |\n+--------------------------------------v----------------------------------------+\n| StackTemplate: azure-static-website                                           |\n|                                                                               |\n|  +---------------------+     +---------------------+     +-----------------+  |\n|  | resource-group      |     | storage-account     |     | web-page-blob   |  |\n|  | type: tfmodule      |     | type: tfmodule      |     | type: tfmodule  |  |\n|  | inputs:             |     | inputs:             |     | inputs:         |  |\n|  |  location           |     | storage_account_name|     |  file_content   |  |\n|  |  resource_group_name|     |                     |     |                 |  |\n|  +---------------------+     +----------^----------+     +--------^--------+  |\n|        |                       | resource-group           | storage-account   |\n|        |                       | name &amp; location          | name              |\n|        |                       | via remoteState          | via remoteState   |\n+--------|-----------------------|--------------------------|-------------------+\n         |                       |                          |\n         v                       v                          v\nAzure Resource Group    Azure Storage Account      Azure Blob (in $web container)\n                                 |\n                                 v\n                       Printer: Static WebsiteUrl\n</code></pre>"},{"location":"get-started-cdev-azure/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following are installed and set up:</p> <ul> <li>Terraform: Version 1.4 or above. Install Terraform.</li> </ul> <pre><code>terraform --version\n</code></pre> <ul> <li>Azure CLI:</li> </ul> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\naz --version\n</code></pre> <ul> <li>Cluster.dev client:</li> </ul> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\ncdev --version\n</code></pre>"},{"location":"get-started-cdev-azure/#authentication","title":"Authentication","text":"<p>Before using the Azure CLI, you'll need to authenticate:</p> <pre><code> az login --use-device-code\n</code></pre> <p>Follow the prompt to sign in.</p>"},{"location":"get-started-cdev-azure/#creating-an-azure-blob-storage-for-storing-state","title":"Creating an Azure Blob Storage for Storing State","text":"<p>First, create a resource group:</p> <pre><code>az group create --name cdevResourceGroup --location EastUS\n</code></pre> <p>Then, create a storage account:</p> <pre><code>az storage account create --name cdevstates --resource-group cdevResourceGroup --location EastUS --sku Standard_LRS\n</code></pre> <p>Then, create a storage container:</p> <pre><code>az storage container create --name tfstate --account-name cdevstates\n</code></pre>"},{"location":"get-started-cdev-azure/#setting-up-your-project","title":"Setting Up Your Project","text":""},{"location":"get-started-cdev-azure/#project-configuration-projectyaml","title":"Project Configuration (<code>project.yaml</code>)","text":"<ul> <li>Defines the overarching project settings. All subsequent stack configurations will inherit and can override these settings.</li> <li>It points to default as the backend, meaning that the Cluster.dev state for resources defined in this project will be stored locally.</li> <li>Project-level variables are defined here and can be referenced in other configurations.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; project.yaml\nname: dev\nkind: Project\nbackend: default\nvariables:\n  organization: cluster.dev\n  location: eastus\n  state_storage_account_name: cdevstates\nEOF\n</code></pre>"},{"location":"get-started-cdev-azure/#backend-configuration-backendyaml","title":"Backend Configuration (<code>backend.yaml</code>)","text":"<p>This specifies where Cluster.dev will store its own state and the Terraform states for any infrastructure it provisions or manages. Given the backend type as S3, it's clear that AWS is the chosen cloud provider.</p> <pre><code>cat &lt;&lt;EOF &gt; backend.yaml\nname: azure-backend\nkind: Backend\nprovider: azurerm\nspec:\n  resource_group_name: cdevResourceGroup\n  storage_account_name: {{ .project.variables.state_storage_account_name }}\n  container_name: tfstate\nEOF\n</code></pre>"},{"location":"get-started-cdev-azure/#stack-configuration-stackyaml","title":"Stack Configuration (<code>stack.yaml</code>)","text":"<ul> <li>This represents a distinct set of infrastructure resources to be provisioned.</li> <li>It references a local template (in this case, the previously provided stack template) to know what resources to create.</li> <li>Variables specified in this file will be passed to the Terraform modules called in the template.</li> <li>The content variable here is especially useful; it dynamically populates the content of an S3 bucket object (a webpage in this case) using the local <code>index.html</code> file.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; stack.yaml\nname: az-blob-website\ntemplate: ./template/\nkind: Stack\nbackend: azure-backend\nvariables:\n  storage_account_name: \"tmpldevtest\"\n  resource_group_name: \"demo-resource-group\"\n  location: {{ .project.variables.location }}\n  file_content: |\n    {{- readFile \"./files/index.html\" | nindent 4 }}\nEOF\n</code></pre>"},{"location":"get-started-cdev-azure/#stack-template-templateyaml","title":"Stack Template (<code>template.yaml</code>)","text":"<p>The <code>StackTemplate</code> serves as a pivotal object within Cluster.dev. It lays out the actual infrastructure components you intend to provision using Terraform modules and resources. Essentially, it determines how your cloud resources should be laid out and interconnected.</p> <pre><code>mkdir template\ncat &lt;&lt;EOF &gt; template/template.yaml\n_p: &amp;provider_azurerm\n- azurerm:\n    features:\n      resource_group:\n        prevent_deletion_if_contains_resources: false\n\n_globals: &amp;global_settings\n  default_region: \"region1\"\n  regions:\n    region1: {{ .variables.location }}\n  prefixes: [\"dev\"]\n  random_length: 4\n  passthrough: false\n  use_slug: false\n  inherit_tags: false\n\n_version: &amp;module_version 5.7.5\n\nname: azure-static-website\nkind: StackTemplate\nunits:\n  -\n    name: resource-group\n    type: tfmodule\n    providers: *provider_azurerm\n    source: aztfmod/caf/azurerm//modules/resource_group\n    version: *module_version\n    inputs:\n      global_settings: *global_settings\n      resource_group_name: {{ .variables.resource_group_name }}\n      settings:\n        region: \"region1\"\n  -\n    name: storage-account\n    type: tfmodule\n    providers: *provider_azurerm\n    source: aztfmod/caf/azurerm//modules/storage_account\n    version: *module_version\n    inputs:\n      base_tags: false\n      global_settings: *global_settings\n      client_config:\n        key: demo\n      resource_group:\n        name: {{ remoteState \"this.resource-group.name\" }}\n        location: {{ remoteState \"this.resource-group.location\" }}\n      storage_account:\n        name: {{ .variables.storage_account_name }}\n        account_kind: \"StorageV2\"\n        account_tier: \"Standard\"\n        static_website:\n          index_document: \"index.html\"\n          error_404_document: \"error.html\"\n      var_folder_path: \"./\"\n  -\n    name: web-page-blob\n    type: tfmodule\n    providers: *provider_azurerm\n    source: aztfmod/caf/azurerm//modules/storage_account/blob\n    version: *module_version\n    inputs:\n      settings:\n        name: \"index.html\"\n        content_type: \"text/html\"\n        source_content: |\n          {{- .variables.file_content | nindent 12 }}\n      storage_account_name: {{ remoteState \"this.storage-account.name\" }}\n      storage_container_name: \"$web\"\n      var_folder_path: \"./\"\n  -\n    name: outputs\n    type: printer\n    depends_on: this.web-page-blob\n    outputs:\n      websiteUrl: https://{{ remoteState \"this.storage-account.primary_web_host\" }}\nEOF\n</code></pre> Click to expand explanation of the Stack Template 1. Provider Definition (_p)   This section uses a YAML anchor, defining the cloud provider and location for the resources in the stack. For this case, Azure is the chosen provider, and the location is dynamically retrieved from the variables:  <pre><code>_p: &amp;provider_azurerm\n- azurerm:\n    features:\n      resource_group:\n        prevent_deletion_if_contains_resources: false\n</code></pre> 2. Units   The units section is where the real action is. Each unit is a self-contained \"piece\" of infrastructure, typically associated with a particular Terraform module or a direct cloud resource.  Storage Account Unit   This unit leverages the `aztfmod/caf/azurerm//modules/storage_account` module to provision an Azure Blob Storage account. Inputs for the module, such as the storage account name, are filled using variables passed into the Stack.  <pre><code>name: storage-account\ntype: tfmodule\nproviders: *provider_azurerm\nsource: aztfmod/caf/azurerm//modules/storage_account\ninputs:\n  name: {{ .variables.storage_account_name }}\n  ...\n</code></pre> Web-page Object Unit   Upon creating the storage account, this unit takes the role of establishing a web-page object inside it. This action is carried out using a sub-module from the storage account module specifically designed for blob creation. A standout feature is the remoteState function, which dynamically extracts the name of the Azure Storage account produced by the preceding unit:  <pre><code>name: web-page-blob\ntype: tfmodule\nproviders: *provider_azurerm\nsource: aztfmod/caf/azurerm//modules/storage_account/blob\ninputs:\n  storage_account_name: {{ remoteState \"this.storage-account.name\" }}\n  ...\n</code></pre> Outputs Unit   Lastly, this unit is designed to provide outputs, allowing users to view certain results of the Stack execution. For this template, it provides the website URL of the hosted Azure website.  <pre><code>name: outputs\ntype: printer\ndepends_on: this.web-page-blob\noutputs:\n  websiteUrl: https://{{ remoteState \"this.storage-account.primary_web_host\" }}\n</code></pre> 3. Variables and Data Flow   The Stack Template is adept at harnessing variables, not just from the Stack (e.g., `stack.yaml`), but also from other resources via the remoteState function. This facilitates a seamless flow of data between resources and units, enabling dynamic infrastructure creation based on real-time cloud resource states and user-defined variables."},{"location":"get-started-cdev-azure/#sample-website-file-filesindexhtml","title":"Sample Website File (<code>files/index.html</code>)","text":"<pre><code>mkdir files\ncat &lt;&lt;EOF &gt; files/index.html\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n    &lt;title&gt;Cdev Demo Website Home Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Welcome to my website&lt;/h1&gt;\n  &lt;p&gt;Now hosted on Azure!&lt;/p&gt;\n  &lt;h2&gt;See you!&lt;/h2&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n</code></pre>"},{"location":"get-started-cdev-azure/#deploying-with-clusterdev","title":"Deploying with Cluster.dev","text":"<ul> <li> <p>Plan the deployment:</p> <pre><code>cdev plan\n</code></pre> </li> <li> <p>Apply the changes:</p> <pre><code>cdev apply\n</code></pre> </li> </ul>"},{"location":"get-started-cdev-azure/#example-screen-cast","title":"Example Screen Cast","text":""},{"location":"get-started-cdev-azure/#clean-up","title":"Clean up","text":"<p>To remove the cluster with created resources run the command:</p> <pre><code>cdev destroy\n</code></pre>"},{"location":"get-started-cdev-gcp/","title":"Getting Started with Cluster.dev on Google Cloud","text":"<p>This guide will walk you through the steps to deploy your first project with Cluster.dev on Google Cloud.</p> <pre><code>                          +---------------------------------+\n                          | Project.yaml                    |\n                          |  - project_name                 |\n                          |  - google_project_id            |\n                          |  - google_cloud_region          |\n                          |  - google_cloud_bucket_location |\n                          +------------+--------------------+\n                                       |\n                                       |\n                          +------------v------------+\n                          | Stack.yaml              |\n                          |  - web_page_content     |\n                          +------------+------------+\n                                       |\n                                       |\n+--------------------------------------v-----------------------------------------------------------------+\n| StackTemplate: gcs-static-website                                                                      |\n|                                                                                                        |\n|  +---------------------+     +---------------------+     +-----------------+    +-----------------+    |\n|  | cloud-storage       |     | cloud-bucket-object |     | cloud-url-map   |    | cloud-lb        |    |\n|  | type: tfmodule      |     | type: tfmodule      |     | type: tfmodule  |    | type: tfmodule  |    |\n|  | inputs:             |     | inputs:             |     | inputs:         |    | inputs:         |    |\n|  |  names              |     |   bucket_name       |     |  name           |    |  name           |    |\n|  |  randomize_suffix   |     |   object_name       |     |  bucket_name    |    |  project        |    |\n|  |  project_id         |     |   object_content    |     +--------^--------+    |  url_map        |    |\n|  |  location           |     +----------^----------+      |                     +--------^--------+    |\n|  +---------------------+       |                          |                       |                    |\n|        |                       | cloud-storage            | cloud-storage         | cloud-url-map      |\n|        |                       | bucket name              | bucket name           | url_map            |\n|        |                       | via remoteState          | via remoteState       | via remoteState    |\n+--------|-----------------------|--------------------------|--------------------------------------------+\n         |                       |                          |                       |\n         v                       v                          v                       v\n  Storage Bucket             Storage Bucket Object     Url Map &amp; Bucket Backend   Load Balancer\n                                 |\n                                 v\n                       Printer: Static WebsiteUrl\n</code></pre>"},{"location":"get-started-cdev-gcp/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following are installed and set up:</p> <ul> <li>Terraform: Version 1.4 or above. Install Terraform.</li> </ul> <pre><code>terraform --version\n</code></pre> <ul> <li>Google Cloud CLI: Install Google Cloud CLI.</li> </ul> <pre><code>gcloud --version\n</code></pre> <ul> <li>Cluster.dev client:</li> </ul> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\ncdev --version\n</code></pre>"},{"location":"get-started-cdev-gcp/#authentication","title":"Authentication","text":"<p>Before using the Google Cloud  CLI, you'll need to authenticate:</p> <pre><code>gcloud auth login\n</code></pre> <p>Authorize cdev/terraform to interact with GCP via SD</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"get-started-cdev-gcp/#creating-an-storage-bucket-for-storing-state","title":"Creating an Storage Bucket for Storing State","text":"<pre><code>gsutil mb gs://cdevstates\n</code></pre>"},{"location":"get-started-cdev-gcp/#setting-up-your-project","title":"Setting Up Your Project","text":"<p>Tip</p> <p>You can clone example files from repo:</p> <pre><code>git clone https://github.com/shalb/cdev-examples\ncd cdev-examples/gcp/gcs-website/\n</code></pre>"},{"location":"get-started-cdev-gcp/#project-configuration-projectyaml","title":"Project Configuration (<code>project.yaml</code>)","text":"<ul> <li>Defines the overarching project settings. All subsequent stack configurations will inherit and can override these settings.</li> <li>It points to aws-backend as the backend, meaning that the Cluster.dev state for resources defined in this project will be stored in the Google Storage bucket specified in <code>backend.yaml</code>.</li> <li>Project-level variables are defined here and can be referenced in other configurations.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; project.yaml\nname: dev\nkind: Project\nbackend: gcs-backend\nvariables:\n  project_name: dev-test\n  google_project_id: cdev-demo\n  google_cloud_region: us-west1\n  google_cloud_bucket_location: EU\n  google_bucket_name: cdevstates\n  google_bucket_prefix: dev\nEOF\n</code></pre>"},{"location":"get-started-cdev-gcp/#backend-configuration-backendyaml","title":"Backend Configuration (<code>backend.yaml</code>)","text":"<p>This specifies where Cluster.dev will store its own state and the Terraform states for any infrastructure it provisions or manages. Given the backend type as GCS.</p> <pre><code>name: gcs-backend\nkind: Backend\nprovider: gcs\nspec:\n  project: {{ .project.variables.google_project_id }}\n  bucket: {{ .project.variables.google_bucket_name }}\n  prefix: {{ .project.variables.google_bucket_prefix }}\nEOF\n</code></pre>"},{"location":"get-started-cdev-gcp/#stack-configuration-stackyaml","title":"Stack Configuration (<code>stack.yaml</code>)","text":"<ul> <li>This represents a distinct set of infrastructure resources to be provisioned.</li> <li>It references a local template (in this case, the previously provided stack template) to know what resources to create.</li> <li>Variables specified in this file will be passed to the Terraform modules called in the template.</li> <li>The content variable here is especially useful; it dynamically populates the content of an Google Storage bucket object (a webpage in this case) using the local <code>index.html</code> file.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; stack.yaml\nname: cloud-storage\ntemplate: ./template/\nkind: Stack\nbackend: gcs-backend\nvariables:\n  project_name: {{ .project.variables.project_name }}\n  google_cloud_region: {{ .project.variables.google_cloud_region }}\n  google_cloud_bucket_location: {{ .project.variables.google_cloud_bucket_location }}\n  google_project_id: {{ .project.variables.google_project_id }}\n  web_page_content: |\n    {{- readFile \"./files/index.html\" | nindent 4 }}\nEOF\n</code></pre>"},{"location":"get-started-cdev-gcp/#stack-template-templateyaml","title":"Stack Template (<code>template.yaml</code>)","text":"<p>The <code>StackTemplate</code> serves as a pivotal object within Cluster.dev. It lays out the actual infrastructure components you intend to provision using Terraform modules and resources. Essentially, it determines how your cloud resources should be laid out and interconnected.</p> <pre><code>mkdir template\ncat &lt;&lt;EOF &gt; template/template.yaml\n_p: &amp;provider_gcp\n- google:\n    project: {{ .variables.google_project_id }}\n    region: {{ .variables.google_cloud_region }}\n\nname: gcs-static-website\nkind: StackTemplate\nunits:\n  -\n    name: cloud-storage\n    type: tfmodule\n    providers: *provider_gcp\n    source: \"github.com/terraform-google-modules/terraform-google-cloud-storage.git?ref=v4.0.1\"\n    inputs:\n      names:\n        - {{ .variables.project_name }}\n      randomize_suffix: true\n      project_id: {{ .variables.google_project_id }}\n      location: {{ .variables.google_cloud_bucket_location }}\n      set_viewer_roles: true\n      viewers:\n        - allUsers\n      website:\n        main_page_suffix: \"index.html\"\n        not_found_page: \"index.html\"\n  -\n    name: cloud-bucket-object\n    type: tfmodule\n    providers: *provider_gcp\n    depends_on: this.cloud-storage\n    source: \"bootlabstech/cloud-storage-bucket-object/google\"\n    version: \"1.0.1\"\n    inputs:\n      bucket_name: {{ remoteState \"this.cloud-storage.name\" }}\n      object_name: \"index.html\"\n      object_content: |\n        {{- .variables.web_page_content | nindent 8 }}\n  -\n    name: cloud-url-map\n    type: tfmodule\n    providers: *provider_gcp\n    depends_on: this.cloud-storage\n    source: \"github.com/shalb/terraform-gcs-bucket-backend.git?ref=0.0.1\"\n    inputs:\n      name: {{ .variables.project_name }}\n      bucket_name: {{ remoteState \"this.cloud-storage.name\" }}\n  -\n    name: cloud-lb\n    type: tfmodule\n    providers: *provider_gcp\n    depends_on: this.cloud-url-map\n    source: \"GoogleCloudPlatform/lb-http/google\"\n    version: \"9.2.0\"\n    inputs:\n      name: {{ .variables.project_name }}\n      project: {{ .variables.google_project_id }}\n      url_map: {{ remoteState \"this.cloud-url-map.url_map_self_link\" }}\n      create_url_map: false\n      ssl: false\n      backends:\n        default:\n          protocol: \"HTTP\"\n          port: 80\n          port_name: \"http\"\n          timeout_sec: 10\n          enable_cdn: false\n          groups: [] \n          health_check:\n            request_path: \"/\"\n            port: 80\n          log_config:\n            enable: true\n            sample_rate: 1.0\n          iap_config:\n            enable: false\n  -\n    name: outputs\n    type: printer\n    depends_on: this.cloud-storage\n    outputs:\n      websiteUrl: http://{{ remoteState \"this.cloud-lb.external_ip\" }}\nEOF\n</code></pre> Click to expand explanation of the Stack Template 1. Provider Definition (_p)   This section uses a YAML anchor, defining the cloud provider and location for the resources in the stack. For this case, GCS is the chosen provider, and the location is dynamically retrieved from the variables:  <pre><code>_p: &amp;provider_gcp\n- google:\n    project: {{ .variables.google_project_id }}\n    region: {{ .variables.google_cloud_region }}\n</code></pre> 2. Units   The units section is where the real action is. Each unit is a self-contained \"piece\" of infrastructure, typically associated with a particular Terraform module or a direct cloud resource.  Cloud Storage Unit   This unit leverages the `github.com/terraform-google-modules/terraform-google-cloud-storage` module to provision an Google Storage Bucket. Inputs for the module, such as the bucket name and project, are filled using variables passed into the Stack.  <pre><code>name: cloud-storage\ntype: tfmodule\nproviders: *provider_gcp\nsource: \"github.com/terraform-google-modules/terraform-google-cloud-storage.git?ref=v4.0.1\"\n  inputs:\n    names:\n      - {{ .variables.name }}\n    randomize_suffix: true\n    project_id: {{ .variables.google_project_id }}\n    location: {{ .variables.google_cloud_bucket_location }}\n    set_viewer_roles: true\n    viewers:\n      - allUsers\n    website:\n      main_page_suffix: \"index.html\"\n      not_found_page: \"index.html\"\n</code></pre> Cloud Bucket Object Unit   Upon creating the storage bucket, this unit takes the role of establishing a web-page object inside it. This action is carried out using a module storage bucket object module specifically designed for blob creation. A standout feature is the remoteState function, which dynamically extracts the name of the Storage Bucket name produced by the preceding unit:  <pre><code>name: cloud-bucket-object\ntype: tfmodule\nproviders: *provider_gcp\ndepends_on: this.cloud-storage\nsource: \"bootlabstech/cloud-storage-bucket-object/google\"\nversion: \"1.0.1\"\ninputs:\n  bucket_name: {{ remoteState \"this.cloud-storage.name\" }}\n  object_name: \"index.html\"\n  object_content: |\n    {{- .variables.web_page_content | nindent 8 }}\n</code></pre> Cloud URL Map Unit   This unit create google_compute_url_map and google_compute_backend_bucket in order to supply it to cloud-lb unit. A standout feature is the remoteState function, which dynamically extracts the name of the Storage Bucket name produced by Cloud Storage unit:  <pre><code>name: cloud-url-map\ntype: tfmodule\nproviders: *provider_gcp\ndepends_on: this.cloud-storage\nsource: \"github.com/shalb/terraform-gcs-bucket-backend.git?ref=0.0.1\"\ninputs:\n  name: {{ .variables.project_name }}\n  bucket_name: {{ remoteState \"this.cloud-storage.name\" }}\n</code></pre> Cloud Load Balancer Unit   This unit create google load balancer. A standout feature is the remoteState function, which dynamically extracts the name of the URL Map URI produced by Cloud URL Map unit:  <pre><code>name: cloud-lb\ntype: tfmodule\nproviders: *provider_gcp\ndepends_on: this.cloud-url-map\nsource: \"GoogleCloudPlatform/lb-http/google\"\nversion: \"9.2.0\"\ninputs:\n  name: {{ .variables.project_name }}\n  project: {{ .variables.google_project_id }}\n  url_map: {{ remoteState \"this.cloud-url-map.url_map_self_link\" }}\n  create_url_map: false\n  ssl: false\n  backends:\n    default:\n      protocol: \"HTTP\"\n      port: 80\n      port_name: \"http\"\n      timeout_sec: 10\n      enable_cdn: false\n      groups: [] \n      health_check:\n        request_path: \"/\"\n        port: 80\n      log_config:\n        enable: true\n        sample_rate: 1.0\n      iap_config:\n        enable: false\n</code></pre> Outputs Unit   Lastly, this unit is designed to provide outputs, allowing users to view certain results of the Stack execution. For this template, it provides the website URL of the hosted website exposed by load balancer.  <pre><code>name: outputs\ntype: printer\ndepends_on: this.cloud-storage\noutputs:\n  websiteUrl: http://{{ remoteState \"this.cloud-lb.external_ip\" }}\n</code></pre> 3. Variables and Data Flow   The Stack Template is adept at harnessing variables, not just from the Stack (e.g., `stack.yaml`), but also from other resources via the remoteState function. This facilitates a seamless flow of data between resources and units, enabling dynamic infrastructure creation based on real-time cloud resource states and user-defined variables."},{"location":"get-started-cdev-gcp/#sample-website-file-filesindexhtml","title":"Sample Website File (<code>files/index.html</code>)","text":"<pre><code>mkdir files\ncat &lt;&lt;EOF &gt; files/index.html\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n    &lt;title&gt;Cdev Demo Website Home Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Welcome to my website&lt;/h1&gt;\n  &lt;p&gt;Now hosted on GCS!&lt;/p&gt;\n  &lt;h2&gt;See you!&lt;/h2&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n</code></pre>"},{"location":"get-started-cdev-gcp/#deploying-with-clusterdev","title":"Deploying with Cluster.dev","text":"<ul> <li> <p>Plan the deployment:</p> <pre><code>cdev plan\n</code></pre> </li> <li> <p>Apply the changes:</p> <pre><code>cdev apply\n</code></pre> </li> </ul>"},{"location":"get-started-cdev-gcp/#clean-up","title":"Clean up","text":"<p>To remove the cluster with created resources run the command:</p> <pre><code>cdev destroy\n</code></pre>"},{"location":"get-started-cdev-gcp/#more-examples","title":"More Examples","text":"<p>In the Examples section you will find ready-to-use advanced Cluster.dev samples that will help you bootstrap more complex cloud infrastructures with Helm and Terraform compositions:</p> <ul> <li>More Advanced example with GKE</li> </ul>"},{"location":"get-started-cdev-helm/","title":"Getting Started with Kubernetes and Helm","text":"<p>This guide will walk you through the steps to deploy a WordPress application along with a MySQL database on a Kubernetes cluster using StackTemplates with Helm units.</p> <pre><code>                          +-------------------------+\n                          | Stack.yaml              |\n                          |  - domain               |\n                          |  - kubeconfig_path      |\n                          +------------+------------+\n                                       |\n                                       |\n+--------------------------------------v---------------------------------+\n| StackTemplate: wordpress                                               |\n|                                                                        |\n|  +---------------------+               +---------------------+         |\n|  | mysql-wp-pass-user  |--------------&gt;| mysql-wordpress     |         |\n|  | type: tfmodule      |               | type: helm          |         |\n|  | output:             |               | inputs:             |         |\n|  |  generated password |               |  kubeconfig         |         |\n|  |                     |               |  values (from mysql.yaml)     |\n|  +---------------------+               +----------|----------+         |\n|                                                   |                    |\n|                                                   v                    |\n|                                           MySQL Deployment             |\n|                                                   |                    |\n|  +---------------------+               +----------|----------+         |\n|  | wp-pass             |--------------&gt;| wordpress           |         |\n|  | type: tfmodule      |               | type: helm          |         |\n|  | output:             |               | inputs:             |         |\n|  |  generated password |               |  kubeconfig         |         |\n|  |                     |               |  values (from wordpress.yaml) |\n|  +---------------------+               +----------|----------+         |\n|                                                   |                    |\n|                                                   v                    |\n|                                           WordPress Deployment         |\n|                                                                        |\n|  +---------------------+                                               |\n|  | outputs             |                                               |\n|  | type: printer       |                                               |\n|  | outputs:            |                                               |\n|  |  wordpress_url      |                                               |\n|  +---------------------+                                               |\n|            |                                                           |\n+------------|-----------------------------------------------------------+\n             |\n             v\n      wordpress_url Output\n</code></pre>"},{"location":"get-started-cdev-helm/#prerequisites","title":"Prerequisites","text":"<ol> <li>A running Kubernetes cluster.</li> <li>Your domain name (for this tutorial, we'll use <code>example.com</code> as a placeholder).</li> <li>The <code>kubeconfig</code> file for your Kubernetes cluster.</li> </ol>"},{"location":"get-started-cdev-helm/#setting-up-your-project","title":"Setting Up Your Project","text":"<p>Tip</p> <p>You can clone example files from repo: <pre><code>git clone https://github.com/shalb/cdev-examples\ncd cdev-examples/helm/wordpress/\n</code></pre></p>"},{"location":"get-started-cdev-helm/#project-configuration-projectyaml","title":"Project Configuration (<code>project.yaml</code>)","text":"<ul> <li>Defines the overarching project settings. All subsequent stack configurations will inherit and can override these settings.</li> <li>It points to aws-backend as the backend, meaning that the Cluster.dev state for resources defined in this project will be stored in the S3 bucket specified in <code>backend.yaml</code>.</li> <li>Project-level variables are defined here and can be referenced in other configurations.</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; project.yaml\nname: wordpress-demo\nkind: Project\nbackend: aws-backend\nvariables:\n  region: eu-central-1\n  state_bucket_name: cdev-states\nEOF\n</code></pre>"},{"location":"get-started-cdev-helm/#backend-configuration-backendyaml","title":"Backend Configuration (<code>backend.yaml</code>)","text":"<p>This specifies where Cluster.dev will store its own state and the Terraform states for any infrastructure it provisions or manages. In this example the AWS s3 is used, but you can choose any other provider.</p> <pre><code>cat &lt;&lt;EOF &gt; backend.yaml\nname: aws-backend\nkind: Backend\nprovider: s3\nspec:\n  bucket: {{ .project.variables.state_bucket_name }}\n  region: {{ .project.variables.region }}\nEOF\n</code></pre>"},{"location":"get-started-cdev-helm/#setting-up-the-stack-file-stackyaml","title":"Setting Up the Stack File (<code>stack.yaml</code>)","text":"<ul> <li>This represents a high level of infrastructure pattern configuration.</li> <li>It references a local template to know what resources to create.</li> <li>Variables specified in this file will be passed to the Terraform modules and Helm charts called in the template.</li> </ul> <p>Replace placeholders in <code>stack.yaml</code> with your actual <code>kubeconfig</code> path and domain.</p> <pre><code>cat &lt;&lt;EOF &gt; stack.yaml\nname: wordpress\ntemplate: \"./template/\"\nkind: Stack\nbackend: aws-backend\ncliVersion: \"&gt;= 0.7.14\"\nvariables:\n  kubeconfig_path: \"/data/home/voa/projects/cdev-aws-eks/examples/kubeconfig\" # Change to your path\n  domain: demo.cluster.dev # Change to your domain\nEOF\n</code></pre>"},{"location":"get-started-cdev-helm/#stack-template-templateyaml","title":"Stack Template (template.yaml)","text":"<p>The StackTemplate serves as a pivotal object within Cluster.dev. It lays out the actual infrastructure components you intend to provision using Terraform modules and resources. Essentially, it determines how your cloud resources should be laid out and interconnected.</p> <pre><code>mkdir template\ncat &lt;&lt;EOF &gt; template/template.yaml\nkind: StackTemplate\nname: wordpress\ncliVersion: \"&gt;= 0.7.15\"\nunits:\n## Generate Passwords with Terraform for MySQL and Wordpress\n  -\n    name: mysql-wp-pass-user\n    type: tfmodule\n    source: github.com/romanprog/terraform-password?ref=0.0.1\n    inputs:\n      length: 12\n      special: false\n  -\n    name: wp-pass\n    type: tfmodule\n    source: github.com/romanprog/terraform-password?ref=0.0.1\n    inputs:\n      length: 12\n      special: false\n## Install MySQL and Wordpress with Helm\n  -\n    name: mysql-wordpress\n    type: helm\n    kubeconfig: {{ .variables.kubeconfig_path }}\n    source:\n      repository: \"oci://registry-1.docker.io/bitnamicharts\"\n      chart: \"mysql\"\n      version: \"9.9.1\"\n    additional_options:\n      namespace: \"wordpress\"\n      create_namespace: true\n    values:\n      - file: ./files/mysql.yaml\n  -\n    name: wordpress\n    type: helm\n    depends_on: this.mysql-wordpress\n    kubeconfig: {{ .variables.kubeconfig_path }}\n    source:\n      repository: \"oci://registry-1.docker.io/bitnamicharts\"\n      chart: \"wordpress\"\n      version: \"16.1.2\"\n    additional_options:\n      namespace: \"wordpress\"\n      create_namespace: true\n    values:\n      - file: ./files/wordpress.yaml\n\n  - name: outputs\n    type: printer\n    depends_on: this.wordpress\n    outputs:\n      wordpress_url: https://wordpress.{{ .variables.domain }}/admin/\n      wordpress_user: user\n      wordpress_password: {{ remoteState \"this.wp-pass.result\" }}\nEOF\n</code></pre> <p>As you can see the StackTemplate contains Helm units and they could use inputs from values.yaml files where it is possible to use outputs from other type of units(like tfmodule) or even other stacks. Lets create that values for MySQL and Wordpress:</p> <pre><code>mkdir files\ncat &lt;&lt;EOF &gt; files/mysql.yaml\nfullNameOverride: mysql-wordpress\nauth:\n  rootPassword: {{ remoteState \"this.mysql-wp-pass-user.result\" }}\n  username: user\n  password: {{ remoteState \"this.mysql-wp-pass-user.result\" }}\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt; files/wordpress.yaml\ncontainerSecurityContext:\n  enabled: false\nmariadb:\n  enabled: false\nexternalDatabase:\n  port: 3306\n  user: user\n  password: {{ remoteState \"this.mysql-wp-pass-user.result\" }}\n  database: my_database\nwordpressPassword: {{ remoteState \"this.wp-pass.result\" }}\nallowOverrideNone: false\ningress:\n  enabled: true\n  ingressClassName: \"nginx\"\n  pathType: Prefix\n  hostname: wordpress.{{ .variables.domain }}\n  path: /\n  tls: true\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\nEOF\n</code></pre> Click to expand explanation of the Stack Template 1. Units  The units section is a list of infrastructure components that are provisioned sequentially. Each unit has a type, which indicates whether it's a Terraform module (`tfmodule`), a Helm chart (`helm`), or simply outputs (`printer`).  Password Generation Units  There are two password generation units which use the Terraform module `github.com/romanprog/terraform-password` to generate random passwords.  <pre><code>name: mysql-wp-pass-user\ntype: tfmodule\nsource: github.com/romanprog/terraform-password?ref=0.0.1\ninputs:\n  length: 12\n  special: false\n</code></pre>  These units will create passwords with a length of 12 characters without special characters. The outputs of these units (the generated passwords) are used in subsequent units.   MySQL Helm Chart Unit  This unit installs the MySQL chart from the `bitnamicharts` Helm repository.  <pre><code>name: mysql-wordpress\ntype: helm\nkubeconfig: {{ .variables.kubeconfig_path }}\nsource:\n  repository: \"oci://registry-1.docker.io/bitnamicharts\"\n  chart: \"mysql\"\n  version: \"9.9.1\"\n</code></pre>  The `kubeconfig` field uses a variable to point to the Kubeconfig file, enabling Helm to deploy to the correct Kubernetes cluster.  WordPress Helm Chart Unit  This unit installs the WordPress chart from the same Helm repository as MySQL. It depends on the `mysql-wordpress` unit, ensuring MySQL is installed first.  <pre><code>name: wordpress\ntype: helm\ndepends_on: this.mysql-wordpress\n</code></pre>  Both Helm units utilize external YAML files (`mysql.yaml` and `wordpress.yaml`) to populate values for the Helm charts. These values files leverage the `remoteState` function to fetch passwords generated by the Terraform modules.  Outputs Unit  This unit outputs the URL to access the WordPress site.  <pre><code>name: outputs\ntype: printer\ndepends_on: this.wordpress\noutputs:\n  wordpress_url: https://wordpress.{{ .variables.domain }}/admin/\n</code></pre>  It waits for the WordPress Helm unit to complete (`depends_on: this.wordpress`) and then provides the URL.  2. Variables and Data Flow  In this stack template:  The `.variables` placeholders, like `{{ .variables.kubeconfig_path }}` and `{{ .variables.domain }}`, fetch values from the stack's variables. The `remoteState` function, such as `{{ remoteState \"this.wp-pass.result\" }}`, fetches the outputs from previous units. For example, it retrieves the randomly generated password for WordPress. These mechanisms ensure dynamic configurations based on real-time resource states and user-defined variables. They enable values generated in one unit (e.g., a password from a Terraform module) to be utilized in a subsequent unit (e.g., a Helm deployment).  3. Additional File (`mysql.yaml` and `wordpress.yaml`) Explanation  Both files serve as value configurations for their respective Helm charts. `mysql.yaml` sets overrides for the MySQL deployment, specifically the authentication details. `wordpress.yaml` customizes the WordPress deployment, such as its database settings, ingress configuration, and password.  Both files leverage the `remoteState` function to pull in passwords generated by the Terraform password modules.  In summary, this stack template and its additional files define a robust deployment that sets up a WordPress application with its database, all while dynamically creating and injecting passwords. It showcases the synergy between Terraform for infrastructure provisioning and Helm for Kubernetes-based application deployments."},{"location":"get-started-cdev-helm/#deploying-wordpress-and-mysql-with-clusterdev","title":"Deploying WordPress and MySQL with cluster.dev","text":""},{"location":"get-started-cdev-helm/#1-planning-the-deployment","title":"1. Planning the Deployment","text":"<pre><code>cdev plan\n</code></pre>"},{"location":"get-started-cdev-helm/#2-applying-the-stacktemplate","title":"2. Applying the StackTemplate","text":"<pre><code>cdev apply\n</code></pre> <p>Upon executing these commands, WordPress and MySQL will be deployed on your Kubernetes cluster using cluster.dev.</p>"},{"location":"get-started-cdev-helm/#example-screen-cast","title":"Example Screen Cast","text":""},{"location":"get-started-cdev-helm/#clean-up","title":"Clean up","text":"<p>To remove the cluster with created resources run the command:</p> <pre><code>cdev destroy\n</code></pre>"},{"location":"get-started-cdev-helm/#conclusion","title":"Conclusion","text":"<p>StackTemplates provide a modular approach to deploying applications on Kubernetes. With Helm and StackTemplates, you can efficiently maintain, scale, and manage your deployments. This guide walked you through deploying WordPress and MySQL seamlessly on a Kubernetes cluster using these tools.</p>"},{"location":"get-started-cdev-helm/#more-examples","title":"More Examples","text":"<p>In the Examples section you will find ready-to-use advanced Cluster.dev samples that will help you bootstrap more complex cloud infrastructures with Helm and Terraform compositions:</p> <ul> <li>Install EKS cluster with Wordpress as separate stack in one project</li> <li>Install sample application by templating multiple Kubernetes manifests</li> <li>Repo with other examples</li> </ul>"},{"location":"get-started-create-project/","title":"Create New Project","text":""},{"location":"get-started-create-project/#quick-start","title":"Quick start","text":"<p>In our example we shall use the tmpl-development sample to create a new project on AWS cloud.</p> <ol> <li> <p>Install the Cluster.dev client.</p> </li> <li> <p>Create a project directory, cd into it and generate a project with the command:</p> <p><code>cdev project create https://github.com/shalb/cluster.dev tmpl-development</code></p> </li> <li> <p>Export environmental variables via an AWS profile.</p> </li> <li> <p>Run <code>cdev plan</code> to build the project and see the infrastructure that will be created.</p> </li> <li> <p>Run <code>cdev apply</code> to deploy the stack.</p> </li> </ol>"},{"location":"get-started-create-project/#workflow-diagram","title":"Workflow diagram","text":"<p>The diagram below describes the steps of creating a new project without generators.</p> <p></p>"},{"location":"get-started-overview/","title":"Cluster.dev Examples Overview","text":""},{"location":"get-started-overview/#working-with-terraform-modules","title":"Working with Terraform Modules","text":"<p>Example of how to create a static website hosting on different clouds:</p> Cloud Provider Sample Link Technologies AWS Quick Start on AWS Azure Quick Start on Azure GCP Quick Start on GCP"},{"location":"get-started-overview/#kubernetes-deployment-with-helm-charts","title":"Kubernetes Deployment with Helm Charts","text":"<p>Example of how to deploy application with Helm and Terraform to Kubernetes:</p> Description Sample Link Technologies Kubernetes Terraform Helm Quick Start with Kubernetes"},{"location":"get-started-overview/#bootstrapping-kubernetes-in-different-clouds","title":"Bootstrapping Kubernetes in Different Clouds","text":"<p>Create fully featured Kubernetes clusters with required addons:</p> Cloud Provider Kubernetes Type Sample Link Technologies AWS EKS AWS-EKS AWS K3s AWS-K3s Azure AKS Azure-AKS GCP GKE GCP-GKE AWS K3s + Prometheus AWS-K3s Prometheus DO K8s DO-K8s"},{"location":"get-started-overview/#deploying-llm-on-top-of-kubernetes-cluster","title":"Deploying LLM on top of Kubernetes cluster","text":"Cloud Provider Kubernetes Type Sample Link Technologies AWS EKS HF LLM in EKS"},{"location":"google-cloud-provider/","title":"Deploying to GCE","text":"<p>Work on setting up access to Google Cloud is in progress, examples are coming soon!</p>"},{"location":"google-cloud-provider/#authentication","title":"Authentication","text":"<p>See Terraform Google cloud provider documentation.</p>"},{"location":"how-does-cdev-work/","title":"How Does It Work?","text":"<p>With Cluster.dev you create or download a predefined stack template, set the variables, then render and deploy a whole stack.</p> <p>Capabilities:</p> <ul> <li>Re-using all existing Terraform private and public modules and Helm Charts.</li> <li>Applying parallel changes in multiple infrastructures concurrently.</li> <li>Using the same global variables and secrets across different infrastructures, clouds, and technologies.</li> <li>Templating anything with the Go-template function, even Terraform modules in Helm style templates.</li> <li>Create and manage secrets with SOPS or cloud secret storages.</li> <li>Generate a ready-to-use Terraform code.</li> </ul>"},{"location":"how-does-cdev-work/#basic-diagram","title":"Basic diagram","text":""},{"location":"how-does-cdev-work/#templating","title":"Templating","text":"<p>Templating is one of the key features that underlie the powerful capabilities of Cluster.dev. Similar to Helm, the cdev templating is based on Go template language and uses Sprig and some other extra functions to expose objects to the templates.</p> <p>Cluster.dev has two levels of templating, one that involves template rendering on a project level and one on a stack template level. For more information please refer to the Templating section.</p>"},{"location":"how-does-cdev-work/#how-to-use-clusterdev","title":"How to use Cluster.dev","text":"<p>Cluster.dev is a powerful framework that can be operated in several modes.</p>"},{"location":"how-does-cdev-work/#create-your-own-stack-template","title":"Create your own stack template","text":"<p>In this mode you can create your own stack templates. Having your own template enables you to launch or copy environments (like dev/stage/prod) with the same template. You'll be able to develop and propagate changes together with your team members, just using Git. Operating Cluster.dev in the developer mode requires some prerequisites. The most important is understanding Terraform and how to work with its modules. The knowledge of <code>go-template</code> syntax or Helm is advisable but not mandatory.</p>"},{"location":"how-does-cdev-work/#deploy-infrastructures-from-existing-stack-templates","title":"Deploy infrastructures from existing stack templates","text":"<p>This mode, also known as user mode, gives you the ability to launch ready-to-use infrastructures from prepared stack templates by just adding your cloud credentials and setting variables (such as name, zones, number of instances, etc.). You don't need to know background tooling like Terraform or Helm, it's as simple as downloading a sample and launching commands. Here are the steps:</p> <ul> <li>Install Cluster.dev binary</li> <li>Choose and download a stack template</li> <li>Set cloud credentials</li> <li>Define variables for the stack template</li> <li>Run Cluster.dev and get a cloud infrastructure</li> </ul>"},{"location":"how-does-cdev-work/#workflow","title":"Workflow","text":"<p>Let's assume you are starting a new infrastructure project. Let's see what your workflow would look like.</p> <ol> <li> <p>Define what kind of infrastructure pattern you need to achieve.</p> <p>a. What Terraform modules it would include (for example: I need to have VPC, Subnet definitions, IAM's and Roles).</p> <p>b. Whether you need to apply any Bash scripts before and after the module, or inside as pre/post-hooks.</p> <p>c. If you are using Kubernetes, check what controllers would be deployed and how (by Helm chart or K8s manifests).</p> </li> <li> <p>Check if there is any similar sample template that already exists.</p> </li> <li> <p>Clone the stack template locally and modify it if needed.</p> </li> <li> <p>Apply it.</p> </li> </ol>"},{"location":"howto-tf-versions/","title":"Use Different Terraform Versions","text":"<p>By default Cluster.dev runs that version of Terraform which is installed on a local machine. If you need to switch between versions, use some third-party utilities, such as Terraform Switcher.</p> <p>Example of <code>tfswitch</code> usage:</p> <p><pre><code>tfswitch 0.15.5\n\ncdev apply\n</code></pre> This will tell Cluster.dev to use Terraform v0.15.5. </p> <p>Use <code>CDEV_TF_BINARY</code> variable to indicate which Terraform binary to use.</p> <p>Info</p> <p>The variable is recommended to use for debug and template development only.</p> <p>You can pin it in <code>project.yaml</code>:</p> <pre><code>    name: dev\n    kind: Project\n    backend: aws-backend\n    variables:\n      organization: cluster-dev\n      region: eu-central-1\n      state_bucket_name: cluster-dev-gha-tests\n    exports:\n      CDEV_TF_BINARY: \"terraform_14\"\n</code></pre>"},{"location":"installation-upgrade/","title":"Installation and Upgrade","text":""},{"location":"installation-upgrade/#prerequisites","title":"Prerequisites","text":"<p>To start using Cluster.dev please make sure that you comply with the following preconditions. </p> <p>Supported operating systems:</p> <ul> <li> <p>Linux amd64</p> </li> <li> <p>Darwin amd64</p> </li> </ul> <p>Required software installed:</p> <ul> <li> <p>Git console client</p> </li> <li> <p>Terraform</p> </li> </ul>"},{"location":"installation-upgrade/#terraform","title":"Terraform","text":"<p>The Cluster.dev client uses the Terraform binary. The required Terraform version is 1.4 or higher. Refer to the Terraform installation instructions to install Terraform.</p>"},{"location":"installation-upgrade/#install-from-script","title":"Install From Script","text":"<p>Tip</p> <p>This is the easiest way to have the Cluster.dev client installed. For other options see the Install From Sources section.</p> <p>Cluster.dev has an installer script that takes the latest version of Cluster.dev client and installs it for you locally. </p> <p>Fetch the script and execute it locally with the command:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/shalb/cluster.dev/master/scripts/get_cdev.sh | sh\n</code></pre>"},{"location":"installation-upgrade/#install-from-sources","title":"Install From Sources","text":""},{"location":"installation-upgrade/#download-from-release","title":"Download from release","text":"<p>Each stable version of Cluster.dev has a binary that can be downloaded and installed manually. The documentation is suitable for v0.4.0 or higher of the Cluster.dev client.</p> <p>Installation example for Linux amd64:</p> <ol> <li> <p>Download your desired version from the releases page.</p> </li> <li> <p>Unpack it.</p> </li> <li> <p>Find the Cluster.dev binary in the unpacked directory.</p> </li> <li> <p>Move the binary to the bin folder (/usr/local/bin/).</p> </li> </ol>"},{"location":"installation-upgrade/#building-from-source","title":"Building from source","text":"<p>Go version 16 or higher is required - see Golang installation instructions.</p> <p>To build the Cluster.dev client from source:</p> <ol> <li> <p>Clone the Cluster.dev Git repo:</p> <pre><code>git clone https://github.com/shalb/cluster.dev/\n</code></pre> </li> <li> <p>Build the binary:</p> <pre><code>cd cluster.dev/ &amp;&amp; make\n</code></pre> </li> <li> <p>Check Cluster.dev and move the binary to the bin folder:</p> <pre><code>./bin/cdev --help\nmv ./bin/cdev /usr/local/bin/\n</code></pre> </li> </ol>"},{"location":"stack-templates-file-system/","title":"The Usage of Absolute and Relative paths in Units","text":"<p>As Cluster.dev is a code-generation utility, applying or destroying units involves executing commands in temporary directories. Accordingly, referencing files or directories from the template or config stack poses challenges. In templates, it's recommended to use relative paths. Given that units are executed in dynamically generated temporary directories, absolute paths may not be relevant. To address this issue, we introduced special functions: <code>workDir</code> and <code>templatePath</code>. </p> <p>The functions return the absolute path to the project/template's working directory. While this addresses part of the issue, other difficulties arise due to the absolute path potentially changing in different environments (such as different users on their workstations, pipelines, etc.). If a path is used as input to a unit, such changes are treated as modifications in the unit state, triggering the <code>apply</code> command even when the actual content of the unit has not changed. Although not critical, this error incurs additional time and inconvenience.</p> <p>We recommend avoiding the use of paths as unit parameters. Instead, consider using the <code>readFile</code> function to directly include the file's content in the unit. This approach ensures that changes in the path do not impact the unit's state since the function reads the content of the file, making the path irrelevant to state changes.</p>"},{"location":"stack-templates-functions/","title":"Functions","text":"<p>You can use basic Go template language and Sprig functions to modify a stack template.</p> <p>Additionally, you can use some enhanced functions that are listed below. </p>"},{"location":"stack-templates-functions/#insertyaml","title":"<code>insertYAML</code>","text":"<p>Pass <code>yaml</code> block as a value of target <code>yaml</code> template. </p> <p>Argument: data to pass, any value or reference to a block. </p> <p>The <code>insertYAML</code> function is integrated with the <code>yaml</code> syntax and can be used only as a full <code>yaml</code> value in units input. Example:</p> <p>Source <code>yaml</code>:</p> <pre><code>  values:\n    node_groups:\n      - name: ng1\n        min_size: 1\n        max_size: 5\n      - name: ng2\n        max_size: 2\n        type: spot\n</code></pre> <p>Target <code>yaml</code> template:</p> <pre><code>  units:\n    - name: k3s\n      type: tfmodule\n      node_groups: {{ insertYAML .values.node_groups }}\n</code></pre> <p>Rendered stack template:</p> <pre><code>  units:\n    - name: k3s\n      type: tfmodule\n      node_groups:\n        - name: ng1\n          min_size: 1\n          max_size: 5\n        - name: ng2\n          max_size: 2\n          type: spot\n</code></pre>"},{"location":"stack-templates-functions/#remotestate","title":"<code>remoteState</code>","text":"<p>Pass data across units and stacks, can be used in pre/post hooks. </p> <p>Important</p> <p>The function can take input only from Terraform-based units, i.e. Tfmodule, Kubernetes, Helm, and Printer. </p> <p>Argument: string, path to remote state consisting of 3 parts separated by a dot: <code>\"stack_name.unit_name.output_name\"</code>. Since the name of the stack is unknown inside the stack template, you can use \"this\" instead:<code>\"this.unit_name.output_name\"</code>. </p> <p>The <code>remoteState</code> function is integrated with the <code>yaml</code> syntax and can be used in following cases:</p> <ul> <li> <p>In units' inputs (all types of units) </p> </li> <li> <p>In units' pre/post hooks (all types of units)</p> </li> <li> <p>In Kubernetes manifests (Kubernetes units)</p> </li> </ul>"},{"location":"stack-templates-functions/#cidrsubnet","title":"<code>cidrSubnet</code>","text":"<p>Calculate a subnet address within given IP network address prefix. Same as Terraform function. Example:</p> <p>Source:   <pre><code>  {{ cidrSubnet \"172.16.0.0/12\" 4 2 }}\n</code></pre></p> <p>Rendered:   <pre><code>  172.18.0.0/16\n</code></pre></p>"},{"location":"stack-templates-functions/#readfile","title":"<code>readFile</code>","text":"<p>Read the passed file and return its contents as a string. </p> <p>Argument: path. The <code>readFile</code> function supports both absolute <code>readFile /path/to/file.txt</code> and relative paths <code>readFile ./files/data.yaml</code>. </p> <p>A relative path must refer to a location where the function is used. When it is used in a template, the path's base folder will be the template directory. When it is used in one of the project files, the path will begin with the project directory.</p> <p>Note</p> <p>The file is read as is; templating is not applied. </p>"},{"location":"stack-templates-functions/#workdir","title":"<code>workDir</code>","text":"<p>Return the absolute path to a working directory where a project runs, and the Terraform code is generated.  </p> <p>Warning</p> <p>Use the function with caution since the absolute path that it returns varies depending on running conditions. This can affect the Cluster.dev's state.  </p>"},{"location":"stack-templates-functions/#templatepath","title":"<code>templatePath</code>","text":"<p>Return the absolute path to the <code>stackTemplate</code> directory. It is designed exclusively for use within templates and cannot be employed in other objects, such as a <code>project</code> or <code>stack</code>.</p>"},{"location":"stack-templates-functions/#reqenv","title":"<code>reqEnv</code>","text":"<p>Return an environment variable required for a system to run. </p> <p>Argument: <code>ENVNAME</code>.  </p> <p>Specifying a non-existent variable in <code>reqEnv</code> function will result in failing <code>cdev apply</code> with an error message. </p>"},{"location":"stack-templates-functions/#bcrypt","title":"<code>bcrypt</code>","text":"<p>Apply the bcrypt encryption algorithm to a passed string.</p> <p>Warning</p> <p>Use the function with caution since it returns a unique hash with each calling, which can affect the Cluster.dev\u2019s state.  </p>"},{"location":"stack-templates-list/","title":"Stack Templates List","text":"<p>Currently there are 3 types of stack templates available:</p> <ul> <li>AWS-K3s</li> <li>AWS-EKS</li> <li>DO-K8s</li> </ul> <p>For more information on the templates please refer to the Examples section.</p>"},{"location":"stack-templates-overview/","title":"Overview","text":""},{"location":"stack-templates-overview/#description","title":"Description","text":"<p>A stack template is a <code>yaml</code> file that tells Cluster.dev which units to run and how. It is a core Cluster.dev resource that makes for its flexibility. Stack templates use Go template language to allow you customise and select the units you want to run.</p> <p>The stack template's config files are stored within the stack template directory that could be located either locally or in a Git repo. Cluster.dev reads all _./*.yaml files from the directory (except for the yaml/yml files specified within the .cdevignore), renders a stack template with the project's data, parses the <code>yaml</code> file and loads units - the most primitive elements of a stack template. </p> <p>A stack template represents a <code>yaml</code> structure with an array of different invocation units. Common view:</p> <pre><code>units:\n  - unit1\n  - unit2\n  - unit3\n  ...\n</code></pre> <p>Stack templates can utilize all kinds of Go templates and Sprig functions (similar to Helm). Along with that it is enhanced with functions like <code>insertYAML</code> that could pass <code>yaml</code> blocks directly.</p>"},{"location":"stack-templates-overview/#cdevignore","title":"<code>.cdevignore</code>","text":"<p>The <code>.cdevignore</code> file is used to specify files that you don't want to be read as project/stacktemplate configs. With this file in the project dir and in the stackTemplate dir, cdev will ignore the list of specified yaml/yml files as it configs. These files should be specified as a list of full names. Templates and patterns are not supported. Example of the <code>.cdevignore</code> file:</p> <pre><code>deployment.yaml\nfoo.yml\nbar.yaml\n</code></pre>"},{"location":"structure-backend/","title":"Backends","text":"<p>File: searching in <code>./*.yaml</code>. Optional.</p> <p>Backend is an object that describes backend storage for Terraform and Cluster.dev states. A backend could be local or remote, depending on where it stores a state.  </p> <p>You can use any options of Terraform backends in the remote backend configuration. The options will be mapped to a generated Terraform backend and converted as is.</p>"},{"location":"structure-backend/#local-backend","title":"Local backend","text":"<p>Local backend stores the cluster state on a local file system in the <code>.cluster.dev/states/cdev-state.json</code> file. Cluster.dev will use the local backend by default unless the remote backend is specified in the <code>project.yaml</code>. </p> <p>Example configuration:</p> <pre><code>name: my-fs\nkind: backend\nprovider: local\nspec:\n  path: /home/cluster.dev/states/\n</code></pre> <p>A path should be absolute or relative to the directory where <code>cdev</code> is running. An absolute path must begin with <code>/</code>, and a relative with <code>./</code> or <code>../</code>. </p>"},{"location":"structure-backend/#remote-backend","title":"Remote backend","text":"<p>Remote backend uses remote cloud services to store the cluster state, making it accessible for team work.</p>"},{"location":"structure-backend/#s3","title":"<code>s3</code>","text":"<p>Stores the cluster state in AWS S3 bucket. The Cluster.dev S3 backend supports some options of Terraform S3 backend. The list of supported options is referred below. </p> <pre><code>name: aws-backend\nkind: backend\nprovider: s3\nspec:\n  bucket: cdev-states\n  region: {{ .project.variables.region }}\n</code></pre>"},{"location":"structure-backend/#options","title":"Options","text":"<ul> <li> <p><code>bucket</code> - required. The name of the S3 bucket. </p> </li> <li> <p><code>region</code> - required. AWS Region of the S3 Bucket and DynamoDB Table (if used). This can also be sourced from the <code>AWS_DEFAULT_REGION</code> and <code>AWS_REGION</code> environment variables.</p> </li> <li> <p><code>access_key</code> - optional. AWS access key. If configured, must also configure <code>secret_key</code>. This can also be sourced from the <code>AWS_ACCESS_KEY_ID</code> environment variable, AWS shared credentials file (e.g. ~/.aws/credentials), or AWS shared configuration file (e.g. ~/.aws/config).</p> </li> <li> <p><code>secret_key</code> - optional. AWS access key. If configured, must also configure <code>access_key</code>. This can also be sourced from the <code>AWS_SECRET_ACCESS_KEY</code> environment variable, AWS shared credentials file (e.g. ~/.aws/credentials), or AWS shared configuration file (e.g. ~/.aws/config).</p> </li> <li> <p><code>profile</code> - optional. Name of AWS profile in AWS shared credentials file (e.g. ~/.aws/credentials) or AWS shared configuration file (e.g. ~/.aws/config) to use for credentials and/or configuration. This can also be sourced from the <code>AWS_PROFILE</code> environment variable.</p> </li> <li> <p><code>token</code> - optional. Multi-Factor Authentication (MFA) token. This can also be sourced from the <code>AWS_SESSION_TOKEN</code> environment variable.</p> </li> <li> <p><code>endpoint</code> - optional. Custom endpoint URL for the AWS S3 API.</p> </li> <li> <p><code>skip_metadata_api_check</code> - optional. Skip usage of EC2 Metadata API.</p> </li> <li> <p><code>skip_credentials_validation</code> - optional. Skip credentials validation via the STS API. Useful for testing and for AWS API implementations that do not have STS available.</p> </li> <li> <p><code>max_retries</code> - optional. The maximum number of times an AWS API request is retried on retryable failure. Defaults to 5.</p> </li> <li> <p><code>shared_credentials_file</code> - optional, deprecated, use <code>shared_credentials_files</code> instead. Path to the AWS shared credentials file. Defaults to ~/.aws/credentials.</p> </li> <li> <p><code>skip_region_validation</code> - optional. Skip validation of provided region name.</p> </li> <li> <p><code>sts_endpoint</code> - optional, deprecated. Custom endpoint URL for the AWS Security Token Service (STS) API. Use <code>endpoints.sts</code> instead.</p> </li> <li> <p><code>iam_endpoint</code> - optional, deprecated. Custom endpoint URL for the AWS Identity and Access Management (IAM) API. Use <code>endpoints.iam</code> instead.</p> </li> <li> <p><code>force_path_style</code> - optional, deprecated. Enable path-style S3 URLs (https:/// instead of https://.). <li> <p><code>assume_role_policy</code> - optional. IAM Policy JSON describing further restricting permissions for the IAM Role being assumed. Use <code>assume_role.policy</code> instead.</p> </li> <li> <p><code>assume_role_policy_arns</code> - optional. Set of Amazon Resource Names (ARNs) of IAM Policies describing further restricting permissions for the IAM Role being assumed. Use <code>assume_role.policy_arns</code> instead.</p> </li> <li> <p><code>assume_role_tags</code> - optional. Map of assume role session tags. Use <code>assume_role.tags</code> instead.</p> </li> <li> <p><code>assume_role_transitive_tag_keys</code> - optional. Set of assume role session tag keys to pass to any subsequent sessions. Use <code>assume_role.transitive_tag_keys</code> instead.</p> </li> <li> <p><code>external_id</code> - optional. External identifier to use when assuming the role. Use <code>assume_role.external_id</code> instead.</p> </li> <li> <p><code>role_arn</code> - optional. Amazon Resource Name (ARN) of the IAM Role to assume. Use <code>assume_role.role_arn</code> instead.</p> </li> <li> <p><code>session_name</code> - optional. Session name to use when assuming the role. Use <code>assume_role.session_name</code> instead.</p> </li>"},{"location":"structure-backend/#azurerm","title":"<code>azurerm</code>","text":"<p>Stores the cluster state in Microsoft Azure cloud. The <code>azurerm</code> backend supports the options of Terraform azurerm backend.</p> <pre><code>name: azurerm-b\nkind: backend\nprovider: azurerm\nspec:\n  resource_group_name: \"StorageAccount-ResourceGroup\"\n  storage_account_name: \"example\"\n  container_name: \"cdev-states\"\n</code></pre>"},{"location":"structure-backend/#gcs","title":"<code>gcs</code>","text":"<p>Stores the cluster state in Google Cloud service. The <code>gcs</code> backend supports some options of Terraform gcs backend. The list of supported options is referred below. </p> <pre><code>name: gcs-b\nkind: backend\nprovider: gcs\nspec:\n  bucket: cdev-states\n  prefix: pref\n</code></pre>"},{"location":"structure-backend/#options_1","title":"Options","text":"<ul> <li> <p><code>bucket</code> - required. The name of the GCS bucket. This name must be globally unique. For more information, see Bucket Naming Guidelines.</p> </li> <li> <p><code>credentials</code> / <code>GOOGLE_BACKEND_CREDENTIALS</code> / <code>GOOGLE_CREDENTIALS</code> - optional. Local path to Google Cloud Platform account credentials in JSON format. If unset, the path uses Google Application Default Credentials. The provided credentials must have the Storage Object Admin role on the bucket. Warning: if using the Google Cloud Platform provider as well, it will also pick up the <code>GOOGLE_CREDENTIALS</code> environment variable.</p> </li> <li> <p><code>impersonate_service_account</code> / GOOGLE_BACKEND_IMPERSONATE_SERVICE_ACCOUNT / GOOGLE_IMPERSONATE_SERVICE_ACCOUNT - optional. The service account to impersonate for accessing the State Bucket. You must have <code>roles/iam.serviceAccountTokenCreator</code> role on that account for the impersonation to succeed. If you are using a delegation chain, you can specify that using the <code>impersonate_service_account_delegates</code> field.</p> </li> <li> <p><code>impersonate_service_account_delegates</code> - optional. The delegation chain for an impersonating a service account as described here.</p> </li> <li> <p><code>access_token</code> - optional. A temporary [OAuth 2.0 access token] obtained from the Google Authorization server, i.e. the <code>Authorization: Bearer</code> token used to authenticate HTTP requests to GCP APIs. This is an alternative to credentials. If both are specified, <code>access_token</code> will be used over the credentials field.</p> </li> <li> <p><code>prefix</code> - optional. GCS prefix inside the bucket. Named states for workspaces are stored in an object called <code>&lt;prefix&gt;/&lt;name&gt;.tfstate</code>.</p> </li> <li> <p><code>encryption_key</code> / GOOGLE_ENCRYPTION_KEY - optional. A 32 byte base64 encoded 'customer-supplied encryption key' used when reading and writing state files in the bucket. For more information see Customer-supplied Encryption Keys.</p> </li> <li> <p><code>storage_custom_endpoint</code> / GOOGLE_BACKEND_STORAGE_CUSTOM_ENDPOINT / GOOGLE_STORAGE_CUSTOM_ENDPOINT - optional. A URL containing three parts: the protocol, the DNS name pointing to a Private Service Connect endpoint, and the path for the Cloud Storage API (<code>/storage/v1/b</code>, see here). You can either use a DNS name automatically made by the Service Directory or a custom DNS name made by you. For example, if you create an endpoint called <code>xyz</code> and want to use the automatically-created DNS name, you should set the field value as <code>https://storage-xyz.p.googleapis.com/storage/v1/b</code>. For help creating a Private Service Connect endpoint using Terraform, see this guide.</p> </li> </ul>"},{"location":"structure-backend/#digital-ocean-spaces-and-minio","title":"Digital Ocean Spaces and MinIO","text":"<p>To use DO spaces or MinIO object storage as a backend, use <code>s3</code> backend provider with additional options. See details: </p> <ul> <li>DO Spaces</li> <li>MinIO</li> </ul> <p>DO Spaces example:</p> <pre><code>name: do-backend\nkind: Backend\nprovider: s3\nspec:\n  bucket: cdev-state\n  region: main\n  access_key: \"&lt;SPACES_SECRET_KEY&gt;\" # Optional, it's better to use environment variable 'export SPACES_SECRET_KEY=\"key\"'\n  secret_key: \"&lt;SPACES_ACCESS_TOKEN&gt;\" # Optional, it's better to use environment variable 'export SPACES_ACCESS_TOKEN=\"token\"'\n  endpoint: \"https://sgp1.digitaloceanspaces.com\"\n  skip_credentials_validation: true\n  skip_region_validation: true\n  skip_metadata_api_check: true\n</code></pre> <p>MinIO example:</p> <pre><code>name: minio-backend\nkind: Backend\nprovider: s3\nspec:\n  bucket: cdev-state\n  region: main\n  access_key: \"minioadmin\"\n  secret_key: \"minioadmin\"\n  endpoint: http://127.0.0.1:9000\n  skip_credentials_validation: true\n  skip_region_validation: true\n  skip_metadata_api_check: true\n  force_path_style: true\n</code></pre>"},{"location":"structure-overview/","title":"Overview","text":""},{"location":"structure-overview/#main-objects","title":"Main objects","text":"<p>Unit \u2013 a block that executes Terraform modules, Helm charts, Kubernetes manifests, Terraform code, Bash scripts. Unit could source input variables from configuration (stacks) and from the outputs of other units. Unit could produce outputs that could be used by other units.</p> <p>Stack template \u2013 a set of units linked together into one infrastructure pattern (describes whole infrastructure). You can think of it like a complex Helm chart or compound Terraform Module.</p> <p>Stack \u2013 a set of variables that would be applied to a stack template (like <code>values.yaml</code> in Helm or <code>tfvars</code> file in Terraform). IT is used to configure the resulting infrastructure.</p> <p>Project \u2013 a high-level metaobject that could arrange multiple stacks and keep global variables. An infrastructure can consist of multiple stacks, while a project acts like an umbrella object for these stacks.</p>"},{"location":"structure-overview/#helper-objects","title":"Helper objects","text":"<p>Backend \u2013 describes the location where Cluster.dev hosts its own state and also could store Terraform unit states.</p> <p>Secret \u2013 an object that contains sensitive data such as a password, a token, or a key. Is used to pass secret values to the tools that don't have a proper support of secret engines.</p>"},{"location":"structure-project/","title":"Project","text":"<p>Project is a storage for variables related to all stacks. It is a high-level abstraction to store and reconcile different stacks, and pass values across them.</p> <p>File: <code>project.yaml</code>. Optional. Represents a set of configuration options for the whole project. Contains global project variables that can be used in other configuration objects, such as backend or stack (except of <code>secrets</code>). Note that the <code>project.conf</code> file is not rendered with the template and you cannot use template units in it.</p> <p>The <code>.cdevignore</code> file in the project dir indicates that the yaml/yml files it contains will be ignored during cdev configs. </p> <p>Example of <code>project.yaml</code>:</p> <pre><code>name: my_project\nkind: project\nbackend: aws-backend\nvariables:\n  organization: shalb\n  region: eu-central-1\n  state_bucket_name: cdev-states\nexports:\n  AWS_PROFILE: cluster-dev  \n</code></pre> <ul> <li> <p><code>name</code>- project name. Required.</p> </li> <li> <p><code>kind</code>- object kind. Must be set as <code>project</code>. Required.</p> </li> <li> <p><code>backend</code>- name of the backend that will be used to store the Cluster.dev state of the current project. Optional. </p> </li> <li> <p><code>variables</code>- a set of data in yaml format that can be referenced in other configuration objects. For the example above, the link to the organization name will look like this: <code>{{ .project.variables.organization }}</code>.</p> </li> <li> <p><code>exports</code>- list of environment variables that will be exported while working with the project. Optional.</p> </li> </ul>"},{"location":"structure-secrets/","title":"Secrets","text":"<p>Secret is an object that contains sensitive data such as a password, a token, or a key. It is used to pass secret values to the tools that don't have a proper support of secret engines.</p> <p>Cluster.dev allows for two ways of working with secrets.  </p>"},{"location":"structure-secrets/#sops-secrets","title":"SOPS secrets","text":"<p>See SOPS installation instructions in official repo.</p> <p>Secrets are encoded/decoded with SOPS utility that supports AWS KMS, GCP KMS, Azure Key Vault and PGP keys. How to use:</p> <ol> <li> <p>Use Cluster.dev console client to create a new secret from scratch:</p> <pre><code>cdev secret create\n</code></pre> </li> <li> <p>Use interactive menu to create a secret.</p> </li> <li> <p>Edit the secret and set secret data in <code>encrypted_data:</code> section.</p> </li> <li> <p>Use references to the secret data in a stack template (you can find the examples in the generated secret file).</p> </li> </ol>"},{"location":"structure-secrets/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Cluster.dev client can use AWS SSM as a secret storage. How to use:</p> <ol> <li> <p>Create a new secret in AWS Secrets Manager using AWS CLI or web console. Both raw and JSON data formats are supported.</p> </li> <li> <p>Use Cluster.dev console client to create a new secret from scratch:</p> <pre><code>cdev secret create\n</code></pre> </li> <li> <p>Answer the questions. For the <code>Name of secret in AWS Secrets Manager</code> enter the name of the AWS secret created above.</p> </li> <li> <p>Use references to the secret data in a stack template (you can find the examples in the generated secret file).</p> </li> </ol> <p>To list and edit any secret, use the commands:</p> <pre><code>cdev secret ls\n</code></pre> <p>and</p> <pre><code>cdev secret edit secret_name\n</code></pre>"},{"location":"structure-secrets/#secrets-reference","title":"Secrets reference","text":"<p>You can refer to a secret data in stack files with {{ .secrets.secret_name.secret_key }} syntax.   </p> <p>For example, we have a secret in AWS Secrets Manager and want to refer to the secret in our <code>stack.yaml</code>:</p> <pre><code>name: my-aws-secret\nkind: Secret\ndriver: aws_secretmanager\nspec: \n    region: eu-central-1\n    aws_secret_name: pass\n</code></pre> <p>In order to do this, we need to define the secret as {{ .secrets.my-aws-secret.some-key }} in the <code>stack.yaml</code>:</p> <pre><code>name: my-stack\ntemplate: https://&lt;template.git.url&gt;\nkind: Stack\nvariables:\n  region: eu-central-1\n  name: my-test-stack\n  password: {{ .secrets.my-aws-secret.some-key }}\n</code></pre>"},{"location":"structure-stack/","title":"Stack","text":"<p>Stack is a yaml file that tells Cluster.dev which template to use and what variables to apply to this template. Usually, users have multiple stacks that reflect their environments or tenants, and point to the same template with different variables.</p> <p>File: searching in <code>./*.yaml</code>. Required at least one. Stack object (<code>kind: stack</code>) contains reference to a stack template, variables to render the template and backend for states.</p> <p>Example of <code>stack.yaml</code>:</p> <pre><code># Define stack itself\nname: k3s-infra\ntemplate: \"./templates/\"\nkind: stack\nbackend: aws-backend\nvariables:\n  bucket: {{ .project.variables.state_bucket_name }} # Using project variables.\n  region: {{ .project.variables.region }}\n  organization: {{ .project.variables.organization }}\n  domain: cluster.dev\n  instance_type: \"t3.medium\"\n  vpc_id: \"vpc-5ecf1234\"\n</code></pre> <ul> <li> <p><code>name</code>- Required. The stack name. </p> </li> <li> <p><code>kind</code>- Required. Object kind <code>stack</code>. </p> </li> <li> <p><code>backend</code>- Optional. Name of the backend that will be used to store the states of this stack. </p> </li> <li> <p><code>variables</code>- data set for the stack template rendering. See variables.</p> </li> <li> <p><code>template</code>- Required. Either a path to a local directory containing the stack template's configuration files, or a remote Git repository as the stack template source. For more details on stack templates please refer to Stack Template section. A local path must begin with either <code>/</code> for absolute path, <code>./</code> or <code>../</code> for relative path. For Git source, use this format: <code>&lt;GIT_URL&gt;//&lt;PATH_TO_TEMPLATE_DIR&gt;?ref=&lt;BRANCH_OR_TAG&gt;</code>:</p> <ul> <li><code>&lt;GIT_URL&gt;</code> - required. Standard Git repo url. See details on official Git page.</li> <li><code>&lt;PATH_TO_TEMPLATE_DIR&gt;</code> - optional, use it if the stack template's configuration is not in repo root.</li> <li><code>&lt;BRANCH_OR_TAG&gt;</code>- Git branch or tag.</li> </ul> </li> <li> <p><code>disabled</code>- bool, optional. Disable stack execution. By default is set to <code>false</code>. If set to <code>true</code> the stack won't be applied. </p> </li> </ul>"},{"location":"structure-stack/#examples","title":"Examples","text":"<pre><code>template: /path/to/dir # absolute local path\ntemplate: ./template/ # relative local path\ntemplate: ../../template/ # relative local path\ntemplate: https://github.com/shalb/cdev-k8s # https Git url\ntemplate: https://github.com/shalb/cdev-k8s//some/dir/ # subdirectory\ntemplate: https://github.com/shalb/cdev-k8s//some/dir/?ref=branch-name # branch\ntemplate: https://github.com/shalb/cdev-k8s?ref=v1.1.1 # tag\ntemplate: git@github.com:shalb/cdev-k8s.git # ssh Git url\ntemplate: git@github.com:shalb/cdev-k8s.git//some/dir/ # subdirectory\ntemplate: git@github.com:shalb/cdev-k8s.git//some/dir/?ref=branch-name # branch\ntemplate: git@github.com:shalb/cdev-k8s.git?ref=v1.1.1 # tag\n</code></pre>"},{"location":"style-guide/","title":"Style guide","text":"<p>For better experience, we recommend using VS Code - we have a list of recommended extensions to prevent many common errors, improve code and save time.</p> <p>We use .editorconfig. It fixes basic mistakes on every file saving.</p> <p>Please make sure to install pre-commit-terraform with all its dependencies. It checks all changed files when you run <code>git commit</code> for more complex problems and tries to fix them for you.</p>"},{"location":"style-guide/#bash","title":"Bash","text":"<p>Firstly, please install shellcheck to have vscode-shellcheck extension working properly.</p> <p>We use Google Style Guide.</p>"},{"location":"style-guide/#terraform","title":"Terraform","text":"<p>We use Terraform Best Practices.com code style and conceptions.</p>"},{"location":"style-guide/#autogenerated-documentation","title":"Autogenerated Documentation","text":"<p>For the successful module documentation initialization, you need to create <code>README.md</code> with:</p> <pre><code>&lt;!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --&gt;\n\n&lt;!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK --&gt;\n</code></pre> <p>It is needed for terraform-docs hooks. The hook rewrites all the things inside with every <code>.tf</code> file change.</p> <p>Then run <code>pre-commit run --all-files</code> or make some changes in any <code>.tf</code> file in the same dir (for ex. <code>variable \"name\" {</code> -&gt; <code>variable \"name\"{</code>).</p>"},{"location":"templating/","title":"Templating","text":""},{"location":"templating/#levels-of-templating","title":"Levels of templating","text":"<p>Cluster.dev has a two-level templating that is applied on the project's and on the stack template's levels.</p> <p></p> <p>On the first level Cluster.dev reads a <code>project.yaml</code> and files with secrets. Then it uses variables from these files to populate and render files from the current project \u2013 stacks and backends.</p> <p>On the second level data from the stack object (an outcome of the first stage) is used to render stack template files.</p> <p>The templating process could be described as follows:</p> <ol> <li> <p>Reading data from <code>project.yaml</code> and secrets.</p> </li> <li> <p>Using the data to render all <code>yaml</code> files within the project directory.</p> </li> <li> <p>Reading data from <code>stack.yaml</code> and <code>backend.yaml</code> (the files rendered in p.#2) \u2013 first-level templating.</p> </li> <li> <p>Downloading specified stack templates.</p> </li> <li> <p>Rendering the stack templates from data contained in the corresponding <code>stack.yaml</code> files (p.#3) \u2013 second-level templating.</p> </li> <li> <p>Reading units from the stack templates.</p> </li> <li> <p>Executing the project.</p> </li> </ol> <p>Note</p> <p>Starting from the release version v0.9.6, it is possible to source variables into stack templates directly from <code>project.yaml</code>.   </p>"},{"location":"units-helm/","title":"Helm Unit","text":"<p>Describes Terraform Helm provider invocation.</p>"},{"location":"units-helm/#example-usage","title":"Example usage","text":"<p>In the example below we use <code>helm</code> unit to deploy Argo CD to a Kubernetes cluster:</p> <pre><code>units:\n  - name: argocd\n    type: helm\n    source:\n      repository: \"https://argoproj.github.io/argo-helm\"\n      chart: \"argo-cd\"\n      version: \"2.11.0\"\n    pre_hook:\n      command: *getKubeconfig\n      on_destroy: true\n    kubeconfig: /home/john/kubeconfig\n    additional_options:\n      namespace: \"argocd\"\n      create_namespace: true\n    values:\n      - file: ./argo/values.yaml\n        apply_template: true\n      - set:\n          global:\n            image:\n              tag: \"v1.8.3\"\n      - set: {{ insertYAML .variables.argocd.values }}\n    inputs:\n      global.image.tag: v1.8.3 # (same as values.set )\n</code></pre>"},{"location":"units-helm/#options","title":"Options","text":"<ul> <li> <p><code>force_apply</code> - bool, optional. By default is false. If set to true, the unit will be applied when any dependent unit is changed.</p> </li> <li> <p><code>source</code> - map, required. This block describes Helm chart source.</p> </li> <li> <p><code>chart</code>, <code>repository</code>, <code>version</code> - correspond to options with the same name from helm_release resource. See chart, repository and version.</p> </li> <li> <p><code>kubeconfig</code> - string, required. Path to the kubeconfig file which is relative to the directory where the unit was executed.</p> </li> <li> <p><code>provider_version</code> - string, optional. Version of Terraform Helm provider to use. Default behavior uses v3.x syntax (nested objects). For v2.x compatibility, explicitly specify a v2 version (e.g., <code>\"2.15.0\"</code>). See terraform helm provider and Provider Version Compatibility below  </p> </li> <li> <p><code>additional_options</code> - map of any, optional. Corresponds to Terraform helm_release resource options. Will be passed as is.</p> </li> <li> <p><code>values</code> - array, optional. List of values (file name or values data) to be passed to Helm. Values will be merged, in order, as Helm does with multiple -f options. For details see below.</p> </li> <li> <p><code>inputs</code> - map of any, optional. A map that represents Terraform helm_release sets. This block allows to use functions <code>remoteState</code> and <code>insertYAML</code>. For example:</p> <pre><code>   inputs:\n     global.image.tag: v1.8.3\n     service.type: LoadBalancer\n</code></pre> <p>Corresponds to:</p> <pre><code>  set {\n    name = \"global.image.tag\"\n    value = \"v1.8.3\"\n  }\n  set  {\n    name = \"service.type\"\n    value = \"LoadBalancer\"\n  }\n</code></pre> </li> <li> <p><code>provider_conf</code> - configuration block that describes authorization in Kubernetes. Supports the same arguments as the Terraform Kubernetes provider. It is allowed to use the <code>remoteState</code> function and Cluster.dev templates within the block. For details see below.</p> </li> </ul>"},{"location":"units-helm/#values","title":"<code>values</code>","text":"<ul> <li> <p><code>set</code> - map of any, required one of set/file. Set of Helm values. This option allows you to transfer the value of the Helm chart without saving it to a file.</p> </li> <li> <p><code>file</code> - string, required one of set/file. Path to the values file.</p> </li> <li> <p><code>apply_template</code> - bool, optional. Defines whether a template should be applied to the values file. By default is set to <code>true</code>. Used only with <code>file</code> option.</p> </li> </ul>"},{"location":"units-helm/#provider_conf","title":"<code>provider_conf</code>","text":"<p>Example usage:</p> <pre><code>  name: cert-manager-issuer\n  type: kubernetes\n  depends_on: this.cert-manager\n  source: ./deployment.yaml\n  provider_conf:\n    host: k8s.example.com\n    username: \"user\"\n    password: \"secretPassword\"\n</code></pre> <ul> <li> <p><code>host</code> - optional. The hostname (in form of URI) of the Kubernetes API. Can be sourced from <code>KUBE_HOST</code>.</p> </li> <li> <p><code>username</code> - optional. The username to use for HTTP basic authentication when accessing the Kubernetes API. Can be sourced from <code>KUBE_USER</code>.</p> </li> <li> <p><code>password</code> - optional. The password to use for HTTP basic authentication when accessing the Kubernetes API. Can be sourced from <code>KUBE_PASSWORD</code>.</p> </li> <li> <p><code>insecure</code> - optional. Whether the server should be accessed without verifying the TLS certificate. Can be sourced from <code>KUBE_INSECURE</code>. Defaults to <code>false</code>.</p> </li> <li> <p><code>tls_server_name</code> - optional. Server name passed to the server for SNI and is used in the client to check server certificates against. Can be sourced from <code>KUBE_TLS_SERVER_NAME</code>.</p> </li> <li> <p><code>client_certificate</code> - optional. PEM-encoded client certificate for TLS authentication. Can be sourced from <code>KUBE_CLIENT_CERT_DATA</code>.</p> </li> <li> <p><code>client_key</code> - optional. PEM-encoded client certificate key for TLS authentication. Can be sourced from <code>KUBE_CLIENT_KEY_DATA</code>.</p> </li> <li> <p><code>client_ca_certificate</code> - optional. PEM-encoded root certificates bundle for TLS authentication. Can be sourced from <code>KUBE_CLUSTER_CA_CERT_DATA</code>.</p> </li> <li> <p><code>config_path</code> - optional. A path to a kube config file. Can be sourced from <code>KUBE_CONFIG_PATH</code>.</p> </li> <li> <p><code>config_paths</code> - optional. A list of paths to the kube config files. Can be sourced from <code>KUBE_CONFIG_PATHS</code>.</p> </li> <li> <p><code>config_context</code> - optional. Context to choose from the config file. Can be sourced from <code>KUBE_CTX</code>.</p> </li> <li> <p><code>config_context_auth_info</code> - optional. Authentication info context of the kube config (name of the kubeconfig user, <code>--user</code> flag in <code>kubectl</code>). Can be sourced from <code>KUBE_CTX_AUTH_INFO</code>.</p> </li> <li> <p><code>config_context_cluster</code> - optional. Cluster context of the kube config (name of the kubeconfig cluster, <code>--cluster</code> flag in <code>kubectl</code>). Can be sourced from <code>KUBE_CTX_CLUSTER</code>.</p> </li> <li> <p><code>token</code> - optional. Token of your service account. Can be sourced from <code>KUBE_TOKEN</code></p> </li> <li> <p><code>proxy_url</code> - optional. URL to the proxy to be used for all API requests. URLs with \"http\", \"https\", and \"socks5\" schemes are supported. Can be sourced from <code>KUBE_PROXY_URL</code>.</p> </li> <li> <p><code>exec</code> - optional. Configuration block to use an exec-based credential plugin, e.g. call an external command to receive user credentials.</p> <ul> <li> <p><code>api_version</code> - required. API version to use when decoding the ExecCredentials resource, e.g. <code>client.authentication.k8s.io/v1beta1</code>.</p> </li> <li> <p><code>command</code> - required. Command to execute.</p> </li> <li> <p><code>args</code> - optional. List of arguments to pass when executing the plugin.</p> </li> <li> <p><code>env</code> - optional. Map of environment variables to set when executing the plugin.</p> </li> </ul> </li> <li> <p><code>ignore_annotations</code> - optional.  List of Kubernetes metadata annotations to ignore across all resources handled by this provider for situations where external systems are managing certain resource annotations. This option does not affect annotations within a template block. Each item is a regular expression.</p> </li> <li> <p><code>ignore_labels</code> - optional. List of Kubernetes metadata labels to ignore across all resources handled by this provider for situations where external systems are managing certain resource labels. This option does not affect annotations within a template block. Each item is a regular expression.</p> </li> </ul>"},{"location":"units-helm/#provider-version-compatibility","title":"Provider Version Compatibility","text":"<p>The Helm unit supports both Terraform Helm provider v2.x and v3.x, which have different syntax requirements for the <code>kubernetes</code> configuration block.</p>"},{"location":"units-helm/#default-behavior-v3x-syntax","title":"Default Behavior (v3.x Syntax)","text":"<p>When no <code>provider_version</code> is specified, the unit defaults to v3.x syntax using nested objects:</p> <pre><code>units:\n  - name: my-chart\n    type: helm\n    source:\n      repository: \"https://charts.example.com\"\n      chart: \"my-chart\"\n      version: \"1.0.0\"\n    provider_conf:\n      config_path: \"/path/to/kubeconfig\"\n      config_context: \"my-context\"\n</code></pre> <p>Generates Terraform code: <pre><code>provider \"helm\" {\n  kubernetes = {\n    config_path    = \"/path/to/kubeconfig\"\n    config_context = \"my-context\"\n  }\n}\n</code></pre></p>"},{"location":"units-helm/#v2x-compatibility-block-syntax","title":"v2.x Compatibility (Block Syntax)","text":"<p>To use the legacy v2.x block syntax, explicitly specify a v2 provider version:</p> <pre><code>units:\n  - name: my-chart\n    type: helm\n    provider_version: \"2.15.0\"\n    source:\n      repository: \"https://charts.example.com\"\n      chart: \"my-chart\"\n      version: \"1.0.0\"\n    provider_conf:\n      config_path: \"/path/to/kubeconfig\"\n      config_context: \"my-context\"\n</code></pre> <p>Generates Terraform code: <pre><code>provider \"helm\" {\n  kubernetes {\n    config_path    = \"/path/to/kubeconfig\"\n    config_context = \"my-context\"\n  }\n}\n</code></pre></p>"},{"location":"units-helm/#version-detection","title":"Version Detection","text":"<ul> <li>No <code>provider_version</code> specified: Uses v3.x syntax (nested objects)</li> <li><code>provider_version</code> starts with \"3\" or higher: Uses v3.x syntax (nested objects)  </li> <li><code>provider_version</code> starts with \"2\" or lower: Uses v2.x syntax (blocks)</li> </ul> <p>Migration from v2 to v3</p> <p>If you're upgrading from Helm provider v2.x to v3.x, you can simply remove the <code>provider_version</code> field from your unit configuration to use the new v3.x syntax, or update it to a v3.x version number.</p>"},{"location":"units-k8s-manifest/","title":"K8s-manifest Unit","text":"<p>Info</p> <p>This unit is deprecated. We suggest using the Kubernetes unit instead.</p> <p>Applies Kubernetes resources from manifests. </p>"},{"location":"units-k8s-manifest/#example-usage","title":"Example usage","text":"<pre><code>- name: kubectl-test2\n  type: k8s-manifest\n  namespace: default\n  create_namespaces: true\n  path: ./manifests/\n  apply_template: true\n  kubeconfig: {{ output \"this.kubeconfig.kubeconfig_path\" }}\n  kubectl_opts: \"--wait=true\"\n</code></pre>"},{"location":"units-k8s-manifest/#options","title":"Options","text":"<ul> <li> <p><code>force_apply</code> - bool, optional. By default is false. If set to true, the unit will be applied when any dependent unit is changed.</p> </li> <li> <p><code>namespace</code> - optional. Corresponds to <code>kubectl -n</code>.</p> </li> <li> <p><code>create_namespaces</code> - bool, optional. By default is false. If set to true, cdev will create namespaces required for the unit (i.e. the namespaces listed in manifests and the one specified within the <code>namespace</code> option), in case they don't exist.</p> </li> <li> <p><code>path</code> - required, string. Indicates the resources that are to be applied: a file (in case of a file path), a directory recursively (in case of a directory path) or URL. In case of URL path, the unit will download the resources by the link and then apply them.  </p> <p>Example of URL <code>path</code>:</p> <pre><code>- name: kubectl-test2\n  type: k8s-manifest\n  namespace: default\n  path: https://git.io/vPieo\n  kubeconfig: {{ output \"this.kubeconfig.kubeconfig_path\" }}\n</code></pre> </li> <li> <p><code>apply_template</code> - bool. By default is set to <code>true</code>. Enables applying templating to all Kubernetes manifests located in the specified <code>path</code>. </p> </li> <li> <p><code>kubeconfig</code> - optional. Specifies the path to a kubeconfig file. See How to get kubeconfig subsection below.</p> </li> <li> <p><code>kubectl_opts</code> - optional. Lists additional arguments of the <code>kubectl</code> command.</p> </li> </ul>"},{"location":"units-k8s-manifest/#how-to-get-kubeconfig","title":"How to get kubeconfig","text":"<p>There are several ways to get a kubeconfig from a cluster and pass it to the units that require it (for example, <code>helm</code>, <code>K8s-manifest</code>). The recommended way is to use the <code>shell</code> unit with the option <code>force_apply</code>. Here is an example of such unit:</p> <pre><code>- name: kubeconfig\n  type: shell\n  force_apply: true\n  depends_on: this.k3s\n  apply:\n    commands:\n      - aws s3 cp s3://{{ .variables.bucket }}/{{ .variables.cluster_name }}/kubeconfig /tmp/kubeconfig_{{ .variables.cluster_name }}\n      - echo \"kubeconfig_base64=$(cat /tmp/kubeconfig_{{ .variables.cluster_name }} | base64 -w 0)\"\n      - echo \"kubeconfig_path=/tmp/kubeconfig_{{ .variables.cluster_name }}\"\n  outputs:\n    type: separator\n    separator: \"=\"\n</code></pre> <p>In the example above, the <code>K3s</code> unit (the one referred to) will deploy a Kubernetes cluster in AWS and place a kubeconfig file in S3 bucket. The <code>kubeconfig</code> unit will download the kubeconfig from the storage and place it within the /tmp directory. </p> <p>The kubeconfig can then be passed as an output to other units:</p> <pre><code>- name: cert-manager-issuer\n  type: k8s-manifest\n  path: ./cert-manager/issuer.yaml\n  kubeconfig: {{ output \"this.kubeconfig.kubeconfig_path\" }}\n</code></pre> <p>An alternative (but not recommended) way is to create a <code>yaml</code> hook in a stack template that would take the required set of commands:</p> <pre><code>_: &amp;getKubeconfig \"rm -f ../kubeconfig_{{ .name }}; aws eks --region {{ .variables.region }} update-kubeconfig --name {{ .name }} --kubeconfig ../kubeconfig_{{ .name }}\"\n</code></pre> <p>And execute it with a pre-hook in each unit:</p> <pre><code>- name: cert-manager-issuer\n  type: kubernetes\n  source: ./cert-manager/\n  provider_version: \"0.6.0\"\n  config_path: ../kubeconfig_{{ .name }}\n  depends_on: this.cert-manager\n  pre_hook:\n    command: *getKubeconfig\n    on_destroy: true\n    on_plan: true\n</code></pre>"},{"location":"units-kubernetes/","title":"Kubernetes Unit","text":"<p>The unit employs the Terraform Kubernetes provider to interact with the resources supported by Kubernetes. Unlike the Terraform Kubernetes provider, it supports direct input of Kubernetes manifests, allowing the use of <code>remoteState</code> and Cluster.dev templating in manifest files.</p> <p>The unit automatically converts Kubernetes manifests into Terraform resources by parsing the <code>apiVersion</code> and <code>kind</code> fields. Check the Kubernetes provider documentation for a list of supported resources. Custom or unsupported resources are converted into the universal resource kubernetes_manifest.</p> <p>Tip</p> <p>To deploy to Kubernetes using Terraform, use this unit to automatically convert YAML files into ready-to-use HCL code.</p>"},{"location":"units-kubernetes/#example-usage","title":"Example usage","text":"<pre><code>units:\n  - name: argocd_apps\n    type: kubernetes\n    source: ./argocd-apps/app1.yaml\n    kubeconfig: ../kubeconfig\n    depends_on: this.argocd\n</code></pre>"},{"location":"units-kubernetes/#options","title":"Options","text":"<ul> <li> <p><code>force_apply</code> - bool, optional. By default is false. If set to true, the unit will be applied when any dependent unit is changed.</p> </li> <li> <p><code>source</code> - string, required. Similar to option <code>path</code> in the <code>k8s-manifest</code> unit. Indicates the resources that are to be applied: a file (in case of a file path), a directory recursively (in case of a directory path) or URL. In case of URL path, the unit will download the resources by the link and then apply them. Source file will be rendered with the stack template, and also allows to use functions <code>remoteState</code> and <code>insertYAML</code>.</p> </li> <li> <p><code>kubeconfig</code> - string, required. Path to the kubeconfig file, which is relative to the directory where the unit was executed.</p> </li> <li> <p><code>apply_template</code> - bool. By default is set to <code>true</code>. Enables applying templating to all Kubernetes manifests located in the specified <code>path</code>. </p> </li> <li> <p><code>provider_conf</code> - configuration block that describes authorization in Kubernetes. Supports the same arguments as the Terraform Kubernetes provider. It is allowed to use the <code>remoteState</code> function and Cluster.dev templates within the block. For details see below.</p> </li> </ul>"},{"location":"units-kubernetes/#provider_conf","title":"<code>provider_conf</code>","text":"<p>Example usage:</p> <pre><code>  name: cert-manager-issuer\n  type: kubernetes\n  depends_on: this.cert-manager\n  source: ./deployment.yaml\n  provider_conf:\n    host: k8s.example.com\n    username: \"user\"\n    password: \"secretPassword\"\n</code></pre> <ul> <li> <p><code>host</code> - optional. The hostname (in form of URI) of the Kubernetes API. Can be sourced from <code>KUBE_HOST</code>.</p> </li> <li> <p><code>username</code> - optional. The username to use for HTTP basic authentication when accessing the Kubernetes API. Can be sourced from <code>KUBE_USER</code>.</p> </li> <li> <p><code>password</code> - optional. The password to use for HTTP basic authentication when accessing the Kubernetes API. Can be sourced from <code>KUBE_PASSWORD</code>.</p> </li> <li> <p><code>insecure</code> - optional. Whether the server should be accessed without verifying the TLS certificate. Can be sourced from <code>KUBE_INSECURE</code>. Defaults to <code>false</code>.</p> </li> <li> <p><code>tls_server_name</code> - optional. Server name passed to the server for SNI and is used in the client to check server certificates against. Can be sourced from <code>KUBE_TLS_SERVER_NAME</code>.</p> </li> <li> <p><code>client_certificate</code> - optional. PEM-encoded client certificate for TLS authentication. Can be sourced from <code>KUBE_CLIENT_CERT_DATA</code>.</p> </li> <li> <p><code>client_key</code> - optional. PEM-encoded client certificate key for TLS authentication. Can be sourced from <code>KUBE_CLIENT_KEY_DATA</code>.</p> </li> <li> <p><code>client_ca_certificate</code> - optional. PEM-encoded root certificates bundle for TLS authentication. Can be sourced from <code>KUBE_CLUSTER_CA_CERT_DATA</code>.</p> </li> <li> <p><code>config_path</code> - optional. A path to a kube config file. Can be sourced from <code>KUBE_CONFIG_PATH</code>.</p> </li> <li> <p><code>config_paths</code> - optional. A list of paths to the kube config files. Can be sourced from <code>KUBE_CONFIG_PATHS</code>.</p> </li> <li> <p><code>config_context</code> - optional. Context to choose from the config file. Can be sourced from <code>KUBE_CTX</code>.</p> </li> <li> <p><code>config_context_auth_info</code> - optional. Authentication info context of the kube config (name of the kubeconfig user, <code>--user</code> flag in <code>kubectl</code>). Can be sourced from <code>KUBE_CTX_AUTH_INFO</code>.</p> </li> <li> <p><code>config_context_cluster</code> - optional. Cluster context of the kube config (name of the kubeconfig cluster, <code>--cluster</code> flag in <code>kubectl</code>). Can be sourced from <code>KUBE_CTX_CLUSTER</code>.</p> </li> <li> <p><code>token</code> - optional. Token of your service account. Can be sourced from <code>KUBE_TOKEN</code></p> </li> <li> <p><code>proxy_url</code> - optional. URL to the proxy to be used for all API requests. URLs with \"http\", \"https\", and \"socks5\" schemes are supported. Can be sourced from <code>KUBE_PROXY_URL</code>.</p> </li> <li> <p><code>exec</code> - optional. Configuration block to use an exec-based credential plugin, e.g. call an external command to receive user credentials.</p> <ul> <li> <p><code>api_version</code> - required. API version to use when decoding the ExecCredentials resource, e.g. <code>client.authentication.k8s.io/v1beta1</code>.</p> </li> <li> <p><code>command</code> - required. Command to execute.</p> </li> <li> <p><code>args</code> - optional. List of arguments to pass when executing the plugin.</p> </li> <li> <p><code>env</code> - optional. Map of environment variables to set when executing the plugin.</p> </li> </ul> </li> <li> <p><code>ignore_annotations</code> - optional.  List of Kubernetes metadata annotations to ignore across all resources handled by this provider for situations where external systems are managing certain resource annotations. This option does not affect annotations within a template block. Each item is a regular expression.</p> </li> <li> <p><code>ignore_labels</code> - optional. List of Kubernetes metadata labels to ignore across all resources handled by this provider for situations where external systems are managing certain resource labels. This option does not affect annotations within a template block. Each item is a regular expression.</p> </li> </ul>"},{"location":"units-overview/","title":"Overview","text":""},{"location":"units-overview/#description","title":"Description","text":"<p>Units are fundamental components forming the basis of stack templates. They can include Terraform modules, Helm charts for installation, or Bash scripts for execution. Units may reside remotely or within the same repository as other Cluster.dev code. These units may reference necessary files, which should be located within the current directory (in the context of the stack template). Since some of these files are rendered with project data, you can use Go templates in them.  </p> <p>Tip</p> <p>You can pass variables across units within the stack template by using outputs or <code>remoteState</code>.</p> <p>All units described below have a common format and common fields. Base example:</p> <pre><code>  - name: k3s\n    type: tfmodule\n    depends_on:\n      - this.unit1_name\n      - this.unit2_name\n#   depends_on: this.unit1_name # is allowed to use string for single, or list for multiple dependencies\n    pre_hook:\n      command: \"echo pre_hook\"\n      # script: \"./scripts/hook.sh\"\n      on_apply: true\n      on_destroy: false\n      on_plan: false\n    post_hook:\n      # command: \"echo post_hook\"\n      script: \"./scripts/hook.sh\"\n      on_apply: true\n      on_destroy: false\n      on_plan: false\n</code></pre> <ul> <li> <p><code>name</code> - unit name. Required.</p> </li> <li> <p><code>type</code> - unit type. One of: <code>shell</code>, <code>tfmodule</code>, <code>helm</code>, <code>kubernetes</code>, <code>printer</code>.</p> </li> <li> <p><code>depends_on</code> - string or list of strings. One or multiple unit dependencies in the format \"stack_name.unit_name\". Since the name of the stack is unknown inside the stack template, you can use \"this\" instead:<code>\"this.unit_name.output_name\"</code>.</p> </li> <li> <p><code>pre_hook</code> and <code>post_hook</code> blocks: See the description in Shell unit. </p> </li> </ul>"},{"location":"units-passing-variables/","title":"Units passing variables","text":"<p>Note</p> <p>If passing outputs across units within one stack template, use \"this\" instead of the stack name: {{ output \"this.unit_name.output\" }}:.</p> <p>Example of passing variables across units in the stack template:</p> <pre><code>name: s3-static-web\nkind: StackTemplate\nunits:\n  - name: s3-web\n    type: tfmodule\n    source: \"terraform-aws-modules/s3-bucket/aws\"\n    providers:\n    - aws:\n        region: {{ .variables.region }}\n    inputs:\n      bucket: {{ .variables.name }}\n      force_destroy: true\n      acl: \"public-read\"\n  - name: outputs\n    type: printer\n    outputs:\n      bucket_name: {{ remoteState \"this.s3-web.s3_bucket_website_endpoint\" }}\n      name: {{ .variables.name }}\n</code></pre>"},{"location":"units-printer/","title":"Printer Unit","text":"<p>This unit exposes outputs that can be used in other units and stacks.</p> <p>Tip</p> <p>If named output, the unit will have all its outputs displayed when running <code>cdev apply</code> or <code>cdev output</code>. </p>"},{"location":"units-printer/#example-usage","title":"Example usage","text":"<pre><code>units:\n  - name: outputs\n    type: printer\n    outputs:\n      bucket_name: \"Endpoint: {{ remoteState \"this.s3-web.s3_bucket_website_endpoint\" }}\"\n      name: {{ .variables.name }}\n</code></pre>"},{"location":"units-printer/#options","title":"Options","text":"<ul> <li> <p><code>outputs</code> - any, required - a map that represents data to be printed in the log. The block allows to use functions <code>remoteState</code> and <code>insertYAML</code>.</p> </li> <li> <p><code>force_apply</code> - bool, optional. By default is false. If set to true, the unit will be applied when any dependent unit is changed.</p> </li> </ul>"},{"location":"units-shell/","title":"Shell Unit","text":"<p>Executes Shell commands and scripts. </p>"},{"location":"units-shell/#example-usage","title":"Example usage","text":"<p>Example of a <code>shell</code> unit that creates an <code>index.html</code> file with a greeting message and downloads the file into an S3 bucket. The bucket name is passed as a variable:</p> <pre><code>units:\n  - name: upload-web\n    type: shell\n    apply:\n      commands:\n        - aws s3 cp ./index.html s3://{{ .variables.name }}/index.html\n    create_files:\n    - file: ./index.html\n      content: |\n        &lt;h1&gt; Hello from {{ .variables.organization }} &lt;/h1&gt;\n        This page was created automatically by cdev tool.\n</code></pre> <p>Complete reference example:</p> <pre><code>units:\n  - name: my-tf-code\n    type: shell\n    env: \n      AWS_PROFILE: {{ .variables.aws_profile }}\n      TF_VAR_region: {{ .project.region }}\n    create_files:\n      - file: ./terraform.tfvars\n        content: |\n{{- range $key, $value := .variables.tfvars }}\n        $key = \"$value\" \n{{- end}}\n    work_dir: ~/env/prod/\n    apply: \n      commands:\n        - terraform apply -var-file terraform.tfvars {{ range $key, $value := .variables.vars_list }} -var=\"$key=$value\"{{ end }}\n    plan:\n      commands:\n        - terraform plan\n    destroy:\n      commands:\n        - terraform destroy\n        - rm ./.terraform\n    outputs: # how to get outputs\n      type: json (regexp, separator)\n      regexp_key: \"regexp\"\n      regexp_value: \"regexp\"\n      separator: \"=\"\n      command: terraform output -json\n    pre_hook:\n      on_apply: true\n      on_destroy: true\n      command: |\n        cat ./main.tf\n    post_hook:\n      on_apply: false\n      on_destroy: true\n      command: |\n        terraform output\n    create_files:\n        - file: ./my_text_file.txt\n          mode: 0644\n          content: \"some text\"\n        - file: ./my_text_file2.txt\n          content: \"some text 2\"\n</code></pre>"},{"location":"units-shell/#options","title":"Options","text":"<ul> <li> <p><code>force_apply</code> - bool, optional. By default is false. If set to true, the unit will be applied when any dependent unit is changed.</p> </li> <li> <p><code>env</code> - map, optional. The list of environment variables that will be exported before executing commands of this unit. The variables defined in shell unit have a priority over variables defined in the project (the option <code>exports</code>) and will rewrite them.</p> </li> <li> <p><code>work_dir</code> - string, required. The working directory within which the code of the unit will be executed.</p> </li> <li> <p><code>apply</code> - optional, map. Describes commands to be executed when running <code>cdev apply</code>. For details see below. </p> </li> <li> <p><code>plan</code> - optional, map. Describes commands to be executed when running <code>cdev plan</code>. For details see below.</p> </li> <li> <p><code>destroy</code> - optional, map. Describes commands to be executed when running <code>cdev destroy</code>. For details see below.</p> </li> <li> <p><code>outputs</code> - optional, map. Describes how to get outputs from a command. For details see below.</p> </li> <li> <p><code>create_files</code> - list of files, optional. The list of files that have to be saved in the state in case of their changing.</p> </li> <li> <p><code>pre_hook</code> and <code>post_hook</code> blocks: describe shell commands to be executed before and after the unit, respectively. The commands will be executed in the same context as the actions of the unit. Environment variables are common to shell commands, the <code>pre_hook</code> and <code>post_hook</code> scripts, and unit execution. You can export a variable in the <code>pre_hook</code> and it will be available in the <code>post_hook</code> or in the unit. For details see below.</p> </li> </ul>"},{"location":"units-shell/#apply","title":"<code>apply</code>","text":"<ul> <li> <p><code>init</code> - optional. Describes commands to be executed prior to running <code>cdev apply</code>.</p> </li> <li> <p><code>commands</code> - list of strings, required. The list of commands to be executed when running <code>cdev apply</code>.</p> </li> </ul>"},{"location":"units-shell/#plan","title":"<code>plan</code>","text":"<ul> <li> <p><code>init</code> - optional. Describes commands to be executed prior to running <code>cdev plan</code>.</p> </li> <li> <p><code>commands</code> - list of strings, required. The list of commands to be executed when running <code>cdev plan</code>.</p> </li> </ul>"},{"location":"units-shell/#destroy","title":"<code>destroy</code>","text":"<ul> <li> <p><code>init</code> - optional. Describes commands to be executed prior to running <code>cdev destroy</code>.</p> </li> <li> <p><code>commands</code> - list of strings, required. The list of commands to be executed when running <code>cdev destroy</code>.</p> </li> </ul>"},{"location":"units-shell/#outputs","title":"<code>outputs</code>","text":"<ul> <li> <p><code>type</code> - string, required. A type of format to deliver the output. Could have 3 options: JSON, regexp, separator. According to the type specified, further options will differ.</p> </li> <li> <p><code>JSON</code> - if the <code>type</code> is defined as JSON, outputs will be parsed as key-value JSON. This type of output makes all other options not required.</p> </li> <li> <p><code>regexp</code> - if the <code>type</code> is defined as <code>regexp</code>, this introduces an additional required option <code>regexp</code>: a regular expression which defines how to parse each line in the module output. Example:</p> </li> </ul> <pre><code>outputs: # how to get outputs\n  type: regexp\n  regexp: \"^(.*)=(.*)$\"\n  command: | \n  echo \"key1=val1\\nkey2=val2\"\n</code></pre> <ul> <li><code>separator</code> - if the <code>type</code> is defined as separator, this introduces an additional option <code>separator</code> (string). Separator is a symbol that defines how a line is divided in two parts: the key and the value.</li> </ul> <pre><code>outputs: # how to get outputs\n  type: separator\n  separator: \"=\"\n  command: |\n  echo \"key1=val1\\nkey2=val2\"\n</code></pre> <ul> <li><code>command</code> - string, optional. The command to take the outputs from. Is used regardless of the type option. If the command is not defined, cdev takes the outputs from the <code>apply</code> command.</li> </ul>"},{"location":"units-shell/#pre_hook-and-post_hook-blocks","title":"<code>pre_hook</code> and <code>post_hook</code> blocks","text":"<p>Example usage:</p> <pre><code>units:\n    ....\n    pre_hook:\n      on_apply: true\n      on_destroy: true\n      command: |\n        cat ./main.tf\n</code></pre> <ul> <li> <p><code>command</code> - string. Shell command in text format. Will be executed in Bash -c \"command\". Can be used if the \"script\" option is not used. One of <code>command</code> or <code>script</code> is required.</p> </li> <li> <p><code>on_apply</code> bool, optional. Turn off/on when unit applying. Default: \"true\".</p> </li> <li> <p><code>on_destroy</code> - bool, optional. Turn off/on when unit destroying. Default: \"false\".</p> </li> <li> <p><code>on_plan</code> - bool, optional. Turn off/on when unit plan executing. Default: \"false\".</p> </li> </ul>"},{"location":"units-terraform/","title":"Tfmodule Unit","text":"<p>Describes direct invocation of Terraform modules.</p>"},{"location":"units-terraform/#example-usage","title":"Example usage","text":"<p>In the example below we use the <code>tfmodule</code> unit to create an S3 bucket for hosting a static web page. The <code>tfmodule</code> unit applies a dedicated Terraform module.   </p> <pre><code>units:\n  - name: s3-web\n    type: tfmodule\n    version: \"2.77.0\"\n    source: \"terraform-aws-modules/s3-bucket/aws\"\n    providers:\n    - aws:\n        region: {{ .variables.region }}\n    inputs:\n      bucket: {{ .variables.name }}\n      force_destroy: true\n      acl: \"public-read\"\n      website:\n        index_document: \"index.html\"\n        error_document: \"index.html\"\n      attach_policy: true\n      policy: |\n        {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Sid\": \"PublicReadGetObject\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": \"*\",\n                    \"Action\": \"s3:GetObject\",\n                    \"Resource\": \"arn:aws:s3:::{{ .variables.name }}/*\"\n                }\n            ]\n        }\n</code></pre>"},{"location":"units-terraform/#options","title":"Options","text":"<ul> <li> <p><code>source</code> - string, required. Terraform module source. It is not allowed to use local folders in source!</p> </li> <li> <p><code>version</code> - string, optional. Module version.</p> </li> <li> <p><code>inputs</code> - map of any, required. A map that corresponds to input variables defined by the module. This block allows to use functions <code>remoteState</code> and <code>insertYAML</code>.</p> </li> <li> <p><code>force_apply</code> - bool, optional. By default is false. If set to true, the unit will be applied when any dependent unit is changed.</p> </li> </ul>"},{"location":"variables/","title":"Variables","text":"<p>Stack configuration contains variables that will be applied to a stack template (similar to <code>values.yaml</code> in Helm or <code>tfvars</code> file in Terraform). The variables from <code>stack.yaml</code> are passed to stack template files to render them.  </p> <p>Example of <code>stack.yaml</code> with variables <code>region</code>, <code>name</code>, <code>organization</code>:</p> <pre><code>name: k3s-demo\ntemplate: https://github.com/shalb/cdev-s3-web\nkind: Stack\nvariables:\n  region: eu-central-1\n  name: web-static-page\n  organization: Cluster.dev\n</code></pre> <p>The values of the variables are passed to a stack template to configure the resulting infrastructure.  </p> <p>Starting from the release version v0.9.6, it is possible to source variables into stack templates directly from <code>project.yaml</code>.   </p>"},{"location":"variables/#passing-variables-across-stacks","title":"Passing variables across stacks","text":"<p>Cluster.dev allows passing variable values across different stacks within one project. This is made possible in 2 ways:</p> <ul> <li>using the output of one stack as an input for another stack: {{ output \"stack_name.unit_name.output\" }}</li> </ul> <p>Example of passing outputs between stacks:</p> <pre><code>name: s3-web-page\ntemplate: ../web-page/\nkind: Stack\nvariables:\n  region: eu-central-1\n  name: web-static-page\n  organization: Shalb\n</code></pre> <pre><code>name: health-check\ntemplate: ../health-check/\nkind: Stack\nvariables:\n  url: {{ output \"s3-web-page.outputs.url\" }}\n</code></pre> <ul> <li>using <code>remoteState</code> with a syntax: {{ remoteState \"stack_name.unit_name.output\" }}</li> </ul>"},{"location":"variables/#global-variables","title":"Global variables","text":"<p>The variables defined on a project level are called global. Global variables are listed in the <code>project.yaml</code>\u2013 a configuration file that defines the parameters and settings for the whole project. From the <code>project.yaml</code> the variables values can be applied to all stack and backend objects within this project. </p> <p>Example of the <code>project.yaml</code> file that contains variables <code>organization</code> and <code>region</code>:</p> <pre><code>name: demo\nkind: Project\nvariables:\n  organization: shalb\n  region: eu-central-1\n</code></pre> <p>To refer to a variable in stack and backend files, use the {{ .project.variables.KEY_NAME }} syntax, where project.variables is the path that corresponds the structure of variables in the <code>project.yaml</code>. The KEY_NAME stands for the variable name defined in the <code>project.yaml</code> and will be replaced by its value. </p> <p>Example of the <code>stack.yaml</code> file that contains reference to the project variables <code>organization</code> and <code>region</code>:</p> <pre><code>name: eks-demo\ntemplate: https://github.com/shalb/cdev-aws-eks?ref=v0.2.0\nkind: Stack\nbackend: aws-backend\nvariables:\n  region: {{ .project.variables.region }}\n  organization: {{ .project.variables.organization }}\n  domain: cluster.dev\n  instance_type: \"t3.medium\"\n  eks_version: \"1.20\"\n</code></pre> <p>Example of the rendered <code>stack.yaml</code>:</p> <pre><code>name: eks-demo\ntemplate: https://github.com/shalb/cdev-aws-eks?ref=v0.2.0\nkind: Stack\nbackend: aws-backend\nvariables:\n  region: eu-central-1\n  organization: shalb\n  domain: cluster.dev\n  instance_type: \"t3.medium\"\n  eks_version: \"1.20\"\n</code></pre>"}]}